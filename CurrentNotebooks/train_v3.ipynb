{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"zJj5OlQdtDSx","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","import argparse\n","import json\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import cuda\n","import sys, os\n","import random\n","import numpy as np\n","from sklearn import metrics\n","import models as Model\n","from SiameseLoss import ContrastiveLoss\n","import evaluate\n","import data_v2 as Data\n","import gc\n","import csv\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","\n","\n","parser = argparse.ArgumentParser(description='DeepDiff')\n","parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate')\n","parser.add_argument('--model_name', type=str, default='raw_d', help='DeepDiff variation')\n","parser.add_argument('--clip', type=float, default=1,help='gradient clipping')\n","parser.add_argument('--epochs', type=int, default=90, help='upper epoch limit')\n","parser.add_argument('--batch_size', type=int, default=10, help='')\n","parser.add_argument('--dropout', type=float, default=0.5, help='dropout applied to layers (0 = no dropout) if n_layers LSTM > 1')\n","parser.add_argument('--cell_1', type=str, default='Cell1', help='cell type 1')\n","parser.add_argument('--cell_2', type=str, default='Cell2', help='cell type 2')\n","parser.add_argument('--save_root', type=str, default='./Results/', help='where to save')\n","parser.add_argument('--data_root', type=str, default='./data/', help='data location')\n","parser.add_argument('--gpuid', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--gpu', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--n_hms', type=int, default=5, help='number of histone modifications')\n","parser.add_argument('--n_bins', type=int, default=1, help='number of bins')\n","parser.add_argument('--bin_rnn_size', type=int, default=32, help='bin rnn size')\n","parser.add_argument('--num_layers', type=int, default=1, help='number of layers')\n","parser.add_argument('--unidirectional', action='store_true', help='bidirectional/undirectional LSTM')\n","parser.add_argument('--save_attention_maps',action='store_true', help='set to save validation beta attention maps')\n","parser.add_argument('--attentionfilename', type=str, default='beta_attention.txt', help='where to save attnetion maps')\n","parser.add_argument('--test_on_saved_model',action='store_true', help='only test on saved model')\n","args = parser.parse_args()\n","\n","torch.manual_seed(1)\n","\n","model_name = ''\n","model_name += (args.cell_1)+('_')+(args.cell_2)+('_')\n","\n","model_name+=args.model_name\n","\n","\n","\n","\n","args.bidirectional=not args.unidirectional\n","\n","print('the model name: ',model_name)\n","args.data_root+=''\n","args.save_root+=''\n","args.dataset=args.cell_1+('_')+args.cell_2\n","args.data_root = os.path.join(args.data_root)\n","print('loading data from:  ',args.data_root)\n","args.save_root = os.path.join(args.save_root,args.dataset)\n","print('saving results in  from: ',args.save_root)\n","model_dir = os.path.join(args.save_root,model_name)\n","if not os.path.exists(model_dir):\n","    os.makedirs(model_dir)\n","attentionmapfile=model_dir+'/'+args.attentionfilename\n","print('==>processing data')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","CON=False\n","AUX=False\n","print('==>building model')\n","if(args.model_name=='raw_d'):\n","    model = Model.raw_d(args)\n","    # model = raw_d(args)\n","\n","elif(args.model_name=='raw_c'):\n","    model = Model.raw_c(args)\n","    # model = raw_c(args)\n","\n","elif(args.model_name=='raw'):\n","    model = Model.raw(args)\n","    # model = raw(args)\n","\n","elif(args.model_name=='aux'):\n","    args.shared=False\n","    \n","    model = Model.aux(args)\n","    # model = aux(args)\n","\n","    AUX=True\n","    args.gamma=0.0\n","elif(args.model_name=='raw_aux'):\n","    args.shared=False\n","\n","    model = Model.raw_aux(args)\n","    # model = raw_aux(args)\n","    \n","    AUX=True\n","    args.gamma=0.0\n","elif(args.model_name=='aux_siamese'):\n","    CON=True\n","    args.shared=True\n","    \n","    model = Model.aux_siamese(args)\n","    # model = aux_siamese(args)\n","    \n","    AUX=True\n","    args.gamma=4.0\n","elif(args.model_name=='raw_aux_siamese'):\n","    CON=True\n","    args.shared=True\n","\n","    model = Model.raw_aux_siamese(args)\n","    # model = raw_aux_siamese(args)\n","\n","    AUX=True\n","    args.gamma=4.0\n","else:\n","    sys.exit(\"invalid model name\")\n","\n","\n","if torch.cuda.device_count() >= 1:\n","    torch.cuda.manual_seed_all(1)\n","    dtype = torch.cuda.FloatTensor\n","    cuda.set_device(args.gpuid)\n","    model.type(dtype)\n","    print('Using GPU '+str(args.gpuid))\n","else:\n","    print(\"No GPU Available\")\n","    dtype = torch.FloatTensor\n","\n","## PRINTING MODEL USES SO MUCH SPACE WHY\n","#print(model)\n","\n","\n","if(args.test_on_saved_model==False):\n","    print(\"==>initializing a new model\")\n","    for p in model.parameters():\n","        p.data.uniform_(-0.1,0.1)\n","\n","DiffLoss = nn.MSELoss(size_average=True).type(dtype)\n","AuxLoss = nn.MSELoss(size_average=True).type(dtype)\n","ConLoss = ContrastiveLoss().type(dtype)\n","\n","optimizer = optim.Adam(model.parameters(), lr = args.lr)\n","#optimizer = optim.SGD(model.parameters(), lr = args.lr, momentum=args.momentum)\n","def train(TrainData):\n","    model.train()\n","    # initialize attention\n","    diff_targets = torch.zeros(TrainData.dataset.__len__(),1)\n","    diff_predictions = torch.zeros(diff_targets.size(0),1)\n","    if(args.model_name=='raw_d'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","    elif(args.model_name=='raw_c'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),3*args.n_hms)\n","\n","    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","\n","    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),5*args.n_hms)\n","\n","    else:\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","\n","    num_batches = int(math.ceil(TrainData.dataset.__len__()/float(args.batch_size)))\n","    all_gene_ids=[None]*TrainData.dataset.__len__()\n","    per_epoch_loss = 0\n","    for idx, Sample in enumerate(TrainData):\n","        if(idx%100==0):\n","            print('TRAINING ON BATCH:',idx)\n","        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, TrainData.dataset.__len__())\n","        optimizer.zero_grad()\n","        # get HM profiles\n","        inputs_1 = Sample['X_A']\n","        inputs_2 = Sample['X_B']\n","\n","\n","        # get targets: both differential and cell specific expression\n","        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","        diff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","        if(CON==True):\n","            # get labels for contrastive loss\n","            batch_contrastive_targets =[]\n","            for label in batch_diff_targets:\n","                if(label<=-2.0):\n","                    batch_contrastive_targets.append(1)\n","                elif(label>=2.0):\n","                    batch_contrastive_targets.append(1)\n","                else:\n","                    batch_contrastive_targets.append(0)\n","            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","        all_gene_ids[start:end]=Sample['geneID']\n","        batch_size = inputs_1.size(0)\n","\n","        if(AUX==False):\n","            # for raw models: raw_d, raw_c, raw\n","            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","        elif(CON==False):\n","            # for aux models\n","            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","        else:\n","            # for aux and siamese models\n","            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","        per_epoch_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n","        optimizer.step()\n","    per_epoch_loss=per_epoch_loss/num_batches\n","    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","def test(ValidData):\n","    model.eval()\n","\n","    diff_targets = torch.zeros(ValidData.dataset.__len__(),1)\n","    diff_predictions = torch.zeros(diff_targets.size(0),1)\n","    if(args.model_name=='raw_d'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","    elif(args.model_name=='raw_c'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),3*args.n_hms)\n","    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),5*args.n_hms)\n","    else:\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","\n","    num_batches = int(math.ceil(ValidData.dataset.__len__()/float(args.batch_size)))\n","    all_gene_ids=[None]*ValidData.dataset.__len__()\n","    per_epoch_loss = 0\n","    for idx, Sample in enumerate(ValidData):\n","        if(idx%100==0):\n","            print('TESTING ON BATCH:',idx)\n","        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, ValidData.dataset.__len__())\n","        optimizer.zero_grad()\n","        # get HM profiles\n","        inputs_1 = Sample['X_A']\n","        inputs_2 = Sample['X_B']\n","\n","        \n","        # get targets: both differential and cell specific expression\n","        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","        diff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","        if(CON==True):\n","            # get labels for contrastive loss\n","            batch_contrastive_targets =[]\n","            for label in batch_diff_targets:\n","                if(label<=-2.0):\n","                    batch_contrastive_targets.append(1)\n","                elif(label>=2.0):\n","                    batch_contrastive_targets.append(1)\n","                else:\n","                    batch_contrastive_targets.append(0)\n","            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","        all_gene_ids[start:end]=Sample['geneID']\n","        batch_size = inputs_1.size(0)\n","\n","        if(AUX==False):\n","            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","        elif(CON==False):\n","            # for aux models\n","            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","        else:\n","            # for aux and siamese models\n","            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","        per_epoch_loss += loss.item()\n","    per_epoch_loss=per_epoch_loss/num_batches\n","    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","    # Train = torch.utils.data.DataLoader(train_inputs, batch_size=args.batch_size, shuffle=True)\n","    # Valid = torch.utils.data.DataLoader(valid_inputs, batch_size=args.batch_size, shuffle=False)\n","    # Test = torch.utils.data.DataLoader(test_inputs, batch_size=args.batch_size, shuffle=False)\n","\n","\n","\n","\n","def dataSpecs(filename,windows,gene_dict, num_hms, name):\n","    print(\"==> Loading data\")\n","    # chromosomes to stratify by (just keep it to the main chr)\n","    hg38 = pd.read_csv(\"data/hg38_genes_chromosome.txt\", skiprows = 1, names = (\"gene\", \"chr\")) # map genes to chromosomes\n","    chromosomes = [str(j) for j in list(range(1, 23))] + [\"X\", \"Y\"]\n","\n","    with open(filename) as fi:\n","        csv_reader=csv.reader(fi)\n","        data = list(csv_reader)\n","    \n","        ncols=(len(data[0]))\n","    fi.close()\n","    \n","    hg38 = hg38[hg38[\"gene\"].isin(np.array(data)[:, 0])]\n","    hg38 = hg38[hg38[\"chr\"].isin(chromosomes)]\n","\n","    # can't cross validate if chromosome only occurs once\n","    for i in chromosomes:\n","      if int(list(hg38[\"chr\"]).count(i)) < 2:\n","        hg38 = hg38[hg38[\"chr\"] != i]\n","\n","    temp = []\n","    j = 0\n","    for i in np.array(data)[:, 0]:\n","      if i in np.array(hg38[\"gene\"]):\n","        temp.append(data[j])\n","      j += 1\n","    data = temp\n","\n","    nrows=len(data)\n","    ngenes=nrows/windows\n","    nfeatures=ncols-1\n","    \n","    # testing args.n_hms vs nfeatures\n","    if nfeatures != num_hms:\n","        print(\"nfeatures != num_hms\")\n","    assert(int(nfeatures) == int(num_hms))\n","   \n","    print(\"Number of genes: %d\" % ngenes)\n","    print(\"Number of entries: %d\" % nrows)\n","    print(\"Number of HMs: %d\" % nfeatures)\n","    return\n","\n","gene_dict=Data.loadDict(args.data_root+\"/\"+args.cell_1+\".expr.csv\")\n","dataSpecs(args.data_root+\"/\"+args.cell_1+\".csv\",\n","                              args.n_bins,gene_dict, args.n_hms, args.cell_1)\n","\n","args.data_root+\"/\"+args.cell_1+\".csv\"\n","\n","best_valid_loss = 10000000000\n","best_valid_MSE=100000\n","best_valid_R2=-1\n","\n","\n","if(args.test_on_saved_model==False):\n","\n","\n","    trials = 1    \n","    splits = 20 \n","    for split in range(0, splits):  \n","    # for epoch in range(0, arg.epochs):\n","        gene_dict1=Data.loadDict(args.data_root+\"/\"+args.cell_1+\".expr.csv\")\n","        gene_dict2=Data.loadDict(args.data_root+\"/\"+args.cell_2+\".expr.csv\")\n","\n","        c1_train, c1_test, c1_val, chr_train, chr_test, chr_val = Data.loadCell1(args.data_root+\"/\"+args.cell_1+\".csv\", \n","                                                                            args.n_bins,gene_dict1, args.n_hms) # n_hms assert \n","\n","        c2_train, c2_test, c2_val = Data.loadCell2(args.data_root+\"/\"+args.cell_2+\".csv\",\n","                                              args.n_bins,gene_dict2, args.n_hms, \n","                                              chr_train, chr_test, chr_val)\n","\n","        train_inpt = Data.HMData(c1_train, c2_train, args.n_hms) # added dynamic args.n_hms \n","        test_inpt = Data.HMData(c1_test, c2_test, args.n_hms)\n","        val_inpt = Data.HMData(c1_val, c2_val, args.n_hms)\n","\n","        Train = torch.utils.data.DataLoader(train_inpt, batch_size=args.batch_size, drop_last = False, shuffle=True)\n","        Valid = torch.utils.data.DataLoader(val_inpt, batch_size=args.batch_size, drop_last = False, shuffle=False)\n","        Test = torch.utils.data.DataLoader(test_inpt, batch_size=args.batch_size, drop_last = False, shuffle=False) \n","\n","        for runs in range(0, int(args.epochs / splits)):\n","            print('=---------------------------------------- Training ', trials, ' -----------------------------------=')\n","            diff_predictions,diff_targets,alpha_train,beta_train,train_loss,_ = train(Train)\n","            train_MSE, train_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","            diff_predictions,diff_targets,alpha_valid,beta_valid,valid_loss,gene_ids_valid = test(Valid)\n","            valid_MSE, valid_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","\n","            if(valid_R2 >= best_valid_R2):\n","                # save best epoch -- models converge early\n","                best_valid_R2=valid_R2\n","                torch.save(model,model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","            print(\"Epoch:\", trials - 1)\n","            print(\"train R2:\",train_R2)\n","            print(\"valid R2:\",valid_R2)\n","            print(\"best valid R2:\", best_valid_R2)\n","\n","            trials += 1\n","\n","    print(\"finished training!!\")\n","    print(\"best validation R2:\",best_valid_R2)\n","    print(\"testing\")\n","    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","    print(\"test R2:\",test_R2)\n","\n","\n","    if(args.save_attention_maps):\n","        attentionfile=open(attentionmapfile,'w')\n","        attentionfilewriter=csv.writer(attentionfile)\n","        beta_test=beta_test.numpy()\n","        for i in range(len(gene_ids_test)):\n","            gene_attention=[]\n","            gene_attention.append(gene_ids_test[i])\n","            for e in beta_test[i,:]:\n","                gene_attention.append(str(e))\n","            attentionfilewriter.writerow(gene_attention)\n","        attentionfile.close()\n","\n","\n","else:\n","    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","    print(\"test R2:\",test_R2)\n","\n","    if(args.save_attention_maps):\n","        attentionfile=open(attentionmapfile,'w')\n","        attentionfilewriter=csv.writer(attentionfile)\n","        beta_test=beta_test.numpy()\n","        for i in range(len(gene_ids_test)):\n","            gene_attention=[]\n","            gene_attention.append(gene_ids_test[i])\n","            for e in beta_test[i,:]:\n","                gene_attention.append(str(e))\n","            attentionfilewriter.writerow(gene_attention)\n","        attentionfile.close()\n"],"execution_count":0,"outputs":[]}]}