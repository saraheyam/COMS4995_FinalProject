{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_from_train_v3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"9SOq5xtzbpPa","colab_type":"code","colab":{}},"source":["import os\n","from google.colab import drive\n","import sys\n","import csv\n","import numpy as np\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EgOsmag8bxH5","colab_type":"code","outputId":"29e2597f-e43d-4245-e986-8359f729b49b","executionInfo":{"status":"ok","timestamp":1575858615245,"user_tz":300,"elapsed":50553,"user":{"displayName":"Sarah Yam","photoUrl":"","userId":"16306504212637033761"}},"colab":{"base_uri":"https://localhost:8080/","height":159}},"source":["### ONLY IF USING GOOGLE COLAB \n","drive.mount('/content/drive')\n","\n","print(os.getcwd())\n","os.chdir(\"/content/drive/My Drive/COMS4995_FinalProject\")\n","os.getcwd()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/COMS4995_FinalProject'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"2xuIF_MKlx0m","colab_type":"code","colab":{}},"source":["sys.argv = [\"--cell_1=gm_bulk\", # placeholder i think because -h messes it up\n","            \"--cell_1=gm_bulk\", \n","            \"--cell_2=k562_bulk\", \n","            \"--model_name=aux_siamese\", \n","            \"--epochs=40\", \n","            \"--lr=0.001\", \n","            \"--data_root=data/Bulk/\", \n","            \"--save_root=Results/\", \n","            \"--n_bins=1\", \n","            \"--n_hms=11\",\n","            \"--gpu=1\"]#,\n","            # \"--attentionfilename=gm_k562_full_40ep.csv\", \n","            # \"--save_attention_maps\"]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1X-IlnF1hq2C","colab_type":"code","colab":{}},"source":["# data\n","import torch\n","import collections\n","import pdb\n","import torch.utils.data\n","import csv\n","import json\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import math\n","import numpy as np\n","\n","# train \n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","import argparse\n","import json\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import cuda\n","import sys, os\n","import random\n","import numpy as np\n","from sklearn import metrics\n","import models as Model\n","from SiameseLoss import ContrastiveLoss\n","import evaluate\n","import data_v2\n","import gc\n","import csv"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k87GE2Xl0aYK","colab_type":"text"},"source":["#### The HMData class object has issues with indexing caching and memory address if you call load_data() instead of the individual functions. This seems to occur on both cloud-based services (i.e. Jupyter and Colab) as well as local terminal based operations (i.e. linux OS). Will likely have to switch over to individual calls."]},{"cell_type":"code","metadata":{"id":"MXXzRx6qI23P","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.model_selection import GroupKFold\n","import random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W6Fr9MWzIcg8","colab_type":"code","colab":{}},"source":["chr_class = []\n","gene_name = []\n","[chr_class.append(thing[i][\"chr\"]) for i in range(len(thing))]\n","[gene_name.append(thing[i][\"geneID\"]) for i in range(len(thing))]\n","indexer = np.column_stack((gene_name, chr_class))\n","chroms = np.unique(chr_class)\n","\n","sss = StratifiedShuffleSplit(n_splits = 2, test_size = 0.2, random_state = 0)\n","for train_index, test_index in sss.split(indexer, chr_class):\n","  print(indexer[train_index])\n","  print([chr_class[i] for i in test_index])\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tV8-MPHBsPPq","colab_type":"code","colab":{}},"source":["def getlabel(c1,c2):\n","    # get log fold change of expression\n","\n","    label1=math.log((float(c1)+1.0),2)\n","    label2=math.log((float(c2)+1.0),2)\n","    label=[]\n","    label.append(label1)\n","    label.append(label2)\n","\n","    fold_change=(float(c2)+1.0)/(float(c1)+1.0)\n","    log_fold_change=math.log((fold_change),2)\n","    return (log_fold_change, label)\n","\n","def loadDict(filename):\n","    # get expression value of each gene from cell*.expr.csv\n","    gene_dict={}\n","    with open(filename) as fi:\n","        for line in fi:\n","            geneID,geneExpr=line.split(',')\n","            gene_dict[str(geneID)]=float(geneExpr)\n","    fi.close()\n","    return(gene_dict)\n","\n","\n","def loadCell1(filename,windows,gene_dict, num_hms):\n","    \n","    # chromosomes to stratify by (just keep it to the main chr)\n","    hg38 = pd.read_csv(\"data/hg38_genes_chromosome.txt\", skiprows = 1, names = (\"gene\", \"chr\")) # map genes to chromosomes\n","    chromosomes = [str(j) for j in list(range(1, 23))] + [\"X\", \"Y\"]\n","\n","    with open(filename) as fi:\n","        csv_reader=csv.reader(fi)\n","        data = list(csv_reader)\n","    \n","        ncols=(len(data[0]))\n","    fi.close()\n","    \n","    \n","    hg38 = hg38[hg38[\"gene\"].isin(np.array(data)[:, 0])]\n","    hg38 = hg38[hg38[\"chr\"].isin(chromosomes)]\n","\n","    # can't cross validate if chromosome only occurs once\n","    for i in chromosomes:\n","      if int(list(hg38[\"chr\"]).count(i)) < 2:\n","        hg38 = hg38[hg38[\"chr\"] != i]\n","\n","    chroms = list(np.unique(np.array(hg38[\"chr\"])))\n","    random.shuffle(chroms)\n","    chrom_train = chroms[0:int(0.6 * len(chroms))]\n","    chrom_test = chroms[int(0.6 * len(chroms)):int(0.9 * len(chroms))]\n","    chrom_val = chroms[int(0.9 * len(chroms)):]\n","\n","    temp = []\n","    j = 0\n","    for i in np.array(data)[:, 0]:\n","      if i in np.array(hg38[\"gene\"]):\n","        temp.append(data[j])\n","      j += 1\n","    data = temp\n","\n","    nrows=len(data)\n","    ngenes=nrows/windows\n","    nfeatures=ncols-1    \n","\n","    train_num = 0\n","    test_num = 0\n","    val_num = 0\n","    attr_train = collections.OrderedDict()\n","    attr_test = collections.OrderedDict()\n","    attr_val = collections.OrderedDict()\n","    alph = [chr(x) for x in range(ord('a'), ord('z') + 1)] # alphabetical key order more stable\n","\n","    gene_keys = [\"geneID\", \"expr\"]\n","    hm_id = [\"hm_\" + alph[i] for i in list(range(nfeatures))]\n","    data_mat = np.array(data)\n","\n","    random.shuffle(chroms)\n","    chrom_train = chroms[0:int(0.6 * len(chroms))]\n","    chrom_test = chroms[int(0.6 * len(chroms)):int(0.9 * len(chroms))]\n","    chrom_val = chroms[int(0.9 * len(chroms)):]      \n","\n","    for i in range(0, nrows, windows):\n","        geneID=str(data[i][0].split(\"_\")[0])\n","    \n","        meta = {}\n","        meta[\"geneID\"] = geneID\n","        meta[\"chr\"] = hg38[hg38[\"gene\"] == geneID].iloc[0][\"chr\"]\n","        meta[\"expr\"] = gene_dict[geneID]\n","    \n","        for j in range(nfeatures):\n","            meta[hm_id[j]] = torch.tensor(np.array([float(z) for z in data_mat[i:(i+windows), j+1]]).reshape(windows, 1))\n","        \n","        if meta[\"chr\"] in chrom_train:\n","          attr_train[train_num] = meta\n","          train_num += 1\n","\n","        elif meta[\"chr\"] in chrom_test:\n","          attr_test[test_num] = meta\n","          test_num += 1\n","\n","        else:\n","          attr_val[val_num] = meta\n","          val_num += 1   \n","        \n","    return attr_train, attr_test, attr_val, chrom_train, chrom_test, chrom_val\n","\n","\n","def loadCell2(filename,windows,gene_dict, num_hms, chrom_train, chrom_test, chrom_val):\n","    \n","    # chromosomes to stratify by (just keep it to the main chr)\n","    hg38 = pd.read_csv(\"data/hg38_genes_chromosome.txt\", skiprows = 1, names = (\"gene\", \"chr\")) # map genes to chromosomes\n","    chromosomes = [str(j) for j in list(range(1, 23))] + [\"X\", \"Y\"]\n","\n","    with open(filename) as fi:\n","        csv_reader=csv.reader(fi)\n","        data = list(csv_reader)\n","    \n","        ncols=(len(data[0]))\n","    fi.close()\n","    \n","    \n","    hg38 = hg38[hg38[\"gene\"].isin(np.array(data)[:, 0])]\n","    hg38 = hg38[hg38[\"chr\"].isin(chromosomes)]\n","\n","    # can't cross validate if chromosome only occurs once\n","    for i in chromosomes:\n","      if int(list(hg38[\"chr\"]).count(i)) < 2:\n","        hg38 = hg38[hg38[\"chr\"] != i]\n","\n","    temp = []\n","    j = 0\n","    for i in np.array(data)[:, 0]:\n","      if i in np.array(hg38[\"gene\"]):\n","        temp.append(data[j])\n","      j += 1\n","    data = temp\n","\n","    nrows=len(data)\n","    ngenes=nrows/windows\n","    nfeatures=ncols-1    \n","\n","    train_num = 0\n","    test_num = 0\n","    val_num = 0\n","    attr_train = collections.OrderedDict()\n","    attr_test = collections.OrderedDict()\n","    attr_val = collections.OrderedDict()\n","    alph = [chr(x) for x in range(ord('a'), ord('z') + 1)] # alphabetical key order more stable\n","\n","    gene_keys = [\"geneID\", \"expr\"]\n","    hm_id = [\"hm_\" + alph[i] for i in list(range(nfeatures))]\n","    data_mat = np.array(data)\n","     \n","    for i in range(0, nrows, windows):\n","        geneID=str(data[i][0].split(\"_\")[0])\n","    \n","        meta = {}\n","        meta[\"geneID\"] = geneID\n","        meta[\"chr\"] = hg38[hg38[\"gene\"] == geneID].iloc[0][\"chr\"]\n","        meta[\"expr\"] = gene_dict[geneID]\n","    \n","        for j in range(nfeatures):\n","            meta[hm_id[j]] = torch.tensor(np.array([float(z) for z in data_mat[i:(i+windows), j+1]]).reshape(windows, 1))\n","        \n","        if meta[\"chr\"] in chrom_train:\n","          attr_train[train_num] = meta\n","          train_num += 1\n","\n","        elif meta[\"chr\"] in chrom_test:\n","          attr_test[test_num] = meta\n","          test_num += 1\n","\n","        else:\n","          attr_val[val_num] = meta\n","          val_num += 1   \n","        \n","    return attr_train, attr_test, attr_val\n","\n","\n","\n","def commonGenes(c1, c2):\n","    set1 = set(list(pd.DataFrame.from_dict(c1, orient = \"index\")[\"geneID\"]))\n","    set2 = set(list(pd.DataFrame.from_dict(c2, orient = \"index\")[\"geneID\"]))\n","\n","    df1 = pd.DataFrame.from_dict(c1, orient = \"index\")\n","    df2 = pd.DataFrame.from_dict(c2, orient = \"index\")\n","\n","    if (set1 & set2):\n","        common = list(set1 & set2)\n","    df1 = df1[df1[\"geneID\"].isin(common)]\n","    df2 = df2[df2[\"geneID\"].isin(common)]\n","\n","    return df1, df2\n","\n","class HMData(Dataset):\n","    # Dataset class for loading data\n","    def __init__(self,data_cell1,data_cell2, n_feat, transform = None): \n","        self.c1=data_cell1\n","        self.c2=data_cell2\n","        self.nfeat = n_feat\n","        assert (len(self.c1)==len(self.c2))\n","    def __len__(self):\n","        return len(self.c1)\n","    def __getitem__(self,i):\n","                 \n","        alph = [chr(x) for x in range(ord('a'), ord('z') + 1)]\n","        hm_id = [\"hm_\" + alph[y] for y in list(range(self.nfeat))]\n","      \n","        final_data_c1 = torch.cat([self.c1[i][z] for z in hm_id], 1) \n","        final_data_c2 = torch.cat([self.c2[i][w] for w in hm_id], 1)         \n","                \n","        label,orig_label=getlabel(self.c1[i]['expr'],self.c2[i]['expr'])\n","        b_label_c1=orig_label[0]\n","        b_label_c2=orig_label[1]\n","        assert self.c1[i]['geneID']==self.c2[i]['geneID']\n","        geneID=self.c1[i]['geneID']\n","        chromosome = self.c1[i][\"chr\"]\n","        sample={'geneID':geneID,\n","                'chr':chromosome,\n","               'X_A':final_data_c1,\n","               'X_B':final_data_c2,\n","               'diff':label,\n","               'abs_A':b_label_c1,'abs_B':b_label_c2}\n","        return sample\n","    \n","### NEED TO PASS args.n_hms TO HMDATA FOR nfeat ARGUMENT    \n","#  assert that nfeatures == args.n_hms\n","def load_data(args):\n","    '''\n","    Loads data into a 3D tensor for processing before cross validation.\n","\n","    '''\n","    gene_dict1=loadDict(args.data_root+\"/\"+args.cell_1+\".expr.csv\")\n","    gene_dict2=loadDict(args.data_root+\"/\"+args.cell_2+\".expr.csv\")\n","    \n","    c1_train, c1_test, c1_val, chr_train, chr_test, chr_val = loadCell1(args.data_root+\"/\"+args.cell_1+\".csv\",\n","                              args.n_bins,gene_dict1, args.n_hms) # n_hms assert \n","\n","    c2_train, c2_test, c2_val = loadCell2(args.data_root+\"/\"+args.cell_2+\".csv\",                              \n","                              args.n_bins,gene_dict2, args.n_hms, \n","                              chr_train, chr_test, chr_val) # n_hms assert\n","\n","    c1_train, c2_train = commonGenes(c1_train, c2_train)\n","    c1_test, c2_test = commonGenes(c1_test, c2_test)\n","    c1_val, c2_val = commonGenes(c1_val, c2_val)\n","\n","    train = HMData(c1_train, c2_train, args.n_hms) # added dynamic args.n_hms \n","    test = HMData(c1_test, c2_test, args.n_hms)\n","    val = HMData(c1_val, c2_val, args.n_hms)\n","      \n","    return train, test, val # return inputs to trainloading/cross validation split later\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBDU-ssT8J_L","colab_type":"code","colab":{}},"source":["gene_dict1=loadDict(args.data_root+\"/\"+args.cell_1+\".expr.csv\")\n","gene_dict2=loadDict(args.data_root+\"/\"+args.cell_2+\".expr.csv\")\n","\n","c1_train, c1_test, c1_val, chr_train, chr_test, chr_val = loadCell1(args.data_root+\"/\"+args.cell_1+\".csv\",\n","                              args.n_bins,gene_dict1, args.n_hms) # n_hms assert \n","\n","c2_train, c2_test, c2_val = loadCell2(args.data_root+\"/\"+args.cell_2+\".csv\",                              \n","                              args.n_bins,gene_dict2, args.n_hms, \n","                              chr_train, chr_test, chr_val)\n","\n","tr = HMData(c1_train, c2_train, args.n_hms) # added dynamic args.n_hms \n","tst = HMData(c1_test, c2_test, args.n_hms)\n","vl = HMData(c1_val, c2_val, args.n_hms)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"latoAUZfM46N","colab_type":"code","colab":{}},"source":["tr = HMData(c1_train, c2_train, args.n_hms) # added dynamic args.n_hms \n","tst = HMData(c1_test, c2_test, args.n_hms)\n","vl = HMData(c1_val, c2_val, args.n_hms)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvOsrQqbjgG3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"fe3481f6-b59c-4195-ddfa-321fc21a7bb9","executionInfo":{"status":"ok","timestamp":1575863231533,"user_tz":300,"elapsed":4556419,"user":{"displayName":"Sarah Yam","photoUrl":"","userId":"16306504212637033761"}}},"source":["parser = argparse.ArgumentParser(description='DeepDiff')\n","parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate')\n","parser.add_argument('--model_name', type=str, default='raw_d', help='DeepDiff variation')\n","parser.add_argument('--clip', type=float, default=1,help='gradient clipping')\n","parser.add_argument('--epochs', type=int, default=90, help='upper epoch limit')\n","parser.add_argument('--batch_size', type=int, default=10, help='')\n","parser.add_argument('--dropout', type=float, default=0.5, help='dropout applied to layers (0 = no dropout) if n_layers LSTM > 1')\n","parser.add_argument('--cell_1', type=str, default='Cell1', help='cell type 1')\n","parser.add_argument('--cell_2', type=str, default='Cell2', help='cell type 2')\n","parser.add_argument('--save_root', type=str, default='./Results/', help='where to save')\n","parser.add_argument('--data_root', type=str, default='./data/', help='data location')\n","parser.add_argument('--gpuid', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--gpu', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--n_hms', type=int, default=5, help='number of histone modifications')\n","parser.add_argument('--n_bins', type=int, default=1, help='number of bins')\n","parser.add_argument('--bin_rnn_size', type=int, default=32, help='bin rnn size')\n","parser.add_argument('--num_layers', type=int, default=1, help='number of layers')\n","parser.add_argument('--unidirectional', action='store_true', help='bidirectional/undirectional LSTM')\n","parser.add_argument('--save_attention_maps',action='store_true', help='set to save validation beta attention maps')\n","parser.add_argument('--attentionfilename', type=str, default='beta_attention.txt', help='where to save attnetion maps')\n","parser.add_argument('--test_on_saved_model',action='store_true', help='only test on saved model')\n","args = parser.parse_args()\n","\n","torch.manual_seed(1)\n","\n","model_name = ''\n","model_name += (args.cell_1)+('_')+(args.cell_2)+('_')\n","\n","model_name+=args.model_name\n","\n","\n","\n","\n","args.bidirectional=not args.unidirectional\n","\n","print('the model name: ',model_name)\n","args.data_root+=''\n","args.save_root+=''\n","args.dataset=args.cell_1+('_')+args.cell_2\n","args.data_root = os.path.join(args.data_root)\n","print('loading data from:  ',args.data_root)\n","args.save_root = os.path.join(args.save_root,args.dataset)\n","print('saving results in  from: ',args.save_root)\n","model_dir = os.path.join(args.save_root,model_name)\n","if not os.path.exists(model_dir):\n","    os.makedirs(model_dir)\n","attentionmapfile=model_dir+'/'+args.attentionfilename\n","print('==>processing data')\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","CON=False\n","AUX=False\n","print('==>building model')\n","if(args.model_name=='raw_d'):\n","    model = Model.raw_d(args)\n","    # model = raw_d(args)\n","\n","elif(args.model_name=='raw_c'):\n","    model = Model.raw_c(args)\n","    # model = raw_c(args)\n","\n","elif(args.model_name=='raw'):\n","    model = Model.raw(args)\n","    # model = raw(args)\n","\n","elif(args.model_name=='aux'):\n","    args.shared=False\n","    \n","    model = Model.aux(args)\n","    # model = aux(args)\n","\n","    AUX=True\n","    args.gamma=0.0\n","elif(args.model_name=='raw_aux'):\n","    args.shared=False\n","\n","    model = Model.raw_aux(args)\n","    # model = raw_aux(args)\n","    \n","    AUX=True\n","    args.gamma=0.0\n","elif(args.model_name=='aux_siamese'):\n","    CON=True\n","    args.shared=True\n","    \n","    model = Model.aux_siamese(args)\n","    # model = aux_siamese(args)\n","    \n","    AUX=True\n","    args.gamma=4.0\n","elif(args.model_name=='raw_aux_siamese'):\n","    CON=True\n","    args.shared=True\n","\n","    model = Model.raw_aux_siamese(args)\n","    # model = raw_aux_siamese(args)\n","\n","    AUX=True\n","    args.gamma=4.0\n","else:\n","    sys.exit(\"invalid model name\")\n","\n","\n","if torch.cuda.device_count() >= 1:\n","    torch.cuda.manual_seed_all(1)\n","    dtype = torch.cuda.FloatTensor\n","    cuda.set_device(args.gpuid)\n","    model.type(dtype)\n","    print('Using GPU '+str(args.gpuid))\n","else:\n","    print(\"No GPU Available\")\n","    dtype = torch.FloatTensor\n","\n","## PRINTING MODEL USES SO MUCH SPACE WHY\n","#print(model)\n","\n","\n","if(args.test_on_saved_model==False):\n","    print(\"==>initializing a new model\")\n","    for p in model.parameters():\n","        p.data.uniform_(-0.1,0.1)\n","\n","DiffLoss = nn.MSELoss(size_average=True).type(dtype)\n","AuxLoss = nn.MSELoss(size_average=True).type(dtype)\n","ConLoss = ContrastiveLoss().type(dtype)\n","\n","optimizer = optim.Adam(model.parameters(), lr = args.lr)\n","#optimizer = optim.SGD(model.parameters(), lr = args.lr, momentum=args.momentum)\n","def train(TrainData):\n","    model.train()\n","    # initialize attention\n","    diff_targets = torch.zeros(TrainData.dataset.__len__(),1)\n","    diff_predictions = torch.zeros(diff_targets.size(0),1)\n","    if(args.model_name=='raw_d'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","    elif(args.model_name=='raw_c'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),3*args.n_hms)\n","\n","    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","\n","    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),5*args.n_hms)\n","\n","    else:\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","\n","    num_batches = int(math.ceil(TrainData.dataset.__len__()/float(args.batch_size)))\n","    all_gene_ids=[None]*TrainData.dataset.__len__()\n","    per_epoch_loss = 0\n","    for idx, Sample in enumerate(TrainData):\n","        if(idx%100==0):\n","            print('TRAINING ON BATCH:',idx)\n","        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, TrainData.dataset.__len__())\n","        optimizer.zero_grad()\n","        # get HM profiles\n","        inputs_1 = Sample['X_A']\n","        inputs_2 = Sample['X_B']\n","\n","\n","        # get targets: both differential and cell specific expression\n","        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","        diff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","        if(CON==True):\n","            # get labels for contrastive loss\n","            batch_contrastive_targets =[]\n","            for label in batch_diff_targets:\n","                if(label<=-2.0):\n","                    batch_contrastive_targets.append(1)\n","                elif(label>=2.0):\n","                    batch_contrastive_targets.append(1)\n","                else:\n","                    batch_contrastive_targets.append(0)\n","            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","        all_gene_ids[start:end]=Sample['geneID']\n","        batch_size = inputs_1.size(0)\n","\n","        if(AUX==False):\n","            # for raw models: raw_d, raw_c, raw\n","            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","        elif(CON==False):\n","            # for aux models\n","            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","        else:\n","            # for aux and siamese models\n","            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","        per_epoch_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n","        optimizer.step()\n","    per_epoch_loss=per_epoch_loss/num_batches\n","    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","def test(ValidData):\n","    model.eval()\n","\n","    diff_targets = torch.zeros(ValidData.dataset.__len__(),1)\n","    diff_predictions = torch.zeros(diff_targets.size(0),1)\n","    if(args.model_name=='raw_d'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","    elif(args.model_name=='raw_c'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),3*args.n_hms)\n","    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),5*args.n_hms)\n","    else:\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","\n","    num_batches = int(math.ceil(ValidData.dataset.__len__()/float(args.batch_size)))\n","    all_gene_ids=[None]*ValidData.dataset.__len__()\n","    per_epoch_loss = 0\n","    for idx, Sample in enumerate(ValidData):\n","        if(idx%100==0):\n","            print('TESTING ON BATCH:',idx)\n","        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, ValidData.dataset.__len__())\n","        optimizer.zero_grad()\n","        # get HM profiles\n","        inputs_1 = Sample['X_A']\n","        inputs_2 = Sample['X_B']\n","\n","        \n","        # get targets: both differential and cell specific expression\n","        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","        diff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","        if(CON==True):\n","            # get labels for contrastive loss\n","            batch_contrastive_targets =[]\n","            for label in batch_diff_targets:\n","                if(label<=-2.0):\n","                    batch_contrastive_targets.append(1)\n","                elif(label>=2.0):\n","                    batch_contrastive_targets.append(1)\n","                else:\n","                    batch_contrastive_targets.append(0)\n","            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","        all_gene_ids[start:end]=Sample['geneID']\n","        batch_size = inputs_1.size(0)\n","\n","        if(AUX==False):\n","            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","        elif(CON==False):\n","            # for aux models\n","            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","        else:\n","            # for aux and siamese models\n","            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","        per_epoch_loss += loss.item()\n","    per_epoch_loss=per_epoch_loss/num_batches\n","    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","    # Train = torch.utils.data.DataLoader(train_inputs, batch_size=args.batch_size, shuffle=True)\n","    # Valid = torch.utils.data.DataLoader(valid_inputs, batch_size=args.batch_size, shuffle=False)\n","    # Test = torch.utils.data.DataLoader(test_inputs, batch_size=args.batch_size, shuffle=False)\n","\n","\n","\n","\n","def dataSpecs(filename,windows,gene_dict, num_hms, name):\n","    print(\"==> Loading data\")\n","    # chromosomes to stratify by (just keep it to the main chr)\n","    hg38 = pd.read_csv(\"data/hg38_genes_chromosome.txt\", skiprows = 1, names = (\"gene\", \"chr\")) # map genes to chromosomes\n","    chromosomes = [str(j) for j in list(range(1, 23))] + [\"X\", \"Y\"]\n","\n","    with open(filename) as fi:\n","        csv_reader=csv.reader(fi)\n","        data = list(csv_reader)\n","    \n","        ncols=(len(data[0]))\n","    fi.close()\n","    \n","    hg38 = hg38[hg38[\"gene\"].isin(np.array(data)[:, 0])]\n","    hg38 = hg38[hg38[\"chr\"].isin(chromosomes)]\n","\n","    # can't cross validate if chromosome only occurs once\n","    for i in chromosomes:\n","      if int(list(hg38[\"chr\"]).count(i)) < 2:\n","        hg38 = hg38[hg38[\"chr\"] != i]\n","\n","    temp = []\n","    j = 0\n","    for i in np.array(data)[:, 0]:\n","      if i in np.array(hg38[\"gene\"]):\n","        temp.append(data[j])\n","      j += 1\n","    data = temp\n","\n","    nrows=len(data)\n","    ngenes=nrows/windows\n","    nfeatures=ncols-1\n","    \n","    # testing args.n_hms vs nfeatures\n","    if nfeatures != num_hms:\n","        print(\"nfeatures != num_hms\")\n","    assert(int(nfeatures) == int(num_hms))\n","   \n","    print(\"Number of genes: %d\" % ngenes)\n","    print(\"Number of entries: %d\" % nrows)\n","    print(\"Number of HMs: %d\" % nfeatures)\n","    return\n","\n","gene_dict=loadDict(args.data_root+\"/\"+args.cell_1+\".expr.csv\")\n","dataSpecs(args.data_root+\"/\"+args.cell_1+\".csv\",\n","                              args.n_bins,gene_dict, args.n_hms, args.cell_1)\n","\n","args.data_root+\"/\"+args.cell_1+\".csv\"\n","\n","best_valid_loss = 10000000000\n","best_valid_MSE=100000\n","best_valid_R2=-1\n","\n","\n","if(args.test_on_saved_model==False):\n","\n","\n","    trials = 1    \n","    splits = 20 \n","    for split in range(0, splits):  \n","    # for epoch in range(0, arg.epochs):\n","        gene_dict1=loadDict(args.data_root+\"/\"+args.cell_1+\".expr.csv\")\n","        gene_dict2=loadDict(args.data_root+\"/\"+args.cell_2+\".expr.csv\")\n","\n","        c1_train, c1_test, c1_val, chr_train, chr_test, chr_val = loadCell1(args.data_root+\"/\"+args.cell_1+\".csv\", \n","                                                                            args.n_bins,gene_dict1, args.n_hms) # n_hms assert \n","\n","        c2_train, c2_test, c2_val = loadCell2(args.data_root+\"/\"+args.cell_2+\".csv\",\n","                                              args.n_bins,gene_dict2, args.n_hms, \n","                                              chr_train, chr_test, chr_val)\n","\n","        tr = HMData(c1_train, c2_train, args.n_hms) # added dynamic args.n_hms \n","        tst = HMData(c1_test, c2_test, args.n_hms)\n","        vl = HMData(c1_val, c2_val, args.n_hms)\n","\n","        Train = torch.utils.data.DataLoader(tr, batch_size=args.batch_size, drop_last = False, shuffle=True)\n","        Valid = torch.utils.data.DataLoader(vl, batch_size=args.batch_size, drop_last = False, shuffle=False)\n","        Test = torch.utils.data.DataLoader(tst, batch_size=args.batch_size, drop_last = False, shuffle=False) # dropping last for dynamic data sizes\n","\n","        for runs in range(0, int(args.epochs / splits)):\n","            print('=---------------------------------------- Training ', trials, ' -----------------------------------=')\n","            diff_predictions,diff_targets,alpha_train,beta_train,train_loss,_ = train(Train)\n","            train_MSE, train_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","            diff_predictions,diff_targets,alpha_valid,beta_valid,valid_loss,gene_ids_valid = test(Valid)\n","            valid_MSE, valid_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","\n","            if(valid_R2 >= best_valid_R2):\n","                # save best epoch -- models converge early\n","                best_valid_R2=valid_R2\n","                torch.save(model,model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","            print(\"Epoch:\", trials - 1)\n","            print(\"train R2:\",train_R2)\n","            print(\"valid R2:\",valid_R2)\n","            print(\"best valid R2:\", best_valid_R2)\n","\n","            trials += 1\n","\n","    print(\"finished training!!\")\n","    print(\"best validation R2:\",best_valid_R2)\n","    print(\"testing\")\n","    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","    print(\"test R2:\",test_R2)\n","\n","\n","    if(args.save_attention_maps):\n","        attentionfile=open(attentionmapfile,'w')\n","        attentionfilewriter=csv.writer(attentionfile)\n","        beta_test=beta_test.numpy()\n","        for i in range(len(gene_ids_test)):\n","            gene_attention=[]\n","            gene_attention.append(gene_ids_test[i])\n","            for e in beta_test[i,:]:\n","                gene_attention.append(str(e))\n","            attentionfilewriter.writerow(gene_attention)\n","        attentionfile.close()\n","\n","\n","else:\n","    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","    print(\"test R2:\",test_R2)\n","\n","    if(args.save_attention_maps):\n","        attentionfile=open(attentionmapfile,'w')\n","        attentionfilewriter=csv.writer(attentionfile)\n","        beta_test=beta_test.numpy()\n","        for i in range(len(gene_ids_test)):\n","            gene_attention=[]\n","            gene_attention.append(gene_ids_test[i])\n","            for e in beta_test[i,:]:\n","                gene_attention.append(str(e))\n","            attentionfilewriter.writerow(gene_attention)\n","        attentionfile.close()\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["the model name:  gm_bulk_k562_bulk_aux_siamese\n","loading data from:   data/Bulk/\n","saving results in  from:  Results/gm_bulk_k562_bulk\n","==>processing data\n","==>building model\n","Using GPU 0\n","==>initializing a new model\n","==> Loading data\n","Number of genes: 11588\n","Number of entries: 11588\n","Number of HMs: 11\n","=---------------------------------------- Training  1  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 0\n","train R2: -0.019260169440663277\n","valid R2: 0.042970669681286634\n","best valid R2: 0.042970669681286634\n","=---------------------------------------- Training  2  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 1\n","train R2: 0.0461064386250605\n","valid R2: 0.048508735356185856\n","best valid R2: 0.048508735356185856\n","=---------------------------------------- Training  3  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","Epoch: 2\n","train R2: 0.05626844088271812\n","valid R2: 0.12082124224528025\n","best valid R2: 0.12082124224528025\n","=---------------------------------------- Training  4  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","Epoch: 3\n","train R2: 0.054199877287344704\n","valid R2: 0.11457710305957283\n","best valid R2: 0.12082124224528025\n","=---------------------------------------- Training  5  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 4\n","train R2: 0.07090969090445863\n","valid R2: 0.07718936168638302\n","best valid R2: 0.12082124224528025\n","=---------------------------------------- Training  6  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 5\n","train R2: 0.07843697141161049\n","valid R2: 0.07608728718124112\n","best valid R2: 0.12082124224528025\n","=---------------------------------------- Training  7  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 6\n","train R2: 0.06188905072148397\n","valid R2: 0.09023100322226248\n","best valid R2: 0.12082124224528025\n","=---------------------------------------- Training  8  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 7\n","train R2: 0.06697846209296214\n","valid R2: 0.10568544526173812\n","best valid R2: 0.12082124224528025\n","=---------------------------------------- Training  9  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","TESTING ON BATCH: 200\n","Epoch: 8\n","train R2: 0.08959975859138554\n","valid R2: 0.0699303699154054\n","best valid R2: 0.12082124224528025\n","=---------------------------------------- Training  10  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","TESTING ON BATCH: 200\n","Epoch: 9\n","train R2: 0.09355975584854986\n","valid R2: 0.08736804761704553\n","best valid R2: 0.12082124224528025\n","=---------------------------------------- Training  11  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 10\n","train R2: 0.11551707465371848\n","valid R2: 0.1313463046316409\n","best valid R2: 0.1313463046316409\n","=---------------------------------------- Training  12  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 11\n","train R2: 0.1289038610655858\n","valid R2: 0.14105161021430482\n","best valid R2: 0.14105161021430482\n","=---------------------------------------- Training  13  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 12\n","train R2: 0.14902938393391374\n","valid R2: 0.1674716808895492\n","best valid R2: 0.1674716808895492\n","=---------------------------------------- Training  14  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 13\n","train R2: 0.1651933130911267\n","valid R2: 0.17485819987273848\n","best valid R2: 0.17485819987273848\n","=---------------------------------------- Training  15  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","Epoch: 14\n","train R2: 0.17285790308756335\n","valid R2: 0.21141788425366898\n","best valid R2: 0.21141788425366898\n","=---------------------------------------- Training  16  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","Epoch: 15\n","train R2: 0.1914280313212294\n","valid R2: 0.22890252881050632\n","best valid R2: 0.22890252881050632\n","=---------------------------------------- Training  17  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 16\n","train R2: 0.20598663331845599\n","valid R2: 0.2637003796138815\n","best valid R2: 0.2637003796138815\n","=---------------------------------------- Training  18  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 17\n","train R2: 0.2303746955863198\n","valid R2: 0.2820047801294524\n","best valid R2: 0.2820047801294524\n","=---------------------------------------- Training  19  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 18\n","train R2: 0.24588607157604206\n","valid R2: 0.23885511011613067\n","best valid R2: 0.2820047801294524\n","=---------------------------------------- Training  20  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 19\n","train R2: 0.2429294629421848\n","valid R2: 0.26223131356919577\n","best valid R2: 0.2820047801294524\n","=---------------------------------------- Training  21  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 20\n","train R2: 0.2551092810538804\n","valid R2: 0.2771452335637684\n","best valid R2: 0.2820047801294524\n","=---------------------------------------- Training  22  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 21\n","train R2: 0.25838535612652463\n","valid R2: 0.2836807197287299\n","best valid R2: 0.2836807197287299\n","=---------------------------------------- Training  23  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 22\n","train R2: 0.25852423093523563\n","valid R2: 0.28404160613321605\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  24  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 23\n","train R2: 0.26895026539412986\n","valid R2: 0.28361436410927865\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  25  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 24\n","train R2: 0.258789940653248\n","valid R2: 0.2765637381938425\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  26  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 25\n","train R2: 0.26347805670143426\n","valid R2: 0.2746460539213177\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  27  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 26\n","train R2: 0.27300509980466237\n","valid R2: 0.28334707256242775\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  28  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 27\n","train R2: 0.27806504235050766\n","valid R2: 0.2797505200690642\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  29  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 28\n","train R2: 0.26929996842665876\n","valid R2: 0.2535603632548432\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  30  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 29\n","train R2: 0.2686739354884825\n","valid R2: 0.24301489583651126\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  31  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 30\n","train R2: 0.2694889887583575\n","valid R2: 0.2825468948168898\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  32  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 31\n","train R2: 0.26753390348923095\n","valid R2: 0.2835823985503482\n","best valid R2: 0.28404160613321605\n","=---------------------------------------- Training  33  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 32\n","train R2: 0.269490578714055\n","valid R2: 0.2940151532460446\n","best valid R2: 0.2940151532460446\n","=---------------------------------------- Training  34  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 33\n","train R2: 0.2738214140267908\n","valid R2: 0.28702371146727845\n","best valid R2: 0.2940151532460446\n","=---------------------------------------- Training  35  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 34\n","train R2: 0.27625428099367205\n","valid R2: 0.2779460013379053\n","best valid R2: 0.2940151532460446\n","=---------------------------------------- Training  36  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 35\n","train R2: 0.277883775005211\n","valid R2: 0.2789307827510353\n","best valid R2: 0.2940151532460446\n","=---------------------------------------- Training  37  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 36\n","train R2: 0.2801272758829803\n","valid R2: 0.2427567889077399\n","best valid R2: 0.2940151532460446\n","=---------------------------------------- Training  38  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 37\n","train R2: 0.28746131379183676\n","valid R2: 0.23340548459289912\n","best valid R2: 0.2940151532460446\n","=---------------------------------------- Training  39  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 38\n","train R2: 0.27855360809712787\n","valid R2: 0.302897564420274\n","best valid R2: 0.302897564420274\n","=---------------------------------------- Training  40  -----------------------------------=\n","TRAINING ON BATCH: 0\n","TRAINING ON BATCH: 100\n","TRAINING ON BATCH: 200\n","TRAINING ON BATCH: 300\n","TRAINING ON BATCH: 400\n","TRAINING ON BATCH: 500\n","TRAINING ON BATCH: 600\n","TRAINING ON BATCH: 700\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","Epoch: 39\n","train R2: 0.2828679352273256\n","valid R2: 0.30172999091450897\n","best valid R2: 0.302897564420274\n","finished training!!\n","best validation R2: 0.302897564420274\n","testing\n","TESTING ON BATCH: 0\n","TESTING ON BATCH: 100\n","TESTING ON BATCH: 200\n","test R2: 0.2759196028512089\n"],"name":"stdout"}]}]}