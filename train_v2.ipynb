{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_v1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LwU1-qE9n9xc",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpMaUjrf2tk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4f82047f-2c78-4b99-a665-7b68d56fe958"
      },
      "source": [
        "### ONLY IF USING GOOGLE COLAB \n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(os.getcwd())\n",
        "os.chdir(\"/content/drive/My Drive/COMS4995_Final/DeepDiff_adapted\")\n",
        "os.getcwd()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/COMS4995_Final/DeepDiff_adapted\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/COMS4995_Final/DeepDiff_adapted'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huu_2SCG2hC1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.argv = [\"--cell_1=gm\", \n",
        "            \"--cell_1=gm\", \n",
        "            \"--cell_2=k562\", \n",
        "            \"--model_name=raw_d\", \n",
        "            \"--epochs=10\", \n",
        "            \"--lr=0.001\", \n",
        "            \"--data_root=data/\", \n",
        "            \"--save_root=Results/\", \n",
        "            \"--attentionfilename=gm_k562_test_11\",  \n",
        "            \"--n_hms=11\", \n",
        "            \"--save_attention_maps\",\n",
        "            \"--gpu=1\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7lBkptx1mZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f48b2e0-7f05-4bf0-f86c-879c17196d8e"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import argparse\n",
        "import json\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import cuda\n",
        "import sys, os\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import models as Model\n",
        "from SiameseLoss import ContrastiveLoss\n",
        "import evaluate\n",
        "import data_v1\n",
        "import gc\n",
        "import csv\n",
        "\n",
        "parser = argparse.ArgumentParser(description='DeepDiff')\n",
        "parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate')\n",
        "parser.add_argument('--model_name', type=str, default='raw_d', help='DeepDiff variation')\n",
        "parser.add_argument('--clip', type=float, default=1,help='gradient clipping')\n",
        "parser.add_argument('--epochs', type=int, default=90, help='upper epoch limit')\n",
        "parser.add_argument('--batch_size', type=int, default=10, help='')\n",
        "parser.add_argument('--dropout', type=float, default=0.5, help='dropout applied to layers (0 = no dropout) if n_layers LSTM > 1')\n",
        "parser.add_argument('--cell_1', type=str, default='Cell1', help='cell type 1')\n",
        "parser.add_argument('--cell_2', type=str, default='Cell2', help='cell type 2')\n",
        "parser.add_argument('--save_root', type=str, default='./Results/', help='where to save')\n",
        "parser.add_argument('--data_root', type=str, default='./data/', help='data location')\n",
        "parser.add_argument('--gpuid', type=int, default=0, help='CUDA gpu')\n",
        "parser.add_argument('--gpu', type=int, default=0, help='CUDA gpu')\n",
        "parser.add_argument('--n_hms', type=int, default=5, help='number of histone modifications')\n",
        "parser.add_argument('--n_bins', type=int, default=200, help='number of bins')\n",
        "parser.add_argument('--bin_rnn_size', type=int, default=32, help='bin rnn size')\n",
        "parser.add_argument('--num_layers', type=int, default=1, help='number of layers')\n",
        "parser.add_argument('--unidirectional', action='store_true', help='bidirectional/undirectional LSTM')\n",
        "parser.add_argument('--save_attention_maps',action='store_true', help='set to save validation beta attention maps')\n",
        "parser.add_argument('--attentionfilename', type=str, default='beta_attention.txt', help='where to save attnetion maps')\n",
        "parser.add_argument('--test_on_saved_model',action='store_true', help='only test on saved model')\n",
        "args = parser.parse_args()\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "model_name = ''\n",
        "model_name += (args.cell_1)+('_')+(args.cell_2)+('_')\n",
        "\n",
        "model_name+=args.model_name\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "args.bidirectional=not args.unidirectional\n",
        "\n",
        "print('the model name: ',model_name)\n",
        "args.data_root+=''\n",
        "args.save_root+=''\n",
        "args.dataset=args.cell_1+('_')+args.cell_2\n",
        "args.data_root = os.path.join(args.data_root)\n",
        "print('loading data from:  ',args.data_root)\n",
        "args.save_root = os.path.join(args.save_root,args.dataset)\n",
        "print('saving results in  from: ',args.save_root)\n",
        "model_dir = os.path.join(args.save_root,model_name)\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "attentionmapfile=model_dir+'/'+args.attentionfilename\n",
        "print('==>processing data')\n",
        "Train,Valid,Test = data_v1.load_data(args)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "CON=False\n",
        "AUX=False\n",
        "print('==>building model')\n",
        "if(args.model_name=='raw_d'):\n",
        "    # model = Model.raw_d(args)\n",
        "\n",
        "    # TEST\n",
        "    model = raw_d(args)\n",
        "\n",
        "elif(args.model_name=='raw_c'):\n",
        "    model = Model.raw_c(args)\n",
        "elif(args.model_name=='raw'):\n",
        "    model = Model.raw(args)\n",
        "elif(args.model_name=='aux'):\n",
        "    args.shared=False\n",
        "    model = Model.aux(args)\n",
        "    AUX=True\n",
        "    args.gamma=0.0\n",
        "elif(args.model_name=='raw_aux'):\n",
        "    args.shared=False\n",
        "    model = Model.raw_aux(args)\n",
        "    AUX=True\n",
        "    args.gamma=0.0\n",
        "elif(args.model_name=='aux_siamese'):\n",
        "    CON=True\n",
        "    args.shared=True\n",
        "    model = Model.aux_siamese(args)\n",
        "    AUX=True\n",
        "    args.gamma=4.0\n",
        "elif(args.model_name=='raw_aux_siamese'):\n",
        "    CON=True\n",
        "    args.shared=True\n",
        "    model = Model.raw_aux_siamese(args)\n",
        "    AUX=True\n",
        "    args.gamma=4.0\n",
        "else:\n",
        "    sys.exit(\"invalid model name\")\n",
        "\n",
        "\n",
        "if torch.cuda.device_count() >= 1:\n",
        "    torch.cuda.manual_seed_all(1)\n",
        "    dtype = torch.cuda.FloatTensor\n",
        "    cuda.set_device(args.gpuid)\n",
        "    model.type(dtype)\n",
        "    print('Using GPU '+str(args.gpuid))\n",
        "else:\n",
        "    print(\"No GPU Available\")\n",
        "    dtype = torch.FloatTensor\n",
        "\n",
        "## PRINTING MODEL USES SO MUCH SPACE WHY\n",
        "#print(model)\n",
        "\n",
        "\n",
        "if(args.test_on_saved_model==False):\n",
        "    print(\"==>initializing a new model\")\n",
        "    for p in model.parameters():\n",
        "        p.data.uniform_(-0.1,0.1)\n",
        "\n",
        "DiffLoss = nn.MSELoss(size_average=True).type(dtype)\n",
        "AuxLoss = nn.MSELoss(size_average=True).type(dtype)\n",
        "ConLoss = ContrastiveLoss().type(dtype)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = args.lr)\n",
        "#optimizer = optim.SGD(model.parameters(), lr = args.lr, momentum=args.momentum)\n",
        "def train(TrainData):\n",
        "    model.train()\n",
        "    # initialize attention\n",
        "    diff_targets = torch.zeros(TrainData.dataset.__len__(),1)\n",
        "    diff_predictions = torch.zeros(diff_targets.size(0),1)\n",
        "    if(args.model_name=='raw_d'):\n",
        "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n",
        "    elif(args.model_name=='raw_c'):\n",
        "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n",
        "    elif(args.model_name=='raw'):\n",
        "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),3*args.n_hms)\n",
        "\n",
        "    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n",
        "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n",
        "\n",
        "    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n",
        "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),5*args.n_hms)\n",
        "\n",
        "    else:\n",
        "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n",
        "\n",
        "    num_batches = int(math.ceil(TrainData.dataset.__len__()/float(args.batch_size)))\n",
        "    all_gene_ids=[None]*TrainData.dataset.__len__()\n",
        "    per_epoch_loss = 0\n",
        "    for idx, Sample in enumerate(TrainData):\n",
        "        if(idx%100==0):\n",
        "            print('TRAINING ON BATCH:',idx)\n",
        "        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, TrainData.dataset.__len__())\n",
        "        optimizer.zero_grad()\n",
        "        # get HM profiles\n",
        "        inputs_1 = Sample['X_A']\n",
        "        inputs_2 = Sample['X_B']\n",
        "\n",
        "\n",
        "        # get targets: both differential and cell specific expression\n",
        "        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n",
        "        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n",
        "        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n",
        "        diff_targets[start:end,0] = batch_diff_targets[:,0]\n",
        "\n",
        "        if(CON==True):\n",
        "            # get labels for contrastive loss\n",
        "            batch_contrastive_targets =[]\n",
        "            for label in batch_diff_targets:\n",
        "                if(label<=-2.0):\n",
        "                    batch_contrastive_targets.append(1)\n",
        "                elif(label>=2.0):\n",
        "                    batch_contrastive_targets.append(1)\n",
        "                else:\n",
        "                    batch_contrastive_targets.append(0)\n",
        "            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n",
        "\n",
        "\n",
        "        all_gene_ids[start:end]=Sample['geneID']\n",
        "        batch_size = inputs_1.size(0)\n",
        "\n",
        "        if(AUX==False):\n",
        "            # for raw models: raw_d, raw_c, raw\n",
        "            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
        "\n",
        "            all_attention_bin[start:end]=batch_alpha.data\n",
        "            all_attention_hm[start:end]=batch_beta.data\n",
        "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
        "        elif(CON==False):\n",
        "            # for aux models\n",
        "            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
        "            all_attention_bin[start:end]=batch_alpha.data\n",
        "            all_attention_hm[start:end]=batch_beta.data\n",
        "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
        "            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n",
        "            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n",
        "        else:\n",
        "            # for aux and siamese models\n",
        "            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
        "\n",
        "            all_attention_bin[start:end]=batch_alpha.data\n",
        "            all_attention_hm[start:end]=batch_beta.data\n",
        "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
        "            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n",
        "            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n",
        "            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n",
        "\n",
        "        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n",
        "        per_epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
        "        optimizer.step()\n",
        "    per_epoch_loss=per_epoch_loss/num_batches\n",
        "    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n",
        "\n",
        "\n",
        "\n",
        "def test(ValidData):\n",
        "    model.eval()\n",
        "\n",
        "    diff_targets = torch.zeros(ValidData.dataset.__len__(),1)\n",
        "    diff_predictions = torch.zeros(diff_targets.size(0),1)\n",
        "    if(args.model_name=='raw_d'):\n",
        "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n",
        "    elif(args.model_name=='raw_c'):\n",
        "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n",
        "    elif(args.model_name=='raw'):\n",
        "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),3*args.n_hms)\n",
        "    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n",
        "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n",
        "    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n",
        "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),5*args.n_hms)\n",
        "    else:\n",
        "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n",
        "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n",
        "\n",
        "    num_batches = int(math.ceil(ValidData.dataset.__len__()/float(args.batch_size)))\n",
        "    all_gene_ids=[None]*ValidData.dataset.__len__()\n",
        "    per_epoch_loss = 0\n",
        "    for idx, Sample in enumerate(ValidData):\n",
        "        if(idx%100==0):\n",
        "            print('TESTING ON BATCH:',idx)\n",
        "        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, ValidData.dataset.__len__())\n",
        "        optimizer.zero_grad()\n",
        "        # get HM profiles\n",
        "        inputs_1 = Sample['X_A']\n",
        "        inputs_2 = Sample['X_B']\n",
        "\n",
        "        \n",
        "        # get targets: both differential and cell specific expression\n",
        "        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n",
        "        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n",
        "        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n",
        "        diff_targets[start:end,0] = batch_diff_targets[:,0]\n",
        "\n",
        "        if(CON==True):\n",
        "            # get labels for contrastive loss\n",
        "            batch_contrastive_targets =[]\n",
        "            for label in batch_diff_targets:\n",
        "                if(label<=-2.0):\n",
        "                    batch_contrastive_targets.append(1)\n",
        "                elif(label>=2.0):\n",
        "                    batch_contrastive_targets.append(1)\n",
        "                else:\n",
        "                    batch_contrastive_targets.append(0)\n",
        "            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n",
        "\n",
        "\n",
        "        all_gene_ids[start:end]=Sample['geneID']\n",
        "        batch_size = inputs_1.size(0)\n",
        "\n",
        "        if(AUX==False):\n",
        "            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
        "\n",
        "            all_attention_bin[start:end]=batch_alpha.data\n",
        "            all_attention_hm[start:end]=batch_beta.data\n",
        "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
        "        elif(CON==False):\n",
        "            # for aux models\n",
        "            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
        "            all_attention_bin[start:end]=batch_alpha.data\n",
        "            all_attention_hm[start:end]=batch_beta.data\n",
        "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
        "            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n",
        "            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n",
        "        else:\n",
        "            # for aux and siamese models\n",
        "            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
        "\n",
        "            all_attention_bin[start:end]=batch_alpha.data\n",
        "            all_attention_hm[start:end]=batch_beta.data\n",
        "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
        "            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n",
        "            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n",
        "            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n",
        "\n",
        "        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n",
        "        per_epoch_loss += loss.item()\n",
        "    per_epoch_loss=per_epoch_loss/num_batches\n",
        "    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "best_valid_loss = 10000000000\n",
        "best_valid_MSE=100000\n",
        "best_valid_R2=-1\n",
        "if(args.test_on_saved_model==False):\n",
        "    for epoch in range(0, args.epochs):\n",
        "        print('=---------------------------------------- Training '+str(epoch+1)+' -----------------------------------=')\n",
        "        diff_predictions,diff_targets,alpha_train,beta_train,train_loss,_ = train(Train)\n",
        "        train_MSE, train_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n",
        "        diff_predictions,diff_targets,alpha_valid,beta_valid,valid_loss,gene_ids_valid = test(Valid)\n",
        "        valid_MSE, valid_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n",
        "\n",
        "        if(valid_R2 >= best_valid_R2):\n",
        "                # save best epoch -- models converge early\n",
        "            best_valid_R2=valid_R2\n",
        "            torch.save(model,model_dir+\"/\"+model_name+'_R2_model.pt')\n",
        "\n",
        "        print(\"Epoch:\",epoch)\n",
        "        print(\"train R2:\",train_R2)\n",
        "        print(\"valid R2:\",valid_R2)\n",
        "        print(\"best valid R2:\", best_valid_R2)\n",
        "\n",
        " \n",
        "    print(\"finished training!!\")\n",
        "    print(\"best validation R2:\",best_valid_R2)\n",
        "    print(\"testing\")\n",
        "    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n",
        "\n",
        "    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n",
        "    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n",
        "    print(\"test R2:\",test_R2)\n",
        "\n",
        "    if(args.save_attention_maps):\n",
        "        attentionfile=open(attentionmapfile,'w')\n",
        "        attentionfilewriter=csv.writer(attentionfile)\n",
        "        beta_test=beta_test.numpy()\n",
        "        for i in range(len(gene_ids_test)):\n",
        "            gene_attention=[]\n",
        "            gene_attention.append(gene_ids_test[i])\n",
        "            for e in beta_test[i,:]:\n",
        "                gene_attention.append(str(e))\n",
        "            attentionfilewriter.writerow(gene_attention)\n",
        "        attentionfile.close()\n",
        "\n",
        "\n",
        "else:\n",
        "    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n",
        "    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n",
        "    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n",
        "    print(\"test R2:\",test_R2)\n",
        "\n",
        "    if(args.save_attention_maps):\n",
        "        attentionfile=open(attentionmapfile,'w')\n",
        "        attentionfilewriter=csv.writer(attentionfile)\n",
        "        beta_test=beta_test.numpy()\n",
        "        for i in range(len(gene_ids_test)):\n",
        "            gene_attention=[]\n",
        "            gene_attention.append(gene_ids_test[i])\n",
        "            for e in beta_test[i,:]:\n",
        "                gene_attention.append(str(e))\n",
        "            attentionfilewriter.writerow(gene_attention)\n",
        "        attentionfile.close()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the model name:  gm_k562_raw_d\n",
            "loading data from:   data/\n",
            "saving results in  from:  Results/gm_k562\n",
            "==>processing data\n",
            "==>loading train data\n",
            "Number of genes: 300\n",
            "Number of entries: 6000\n",
            "Number of HMs: 11\n",
            "Number of genes: 300\n",
            "Number of entries: 6000\n",
            "Number of HMs: 11\n",
            "==>loading valid data\n",
            "Number of genes: 300\n",
            "Number of entries: 6000\n",
            "Number of HMs: 11\n",
            "Number of genes: 300\n",
            "Number of entries: 6000\n",
            "Number of HMs: 11\n",
            "==>loading test data\n",
            "Number of genes: 300\n",
            "Number of entries: 6000\n",
            "Number of HMs: 11\n",
            "Number of genes: 300\n",
            "Number of entries: 6000\n",
            "Number of HMs: 11\n",
            "==>building model\n",
            "Using GPU 0\n",
            "==>initializing a new model\n",
            "=---------------------------------------- Training 1 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 0\n",
            "train R2: -0.09944761164898522\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "=---------------------------------------- Training 2 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 1\n",
            "train R2: -0.1660383781342256\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "=---------------------------------------- Training 3 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 2\n",
            "train R2: -0.08010098454662179\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "=---------------------------------------- Training 4 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 3\n",
            "train R2: -0.05522193643696291\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "=---------------------------------------- Training 5 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 4\n",
            "train R2: -0.08934159163613879\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "=---------------------------------------- Training 6 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 5\n",
            "train R2: -0.04120367475640372\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "=---------------------------------------- Training 7 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 6\n",
            "train R2: -0.01018528080315567\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "=---------------------------------------- Training 8 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 7\n",
            "train R2: -0.030734067992044752\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "=---------------------------------------- Training 9 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 8\n",
            "train R2: -0.057181721303700014\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "=---------------------------------------- Training 10 -----------------------------------=\n",
            "TRAINING ON BATCH: 0\n",
            "TESTING ON BATCH: 0\n",
            "Epoch: 9\n",
            "train R2: -0.06976998538594091\n",
            "valid R2: nan\n",
            "best valid R2: -1\n",
            "finished training!!\n",
            "best validation R2: -1\n",
            "testing\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-07710505a69b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best validation R2:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbest_valid_R2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_R2_model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mdiff_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiff_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgene_ids_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Results/gm_k562/gm_k562_raw_d/gm_k562_raw_d_R2_model.pt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k44IR4g-NJg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_product(iput, mat2):\n",
        "        result = None\n",
        "        for i in range(iput.size()[0]):\n",
        "            op = torch.mm(iput[i], mat2)\n",
        "            op = op.unsqueeze(0)\n",
        "            if(result is None):\n",
        "                result = op\n",
        "            else:\n",
        "                result = torch.cat((result,op),0)\n",
        "        return result.squeeze(2)\n",
        "\n",
        "\n",
        "class rec_attention(nn.Module):\n",
        "    # attention with bin context vector per HM and HM context vector\n",
        "    def __init__(self,hm,args):\n",
        "        super(rec_attention,self).__init__()\n",
        "        self.num_directions=2 if args.bidirectional else 1\n",
        "        if (hm==False):\n",
        "            self.bin_rep_size=args.bin_rnn_size*self.num_directions\n",
        "        else:\n",
        "            self.bin_rep_size=args.bin_rnn_size\n",
        "    \n",
        "        self.bin_context_vector=nn.Parameter(torch.Tensor(self.bin_rep_size,1),requires_grad=True)\n",
        "    \n",
        "\n",
        "        self.softmax=nn.Softmax()\n",
        "\n",
        "        self.bin_context_vector.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def forward(self,iput):\n",
        "        alpha=self.softmax(batch_product(iput,self.bin_context_vector))\n",
        "        [batch_size,source_length,bin_rep_size2]=iput.size()\n",
        "        repres=torch.bmm(alpha.unsqueeze(2).view(batch_size,-1,source_length),iput)\n",
        "        return repres,alpha\n",
        "\n",
        "\n",
        "\n",
        "class recurrent_encoder(nn.Module):\n",
        "    # modular LSTM encoder\n",
        "    def __init__(self,n_bins,ip_bin_size,hm,args):\n",
        "        super(recurrent_encoder,self).__init__()\n",
        "        self.bin_rnn_size=args.bin_rnn_size\n",
        "        self.ipsize=ip_bin_size\n",
        "        self.seq_length=n_bins\n",
        "\n",
        "        self.num_directions=2 if args.bidirectional else 1\n",
        "        if (hm==False):\n",
        "            self.bin_rnn_size=args.bin_rnn_size\n",
        "        else:\n",
        "            self.bin_rnn_size=args.bin_rnn_size // 2\n",
        "        self.bin_rep_size=self.bin_rnn_size*self.num_directions\n",
        "\n",
        "\n",
        "        self.rnn=nn.LSTM(self.ipsize,self.bin_rnn_size,num_layers=args.num_layers,dropout=args.dropout,bidirectional=args.bidirectional)\n",
        "\n",
        "        self.bin_attention=rec_attention(hm,args)\n",
        "    def outputlength(self):\n",
        "        return self.bin_rep_size\n",
        "    def forward(self,single_hm,hidden=None):\n",
        "\n",
        "        bin_output, hidden = self.rnn(single_hm,hidden)\n",
        "        bin_output = bin_output.permute(1,0,2)\n",
        "        hm_rep,bin_alpha = self.bin_attention(bin_output)\n",
        "        return hm_rep,bin_alpha\n",
        "\n",
        "class raw_d(nn.Module):\n",
        "    def __init__(self,args):\n",
        "        self.n_hms=args.n_hms\n",
        "        self.n_bins=args.n_bins\n",
        "        self.ip_bin_size=1\n",
        "        super(raw_d,self).__init__()\n",
        "        self.rnn_hms=nn.ModuleList()\n",
        "        for i in range(self.n_hms):\n",
        "            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
        "        self.opsize = self.rnn_hms[0].outputlength()\n",
        "        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
        "        self.opsize2=self.hm_level_rnn_1.outputlength()\n",
        "        self.diffopsize=2*(self.opsize2)\n",
        "        self.fdiff1_1=nn.Linear(self.opsize2,1)\n",
        "\n",
        "    def forward(self,iput1,iput2):\n",
        "\n",
        "        iput=iput1-iput2\n",
        "        bin_a=None\n",
        "        level1_rep=None\n",
        "        [batch_size,_,_]=iput.size()\n",
        "\n",
        "        for hm,hm_encdr in enumerate(self.rnn_hms):\n",
        "            hmod=iput[:,:,hm].contiguous()\n",
        "            hmod=torch.t(hmod).unsqueeze(2)\n",
        "\n",
        "            op,a= hm_encdr(hmod)\n",
        "            if level1_rep is None:\n",
        "\n",
        "                level1_rep=op\n",
        "                bin_a=a\n",
        "            else:\n",
        "                level1_rep=torch.cat((level1_rep,op),1)\n",
        "                bin_a=torch.cat((bin_a,a),1)\n",
        "        \n",
        "        # flatten bin_a to match all_attention dimensions\n",
        "        bin_a = bin_a.reshape(-1)\n",
        "\n",
        "        level1_rep=level1_rep.permute(1,0,2)\n",
        "        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n",
        "        final_rep_1=final_rep_1.squeeze(1)\n",
        "        prediction_m=((self.fdiff1_1(final_rep_1)))\n",
        "        return prediction_m,hm_level_attention_1, bin_a"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}