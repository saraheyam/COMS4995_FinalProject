{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dswCWzAlo-P_"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import csv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "def batch_product(iput, mat2):\n",
    "        result = None\n",
    "        for i in range(iput.size()[0]):\n",
    "            op = torch.mm(iput[i], mat2)\n",
    "            op = op.unsqueeze(0)\n",
    "            if(result is None):\n",
    "                result = op\n",
    "            else:\n",
    "                result = torch.cat((result,op),0)\n",
    "        return result.squeeze(2)\n",
    "\n",
    "\n",
    "class rec_attention(nn.Module):\n",
    "    # attention with bin context vector per HM and HM context vector\n",
    "    def __init__(self,hm,args):\n",
    "        super(rec_attention,self).__init__()\n",
    "        self.num_directions=2 if args.bidirectional else 1\n",
    "        if (hm==False):\n",
    "            self.bin_rep_size=args.bin_rnn_size*self.num_directions\n",
    "        else:\n",
    "            self.bin_rep_size=args.bin_rnn_size\n",
    "    \n",
    "        self.bin_context_vector=nn.Parameter(torch.Tensor(self.bin_rep_size,1),requires_grad=True)\n",
    "    \n",
    "\n",
    "        self.softmax=nn.Softmax()\n",
    "\n",
    "        self.bin_context_vector.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self,iput):\n",
    "        alpha=self.softmax(batch_product(iput,self.bin_context_vector))\n",
    "        [batch_size,source_length,bin_rep_size2]=iput.size()\n",
    "        repres=torch.bmm(alpha.unsqueeze(2).view(batch_size,-1,source_length),iput)\n",
    "        return repres,alpha\n",
    "\n",
    "\n",
    "\n",
    "class recurrent_encoder(nn.Module):\n",
    "    # modular LSTM encoder\n",
    "    def __init__(self,n_bins,ip_bin_size,hm,args):\n",
    "        super(recurrent_encoder,self).__init__()\n",
    "        self.bin_rnn_size=args.bin_rnn_size\n",
    "        self.ipsize=ip_bin_size\n",
    "        self.seq_length=n_bins\n",
    "\n",
    "        self.num_directions=2 if args.bidirectional else 1\n",
    "        if (hm==False):\n",
    "            self.bin_rnn_size=args.bin_rnn_size\n",
    "        else:\n",
    "            self.bin_rnn_size=args.bin_rnn_size // 2\n",
    "        self.bin_rep_size=self.bin_rnn_size*self.num_directions\n",
    "\n",
    "\n",
    "        self.rnn=nn.LSTM(self.ipsize,self.bin_rnn_size,num_layers=args.num_layers,dropout=args.dropout,bidirectional=args.bidirectional)\n",
    "\n",
    "        self.bin_attention=rec_attention(hm,args)\n",
    "    def outputlength(self):\n",
    "        return self.bin_rep_size\n",
    "    def forward(self,single_hm,hidden=None):\n",
    "\n",
    "        bin_output, hidden = self.rnn(single_hm,hidden)\n",
    "        bin_output = bin_output.permute(1,0,2)\n",
    "        hm_rep,bin_alpha = self.bin_attention(bin_output)\n",
    "        return hm_rep,bin_alpha\n",
    "\n",
    "class raw_d(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        self.n_hms=args.n_hms\n",
    "        self.n_bins=args.n_bins\n",
    "        self.ip_bin_size=1\n",
    "        super(raw_d,self).__init__()\n",
    "        self.rnn_hms=nn.ModuleList()\n",
    "        for i in range(self.n_hms):\n",
    "            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.opsize = self.rnn_hms[0].outputlength()\n",
    "        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.opsize2=self.hm_level_rnn_1.outputlength()\n",
    "        self.diffopsize=2*(self.opsize2)\n",
    "        self.fdiff1_1=nn.Linear(self.opsize2,1)\n",
    "\n",
    "    def forward(self,iput1,iput2):\n",
    "\n",
    "        iput=iput1-iput2\n",
    "        bin_a=None\n",
    "        level1_rep=None\n",
    "        [batch_size,_,_]=iput.size()\n",
    "\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hms):\n",
    "            hmod=iput[:,:,hm].contiguous()\n",
    "            hmod=torch.t(hmod).unsqueeze(2)\n",
    "\n",
    "            op,a= hm_encdr(hmod)\n",
    "            if level1_rep is None:\n",
    "                level1_rep=op\n",
    "                bin_a=a\n",
    "            else:\n",
    "                level1_rep=torch.cat((level1_rep,op),1)\n",
    "                bin_a=torch.cat((bin_a,a),1)\n",
    "        level1_rep=level1_rep.permute(1,0,2)\n",
    "        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n",
    "        final_rep_1=final_rep_1.squeeze(1)\n",
    "        prediction_m=((self.fdiff1_1(final_rep_1)))\n",
    "        return prediction_m,hm_level_attention_1, bin_a\n",
    "\n",
    "\n",
    "class raw_c(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(raw_c,self).__init__()\n",
    "        self.n_hms=args.n_hms\n",
    "        self.n_bins=args.n_bins\n",
    "        self.ip_bin_size=1\n",
    "        self.joint=False\n",
    "        self.rnn_hms=nn.ModuleList()\n",
    "        for i in range(2*self.n_hms):\n",
    "            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.opsize = self.rnn_hms[0].outputlength()\n",
    "        self.hm_level_rnn_1=recurrent_encoder(2*self.n_hms,self.opsize,True,args)\n",
    "        self.opsize2=self.hm_level_rnn_1.outputlength()\n",
    "        self.diffopsize=2*(self.opsize2)\n",
    "        self.fdiff1_1=nn.Linear(self.opsize2,1)\n",
    "\n",
    "    def forward(self,iput1,iput2):\n",
    "        iput=torch.cat((iput1,iput2),2)\n",
    "        bin_a=None\n",
    "        level1_rep=None\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hms):\n",
    "            hmod=iput[:,:,hm].contiguous()\n",
    "\n",
    "            hmod=torch.t(hmod).unsqueeze(2)\n",
    "            op,a= hm_encdr(hmod)\n",
    "\n",
    "            if level1_rep is None:\n",
    "                level1_rep=op\n",
    "                bin_a=a\n",
    "            else:\n",
    "                level1_rep=torch.cat((level1_rep,op),1)\n",
    "                bin_a=torch.cat((bin_a,a),1)\n",
    "\n",
    "        level1_rep=level1_rep.permute(1,0,2)\n",
    "        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n",
    "        final_rep_1=final_rep_1.squeeze(1)\n",
    "        prediction_m=((self.fdiff1_1(final_rep_1)))\n",
    "\n",
    "        return prediction_m,hm_level_attention_1, bin_a\n",
    "\n",
    "\n",
    "class raw(nn.Module):\n",
    "    # Model with all raw features: difference and absolute features\n",
    "    def __init__(self,args):\n",
    "        super(raw,self).__init__()\n",
    "        self.n_hms=args.n_hms\n",
    "        self.n_bins=args.n_bins\n",
    "        self.ip_bin_size=1\n",
    "        self.rnn_hms=nn.ModuleList()\n",
    "        for i in range(3*self.n_hms):\n",
    "            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.opsize = self.rnn_hms[0].outputlength()\n",
    "        self.hm_level_rnn_1=recurrent_encoder(3*self.n_hms,self.opsize,True,args)\n",
    "        self.opsize2=self.hm_level_rnn_1.outputlength()\n",
    "        self.diffopsize=2*(self.opsize2)\n",
    "        self.fdiff1_1=nn.Linear(self.opsize2,1)\n",
    "\n",
    "    def forward(self,iput1,iput2):\n",
    "        iput3=iput1-iput2\n",
    "        iput4=torch.cat((iput1,iput2),2)\n",
    "        iput=(torch.cat((iput4,iput3),2))\n",
    "\n",
    "        bin_a=None\n",
    "        level1_rep=None\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hms):\n",
    "            hmod=iput[:,:,hm].contiguous()\n",
    "            hmod=torch.t(hmod).unsqueeze(2)\n",
    "            op,a= hm_encdr(hmod)\n",
    "            if level1_rep is None:\n",
    "                level1_rep=op\n",
    "                bin_a=a\n",
    "            else:\n",
    "                level1_rep=torch.cat((level1_rep,op),1)\n",
    "                bin_a=torch.cat((bin_a,a),1)\n",
    "\n",
    "        level1_rep=level1_rep.permute(1,0,2)\n",
    "        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n",
    "        final_rep_1=final_rep_1.squeeze(1)\n",
    "\n",
    "        prediction_m=((self.fdiff1_1(final_rep_1)))\n",
    "        return prediction_m,hm_level_attention_1, bin_a\n",
    "\n",
    "\n",
    "class aux(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(aux,self).__init__()\n",
    "        self.n_hms=args.n_hms\n",
    "        self.n_bins=args.n_bins\n",
    "        self.ip_bin_size=1\n",
    "        self.joint=False\n",
    "        self.shared=False\n",
    "        self.rnn_hms=nn.ModuleList()\n",
    "        for i in range(self.n_hms):\n",
    "            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.opsize = self.rnn_hms[0].outputlength()\n",
    "        self.rnn_hms2=nn.ModuleList()\n",
    "        for i in range(self.n_hms):\n",
    "            self.rnn_hms2.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.opsize2=self.hm_level_rnn_1.outputlength()\n",
    "        self.f1_1=nn.Linear(self.opsize2,1)\n",
    "        self.f2_1=nn.Linear(self.opsize2,1)\n",
    "        self.diffopsize=2*(self.opsize2)\n",
    "        self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n",
    "        self.predictor_=nn.Linear(self.diffopsize//2,1)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward_once(self,iput,shared,cellid):\n",
    "        bin_a=None\n",
    "        level1_rep=None\n",
    "        if(shared or cellid==1):\n",
    "            for hm,hm_encdr in enumerate(self.rnn_hms):\n",
    "                hmod=iput[:,:,hm].contiguous()\n",
    "                hmod=torch.t(hmod).unsqueeze(2)\n",
    "                op,a= hm_encdr(hmod)\n",
    "                if level1_rep is None:\n",
    "                    level1_rep=op\n",
    "                    bin_a=a\n",
    "                else:\n",
    "                    level1_rep=torch.cat((level1_rep,op),1)\n",
    "                    bin_a=torch.cat((bin_a,a),1)\n",
    "            level1_rep=level1_rep.permute(1,0,2)\n",
    "        else:\n",
    "            for hm,hm_encdr in enumerate(self.rnn_hms2):\n",
    "\n",
    "                hmod=iput[:,:,hm].contiguous()\n",
    "                hmod=torch.t(hmod).unsqueeze(2)\n",
    "                op,a= hm_encdr(hmod)\n",
    "                if level1_rep is None:\n",
    "                    level1_rep=op\n",
    "                    bin_a=a\n",
    "                else:\n",
    "                    level1_rep=torch.cat((level1_rep,op),1)\n",
    "                    bin_a=torch.cat((bin_a,a),1)\n",
    "            level1_rep=level1_rep.permute(1,0,2)\n",
    "        return level1_rep,bin_a\n",
    "\n",
    "    def forward(self,iput1,iput2):\n",
    " \n",
    "        level1_rep1,bin_a1=self.forward_once(iput1,self.shared,1)\n",
    "        level1_rep2,bin_a2=self.forward_once(iput2,self.shared,2)\n",
    "        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n",
    "        if(self.joint):\n",
    "            final_rep_2,hm_level_attention_2=self.hm_level_rnn_1(level1_rep2)\n",
    "        else:\n",
    "            final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n",
    "        final_rep_1=final_rep_1.squeeze(1)\n",
    "        final_rep_2=final_rep_2.squeeze(1)\n",
    "        prediction1=((self.f1_1(final_rep_1)))\n",
    "        prediction2=((self.f2_1(final_rep_2)))\n",
    "\n",
    "        mlp_input=torch.cat((final_rep_1,final_rep_2),1)\n",
    "        mlp_input=self.relu(self.predictor_1(mlp_input))\n",
    "        prediction=(self.predictor_(mlp_input))\n",
    "        hm_level_attention=torch.cat((hm_level_attention_1,hm_level_attention_2),1)\n",
    "        bin_a=torch.cat((bin_a1,bin_a2),1)\n",
    "\n",
    "        return prediction,hm_level_attention,bin_a,prediction1,prediction2\n",
    "\n",
    "\n",
    "class raw_aux(nn.Module):\n",
    "    # level 1 takes raw feaures, level 2 takes aux features as well as raw feature embeddings from level 1\n",
    "    # returns attention scores from raw part only\n",
    "    def __init__(self,args):\n",
    "        super(raw_aux,self).__init__()\n",
    "        self.n_hms=args.n_hms\n",
    "        self.n_bins=args.n_bins\n",
    "        self.ip_bin_size=1\n",
    "        self.joint=False\n",
    "        self.shared=False\n",
    "        self.rnn_hms=nn.ModuleList()\n",
    "        for i in range(self.n_hms):\n",
    "            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.opsize = self.rnn_hms[0].outputlength()\n",
    "        self.rnn_hms3=nn.ModuleList()\n",
    "        for i in range(3*self.n_hms):\n",
    "            self.rnn_hms3.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.opsize = self.rnn_hms[0].outputlength()\n",
    "        self.rnn_hms2=nn.ModuleList()\n",
    "        for i in range(self.n_hms):\n",
    "            self.rnn_hms2.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.hm_level_rnn_3=recurrent_encoder(5*self.n_hms,self.opsize,True,args)\n",
    "        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.opsize2=self.hm_level_rnn_1.outputlength()\n",
    "        self.f1_1=nn.Linear(self.opsize2,1)\n",
    "        self.f2_1=nn.Linear(self.opsize2,1)\n",
    "        self.diffopsize=3*(self.opsize2)\n",
    "        self.predictor_=nn.Linear(self.opsize2,1)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self,iput1,iput2):\n",
    "        iput3=iput1-iput2\n",
    "        iput4=torch.cat((iput1,iput2),2)\n",
    "        iput=(torch.cat((iput4,iput3),2))\n",
    "        bin_a=None\n",
    "        level1_rep=None\n",
    "        [batch_size,_,_]=iput.size()\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hms3):\n",
    "\n",
    "            hmod=iput[:,:,hm].contiguous()\n",
    "            hmod=torch.t(hmod).unsqueeze(2)\n",
    "            op,a= hm_encdr(hmod)\n",
    "            if level1_rep is None:\n",
    "                level1_rep=op\n",
    "                bin_a=a\n",
    "            else:\n",
    "                level1_rep=torch.cat((level1_rep,op),1)\n",
    "                bin_a=torch.cat((bin_a,a),1)\n",
    "        level1_rep=level1_rep.permute(1,0,2)\n",
    "\n",
    "        bin_a1=None\n",
    "        level1_rep1=None\n",
    "        [batch_size,_,_]=iput1.size()\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hms):\n",
    "            hmod=iput1[:,:,hm].contiguous()\n",
    "            hmod=torch.t(hmod).unsqueeze(2)\n",
    "            op,a= hm_encdr(hmod)\n",
    "            if level1_rep1 is None:\n",
    "                level1_rep1=op\n",
    "                bin_a1=a\n",
    "            else:\n",
    "                level1_rep1=torch.cat((level1_rep1,op),1)\n",
    "                bin_a1=torch.cat((bin_a1,a),1)\n",
    "\n",
    "        level1_rep1=level1_rep1.permute(1,0,2)\n",
    "        bin_a2=None\n",
    "        level1_rep2=None\n",
    "\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hms2):\n",
    "            hmod=iput2[:,:,hm].contiguous()\n",
    "            hmod=torch.t(hmod).unsqueeze(2)\n",
    "            op,a= hm_encdr(hmod)\n",
    "            if level1_rep2 is None:\n",
    "                level1_rep2=op\n",
    "                bin_a2=a\n",
    "            else:\n",
    "                level1_rep2=torch.cat((level1_rep2,op),1)\n",
    "                bin_a2=torch.cat((bin_a2,a),1)\n",
    "        level1_rep2=level1_rep2.permute(1,0,2)\n",
    "        level1_rep3=torch.cat((level1_rep,level1_rep1,level1_rep2),0)\n",
    "        final_rep_3,hm_level_attention_3=self.hm_level_rnn_3(level1_rep3)\n",
    "        final_rep_3=final_rep_3.squeeze(1)\n",
    "\n",
    "        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n",
    "        final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n",
    "        final_rep_1=final_rep_1.squeeze(1)\n",
    "        final_rep_2=final_rep_2.squeeze(1)\n",
    "        prediction1=((self.f1_1(final_rep_1)))\n",
    "        prediction2=((self.f2_1(final_rep_2)))\n",
    "        prediction=(self.predictor_(final_rep_3))\n",
    "        return prediction,hm_level_attention_3,bin_a,prediction1,prediction2\n",
    "\n",
    "\n",
    "class aux_siamese(nn.Module):\n",
    "    # aux with siamese  same as aux model mostly, but with shared level 1 embedding\n",
    "    def __init__(self,args):\n",
    "        super(aux_siamese,self).__init__()\n",
    "        self.n_hms=args.n_hms\n",
    "        self.n_bins=args.n_bins\n",
    "        self.ip_bin_size=1\n",
    "        self.joint=False\n",
    "        self.shared=True\n",
    "        self.rnn_hms=nn.ModuleList()\n",
    "        for i in range(self.n_hms):\n",
    "            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.opsize = self.rnn_hms[0].outputlength()\n",
    "        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.opsize2=self.hm_level_rnn_1.outputlength()\n",
    "        self.f1_1=nn.Linear(self.opsize2,1)\n",
    "        self.f2_1=nn.Linear(self.opsize2,1)\n",
    "        self.diffopsize=2*(self.opsize2)\n",
    "        self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n",
    "        self.predictor_=nn.Linear(self.diffopsize//2,1)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward_once(self,iput):\n",
    "        bin_a=None\n",
    "        level1_rep=None\n",
    "        [batch_size,_,_]=iput.size()\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hms):\n",
    "            hmod=iput[:,:,hm].contiguous()\n",
    "            hmod=torch.t(hmod).unsqueeze(2)\n",
    "            op,a= hm_encdr(hmod)\n",
    "            if level1_rep is None:\n",
    "                level1_rep=op\n",
    "                bin_a=a\n",
    "            else:\n",
    "                level1_rep=torch.cat((level1_rep,op),1)\n",
    "                bin_a=torch.cat((bin_a,a),1)\n",
    "        level1_rep=level1_rep.permute(1,0,2)\n",
    "\n",
    "        return level1_rep,bin_a\n",
    "\n",
    "\n",
    "    def forward(self,iput1,iput2):\n",
    "        level1_rep1,bin_a1=self.forward_once(iput1)\n",
    "\n",
    "        level1_rep2,bin_a2=self.forward_once(iput2)\n",
    "        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n",
    "        final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n",
    "        final_rep_1=final_rep_1.squeeze(1)\n",
    "        final_rep_2=final_rep_2.squeeze(1)\n",
    "        [a,b,c]=(level1_rep1.size())\n",
    "        alpha_rep_1=level1_rep1.permute(1,0,2).view(b,a*c)\n",
    "        alpha_rep_2=level1_rep2.permute(1,0,2).view(b,a*c)\n",
    "        prediction1=((self.f1_1(final_rep_1)))\n",
    "        prediction2=((self.f2_1(final_rep_2)))\n",
    "        mlp_input=torch.cat((final_rep_1,final_rep_2),1)\n",
    "        mlp_input=self.relu(self.predictor_1(mlp_input))\n",
    "        prediction=(self.predictor_(mlp_input))\n",
    "        hm_level_attention=torch.cat((hm_level_attention_1,hm_level_attention_2),1)\n",
    "        bin_a=torch.cat((bin_a1,bin_a2),1)\n",
    "\n",
    "        return prediction,hm_level_attention,bin_a,alpha_rep_1,alpha_rep_2,prediction1,prediction2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class raw_aux_siamese(nn.Module):\n",
    "    # similar to raw_aux model with shared level 1 embedding\n",
    "    # returns only raw level attentions\n",
    "    # returns embeddings from shared level 1 for contrastive loss\n",
    "    def __init__(self,args):\n",
    "        self.n_hms=args.n_hms\n",
    "        self.n_bins=args.n_bins\n",
    "        self.ip_bin_size=1\n",
    "        self.joint=False\n",
    "        self.shared=True\n",
    "        super(raw_aux_siamese,self).__init__()\n",
    "        self.rnn_hms=nn.ModuleList()\n",
    "        self.rnn_hmsx=nn.ModuleList()\n",
    "        for i in range(self.n_hms):\n",
    "            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        for i in range(3*self.n_hms):\n",
    "            self.rnn_hmsx.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n",
    "        self.opsize = self.rnn_hms[0].outputlength()\n",
    "        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.hm_level_rnn_1x=recurrent_encoder(5*self.n_hms,self.opsize,True,args)\n",
    "        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n",
    "        self.opsize2=self.hm_level_rnn_1.outputlength()\n",
    "        self.f1_1=nn.Linear(self.opsize2,1)\n",
    "        self.f2_1=nn.Linear(self.opsize2,1)\n",
    "        self.diffopsize=3*(self.opsize2)\n",
    "        #self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n",
    "        self.predictor_=nn.Linear(self.opsize2,1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.finalsoftmax=nn.LogSoftmax()\n",
    "    def forward_once(self,iput):\n",
    "        bin_a=None\n",
    "        level1_rep=None\n",
    "        [batch_size,_,_]=iput.size()\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hms):\n",
    "            hmod=iput[:,:,hm].contiguous()\n",
    "            hmod=torch.t(hmod).unsqueeze(2)\n",
    "            op,a= hm_encdr(hmod)\n",
    "            if level1_rep is None:\n",
    "                level1_rep=op\n",
    "                bin_a=a\n",
    "            else:\n",
    "                level1_rep=torch.cat((level1_rep,op),1)\n",
    "                bin_a=torch.cat((bin_a,a),1)\n",
    "        level1_rep=level1_rep.permute(1,0,2)\n",
    "        return level1_rep,bin_a\n",
    "\n",
    "\n",
    "    def forward(self,iput1,iput2):\n",
    "        iput3=iput1-iput2\n",
    "        iput4=torch.cat((iput1,iput2),2)\n",
    "        iput=(torch.cat((iput4,iput3),2))\n",
    "        bin_ax=None\n",
    "        level1_repx=None\n",
    "        [batch_size,_,_]=iput.size()\n",
    "        for hm,hm_encdr in enumerate(self.rnn_hmsx):\n",
    "            hmodx=iput[:,:,hm].contiguous()\n",
    "            hmodx=torch.t(hmodx).unsqueeze(2)\n",
    "            opx,ax= hm_encdr(hmodx)\n",
    "            if level1_repx is None:\n",
    "                level1_repx=opx\n",
    "                bin_ax=ax\n",
    "            else:\n",
    "                level1_repx=torch.cat((level1_repx,opx),1)\n",
    "                bin_ax=torch.cat((bin_ax,ax),1)\n",
    "        level1_repx=level1_repx.permute(1,0,2)\n",
    "\n",
    "        level1_rep1,bin_a1=self.forward_once(iput1)\n",
    "\n",
    "        level1_rep2,bin_a2=self.forward_once(iput2)\n",
    "        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n",
    "        final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n",
    "        final_rep_1=final_rep_1.squeeze(1)\n",
    "        final_rep_2=final_rep_2.squeeze(1)\n",
    "        [a,b,c]=(level1_rep1.size())\n",
    "        alpha_rep_1=level1_rep1.permute(1,0,2).view(b,a*c)\n",
    "        alpha_rep_2=level1_rep2.permute(1,0,2).view(b,a*c)\n",
    "        level1_rep3=torch.cat((level1_repx,level1_rep1,level1_rep2),0)\n",
    "        final_rep_1x,hm_level_attention_1x=self.hm_level_rnn_1x(level1_rep3)\n",
    "        final_rep_1x=final_rep_1x.squeeze(1)\n",
    "        prediction1=((self.f1_1(final_rep_1)))\n",
    "        prediction2=((self.f2_1(final_rep_2)))\n",
    "        prediction=(self.predictor_(final_rep_1x))\n",
    "\n",
    "        return prediction,hm_level_attention_1x,bin_ax,alpha_rep_1,alpha_rep_2,prediction1,prediction2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "models_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
