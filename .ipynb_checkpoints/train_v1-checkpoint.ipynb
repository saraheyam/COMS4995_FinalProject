{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwU1-qE9n9xc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--lr LR] [--model_name MODEL_NAME]\n",
      "                             [--clip CLIP] [--epochs EPOCHS]\n",
      "                             [--batch_size BATCH_SIZE] [--dropout DROPOUT]\n",
      "                             [--cell_1 CELL_1] [--cell_2 CELL_2]\n",
      "                             [--save_root SAVE_ROOT] [--data_root DATA_ROOT]\n",
      "                             [--gpuid GPUID] [--gpu GPU] [--n_hms N_HMS]\n",
      "                             [--n_bins N_BINS] [--bin_rnn_size BIN_RNN_SIZE]\n",
      "                             [--num_layers NUM_LAYERS] [--unidirectional]\n",
      "                             [--save_attention_maps]\n",
      "                             [--attentionfilename ATTENTIONFILENAME]\n",
      "                             [--test_on_saved_model]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/seyam/.local/share/jupyter/runtime/kernel-bbfd78bb-7f17-40cd-8d0c-e0d73fda64e6.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import cuda\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import models as Model\n",
    "from SiameseLoss import ContrastiveLoss\n",
    "import evaluate\n",
    "import data_v1\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "parser = argparse.ArgumentParser(description='DeepDiff')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate')\n",
    "parser.add_argument('--model_name', type=str, default='raw_d', help='DeepDiff variation')\n",
    "parser.add_argument('--clip', type=float, default=1,help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=90, help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=10, help='')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='dropout applied to layers (0 = no dropout) if n_layers LSTM > 1')\n",
    "parser.add_argument('--cell_1', type=str, default='Cell1', help='cell type 1')\n",
    "parser.add_argument('--cell_2', type=str, default='Cell2', help='cell type 2')\n",
    "parser.add_argument('--save_root', type=str, default='./Results/', help='where to save')\n",
    "parser.add_argument('--data_root', type=str, default='./data/', help='data location')\n",
    "parser.add_argument('--gpuid', type=int, default=0, help='CUDA gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='CUDA gpu')\n",
    "parser.add_argument('--n_hms', type=int, default=5, help='number of histone modifications')\n",
    "parser.add_argument('--n_bins', type=int, default=200, help='number of bins')\n",
    "parser.add_argument('--bin_rnn_size', type=int, default=32, help='bin rnn size')\n",
    "parser.add_argument('--num_layers', type=int, default=1, help='number of layers')\n",
    "parser.add_argument('--unidirectional', action='store_true', help='bidirectional/undirectional LSTM')\n",
    "parser.add_argument('--save_attention_maps',action='store_true', help='set to save validation beta attention maps')\n",
    "parser.add_argument('--attentionfilename', type=str, default='beta_attention.txt', help='where to save attnetion maps')\n",
    "parser.add_argument('--test_on_saved_model',action='store_true', help='only test on saved model')\n",
    "args = parser.parse_args()\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "\n",
    "model_name = ''\n",
    "model_name += (args.cell_1)+('_')+(args.cell_2)+('_')\n",
    "\n",
    "model_name+=args.model_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "args.bidirectional=not args.unidirectional\n",
    "\n",
    "print('the model name: ',model_name)\n",
    "args.data_root+=''\n",
    "args.save_root+=''\n",
    "args.dataset=args.cell_1+('_')+args.cell_2\n",
    "args.data_root = os.path.join(args.data_root)\n",
    "print('loading data from:  ',args.data_root)\n",
    "args.save_root = os.path.join(args.save_root,args.dataset)\n",
    "print('saving results in  from: ',args.save_root)\n",
    "model_dir = os.path.join(args.save_root,model_name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "attentionmapfile=model_dir+'/'+args.attentionfilename\n",
    "print('==>processing data')\n",
    "Train,Valid,Test = data.load_data(args)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CON=False\n",
    "AUX=False\n",
    "print('==>building model')\n",
    "if(args.model_name=='raw_d'):\n",
    "    model = Model.raw_d(args)\n",
    "elif(args.model_name=='raw_c'):\n",
    "    model = Model.raw_c(args)\n",
    "elif(args.model_name=='raw'):\n",
    "    model = Model.raw(args)\n",
    "elif(args.model_name=='aux'):\n",
    "    args.shared=False\n",
    "    model = Model.aux(args)\n",
    "    AUX=True\n",
    "    args.gamma=0.0\n",
    "elif(args.model_name=='raw_aux'):\n",
    "    args.shared=False\n",
    "    model = Model.raw_aux(args)\n",
    "    AUX=True\n",
    "    args.gamma=0.0\n",
    "elif(args.model_name=='aux_siamese'):\n",
    "    CON=True\n",
    "    args.shared=True\n",
    "    model = Model.aux_siamese(args)\n",
    "    AUX=True\n",
    "    args.gamma=4.0\n",
    "elif(args.model_name=='raw_aux_siamese'):\n",
    "    CON=True\n",
    "    args.shared=True\n",
    "    model = Model.raw_aux_siamese(args)\n",
    "    AUX=True\n",
    "    args.gamma=4.0\n",
    "else:\n",
    "    sys.exit(\"invalid model name\")\n",
    "\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    cuda.set_device(args.gpuid)\n",
    "    model.type(dtype)\n",
    "    print('Using GPU '+str(args.gpuid))\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "print(model)\n",
    "if(args.test_on_saved_model==False):\n",
    "    print(\"==>initializing a new model\")\n",
    "    for p in model.parameters():\n",
    "        p.data.uniform_(-0.1,0.1)\n",
    "\n",
    "DiffLoss = nn.MSELoss(size_average=True).type(dtype)\n",
    "AuxLoss = nn.MSELoss(size_average=True).type(dtype)\n",
    "ConLoss = ContrastiveLoss().type(dtype)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = args.lr)\n",
    "#optimizer = optim.SGD(model.parameters(), lr = args.lr, momentum=args.momentum)\n",
    "def train(TrainData):\n",
    "    model.train()\n",
    "    # initialize attention\n",
    "    diff_targets = torch.zeros(TrainData.dataset.__len__(),1)\n",
    "    diff_predictions = torch.zeros(diff_targets.size(0),1)\n",
    "    if(args.model_name=='raw_d'):\n",
    "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n",
    "    elif(args.model_name=='raw_c'):\n",
    "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n",
    "    elif(args.model_name=='raw'):\n",
    "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),3*args.n_hms)\n",
    "\n",
    "    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n",
    "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n",
    "\n",
    "    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n",
    "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),5*args.n_hms)\n",
    "\n",
    "    else:\n",
    "        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n",
    "\n",
    "    num_batches = int(math.ceil(TrainData.dataset.__len__()/float(args.batch_size)))\n",
    "    all_gene_ids=[None]*TrainData.dataset.__len__()\n",
    "    per_epoch_loss = 0\n",
    "    for idx, Sample in enumerate(TrainData):\n",
    "        if(idx%100==0):\n",
    "            print('TRAINING ON BATCH:',idx)\n",
    "        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, TrainData.dataset.__len__())\n",
    "        optimizer.zero_grad()\n",
    "        # get HM profiles\n",
    "        inputs_1 = Sample['X_A']\n",
    "        inputs_2 = Sample['X_B']\n",
    "        # get targets: both differential and cell specific expression\n",
    "        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n",
    "        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n",
    "        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n",
    "        diff_targets[start:end,0] = batch_diff_targets[:,0]\n",
    "\n",
    "        if(CON==True):\n",
    "            # get labels for contrastive loss\n",
    "            batch_contrastive_targets =[]\n",
    "            for label in batch_diff_targets:\n",
    "                if(label<=-2.0):\n",
    "                    batch_contrastive_targets.append(1)\n",
    "                elif(label>=2.0):\n",
    "                    batch_contrastive_targets.append(1)\n",
    "                else:\n",
    "                    batch_contrastive_targets.append(0)\n",
    "            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n",
    "\n",
    "\n",
    "        all_gene_ids[start:end]=Sample['geneID']\n",
    "        batch_size = inputs_1.size(0)\n",
    "\n",
    "        if(AUX==False):\n",
    "            # for raw models: raw_d, raw_c, raw\n",
    "            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
    "            all_attention_bin[start:end]=batch_alpha.data\n",
    "            all_attention_hm[start:end]=batch_beta.data\n",
    "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
    "        elif(CON==False):\n",
    "            # for aux models\n",
    "            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
    "            all_attention_bin[start:end]=batch_alpha.data\n",
    "            all_attention_hm[start:end]=batch_beta.data\n",
    "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
    "            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n",
    "            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n",
    "        else:\n",
    "            # for aux and siamese models\n",
    "            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
    "\n",
    "            all_attention_bin[start:end]=batch_alpha.data\n",
    "            all_attention_hm[start:end]=batch_beta.data\n",
    "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
    "            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n",
    "            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n",
    "            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n",
    "\n",
    "        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n",
    "        per_epoch_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
    "        optimizer.step()\n",
    "    per_epoch_loss=per_epoch_loss/num_batches\n",
    "    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n",
    "\n",
    "\n",
    "\n",
    "def test(ValidData):\n",
    "    model.eval()\n",
    "\n",
    "    diff_targets = torch.zeros(ValidData.dataset.__len__(),1)\n",
    "    diff_predictions = torch.zeros(diff_targets.size(0),1)\n",
    "    if(args.model_name=='raw_d'):\n",
    "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n",
    "    elif(args.model_name=='raw_c'):\n",
    "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n",
    "    elif(args.model_name=='raw'):\n",
    "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),3*args.n_hms)\n",
    "    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n",
    "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n",
    "    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n",
    "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),5*args.n_hms)\n",
    "    else:\n",
    "        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n",
    "        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n",
    "\n",
    "    num_batches = int(math.ceil(ValidData.dataset.__len__()/float(args.batch_size)))\n",
    "    all_gene_ids=[None]*ValidData.dataset.__len__()\n",
    "    per_epoch_loss = 0\n",
    "    for idx, Sample in enumerate(ValidData):\n",
    "        if(idx%100==0):\n",
    "            print('TESTING ON BATCH:',idx)\n",
    "        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, ValidData.dataset.__len__())\n",
    "        optimizer.zero_grad()\n",
    "        # get HM profiles\n",
    "        inputs_1 = Sample['X_A']\n",
    "        inputs_2 = Sample['X_B']\n",
    "        # get targets: both differential and cell specific expression\n",
    "        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n",
    "        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n",
    "        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n",
    "        diff_targets[start:end,0] = batch_diff_targets[:,0]\n",
    "\n",
    "        if(CON==True):\n",
    "            # get labels for contrastive loss\n",
    "            batch_contrastive_targets =[]\n",
    "            for label in batch_diff_targets:\n",
    "                if(label<=-2.0):\n",
    "                    batch_contrastive_targets.append(1)\n",
    "                elif(label>=2.0):\n",
    "                    batch_contrastive_targets.append(1)\n",
    "                else:\n",
    "                    batch_contrastive_targets.append(0)\n",
    "            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n",
    "\n",
    "\n",
    "        all_gene_ids[start:end]=Sample['geneID']\n",
    "        batch_size = inputs_1.size(0)\n",
    "\n",
    "        if(AUX==False):\n",
    "            # for raw models: raw_d, raw_c, raw\n",
    "            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
    "            all_attention_bin[start:end]=batch_alpha.data\n",
    "            all_attention_hm[start:end]=batch_beta.data\n",
    "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
    "        elif(CON==False):\n",
    "            # for aux models\n",
    "            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
    "            all_attention_bin[start:end]=batch_alpha.data\n",
    "            all_attention_hm[start:end]=batch_beta.data\n",
    "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
    "            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n",
    "            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n",
    "        else:\n",
    "            # for aux and siamese models\n",
    "            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n",
    "\n",
    "            all_attention_bin[start:end]=batch_alpha.data\n",
    "            all_attention_hm[start:end]=batch_beta.data\n",
    "            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n",
    "            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n",
    "            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n",
    "            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n",
    "\n",
    "        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n",
    "        per_epoch_loss += loss.item()\n",
    "    per_epoch_loss=per_epoch_loss/num_batches\n",
    "    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "best_valid_loss = 10000000000\n",
    "best_valid_MSE=100000\n",
    "best_valid_R2=-1\n",
    "if(args.test_on_saved_model==False):\n",
    "    for epoch in range(0, args.epochs):\n",
    "        print('=---------------------------------------- Training '+str(epoch+1)+' -----------------------------------=')\n",
    "        diff_predictions,diff_targets,alpha_train,beta_train,train_loss,_ = train(Train)\n",
    "        train_MSE, train_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n",
    "        diff_predictions,diff_targets,alpha_valid,beta_valid,valid_loss,gene_ids_valid = test(Valid)\n",
    "        valid_MSE, valid_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n",
    "\n",
    "        if(valid_R2 >= best_valid_R2):\n",
    "                # save best epoch -- models converge early\n",
    "            best_valid_R2=valid_R2\n",
    "            torch.save(model,model_dir+\"/\"+model_name+'_R2_model.pt')\n",
    "\n",
    "        print(\"Epoch:\",epoch)\n",
    "        print(\"train R2:\",train_R2)\n",
    "        print(\"valid R2:\",valid_R2)\n",
    "        print(\"best valid R2:\", best_valid_R2)\n",
    "\n",
    " \n",
    "    print(\"finished training!!\")\n",
    "    print(\"best validation R2:\",best_valid_R2)\n",
    "    print(\"testing\")\n",
    "    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n",
    "\n",
    "    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n",
    "    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n",
    "    print(\"test R2:\",test_R2)\n",
    "\n",
    "    if(args.save_attention_maps):\n",
    "        attentionfile=open(attentionmapfile,'w')\n",
    "        attentionfilewriter=csv.writer(attentionfile)\n",
    "        beta_test=beta_test.numpy()\n",
    "        for i in range(len(gene_ids_test)):\n",
    "            gene_attention=[]\n",
    "            gene_attention.append(gene_ids_test[i])\n",
    "            for e in beta_test[i,:]:\n",
    "                gene_attention.append(str(e))\n",
    "            attentionfilewriter.writerow(gene_attention)\n",
    "        attentionfile.close()\n",
    "\n",
    "\n",
    "else:\n",
    "    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n",
    "    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n",
    "    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n",
    "    print(\"test R2:\",test_R2)\n",
    "\n",
    "    if(args.save_attention_maps):\n",
    "        attentionfile=open(attentionmapfile,'w')\n",
    "        attentionfilewriter=csv.writer(attentionfile)\n",
    "        beta_test=beta_test.numpy()\n",
    "        for i in range(len(gene_ids_test)):\n",
    "            gene_attention=[]\n",
    "            gene_attention.append(gene_ids_test[i])\n",
    "            for e in beta_test[i,:]:\n",
    "                gene_attention.append(str(e))\n",
    "            attentionfilewriter.writerow(gene_attention)\n",
    "        attentionfile.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "train_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
