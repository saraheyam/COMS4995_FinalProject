{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_v1.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"LwU1-qE9n9xc","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","import argparse\n","import json\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import cuda\n","import sys, os\n","import random\n","import numpy as np\n","from sklearn import metrics\n","import models as Model\n","from SiameseLoss import ContrastiveLoss\n","import evaluate\n","import data\n","import gc\n","import csv\n","\n","parser = argparse.ArgumentParser(description='DeepDiff')\n","parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate')\n","parser.add_argument('--model_name', type=str, default='raw_d', help='DeepDiff variation')\n","parser.add_argument('--clip', type=float, default=1,help='gradient clipping')\n","parser.add_argument('--epochs', type=int, default=90, help='upper epoch limit')\n","parser.add_argument('--batch_size', type=int, default=10, help='')\n","parser.add_argument('--dropout', type=float, default=0.5, help='dropout applied to layers (0 = no dropout) if n_layers LSTM > 1')\n","parser.add_argument('--cell_1', type=str, default='Cell1', help='cell type 1')\n","parser.add_argument('--cell_2', type=str, default='Cell2', help='cell type 2')\n","parser.add_argument('--save_root', type=str, default='./Results/', help='where to save')\n","parser.add_argument('--data_root', type=str, default='./data/', help='data location')\n","parser.add_argument('--gpuid', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--gpu', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--n_hms', type=int, default=5, help='number of histone modifications')\n","parser.add_argument('--n_bins', type=int, default=200, help='number of bins')\n","parser.add_argument('--bin_rnn_size', type=int, default=32, help='bin rnn size')\n","parser.add_argument('--num_layers', type=int, default=1, help='number of layers')\n","parser.add_argument('--unidirectional', action='store_true', help='bidirectional/undirectional LSTM')\n","parser.add_argument('--save_attention_maps',action='store_true', help='set to save validation beta attention maps')\n","parser.add_argument('--attentionfilename', type=str, default='beta_attention.txt', help='where to save attnetion maps')\n","parser.add_argument('--test_on_saved_model',action='store_true', help='only test on saved model')\n","args = parser.parse_args()\n","\n","torch.manual_seed(1)\n","\n","\n","\n","model_name = ''\n","model_name += (args.cell_1)+('_')+(args.cell_2)+('_')\n","\n","model_name+=args.model_name\n","\n","\n","\n","\n","args.bidirectional=not args.unidirectional\n","\n","print('the model name: ',model_name)\n","args.data_root+=''\n","args.save_root+=''\n","args.dataset=args.cell_1+('_')+args.cell_2\n","args.data_root = os.path.join(args.data_root)\n","print('loading data from:  ',args.data_root)\n","args.save_root = os.path.join(args.save_root,args.dataset)\n","print('saving results in  from: ',args.save_root)\n","model_dir = os.path.join(args.save_root,model_name)\n","if not os.path.exists(model_dir):\n","\tos.makedirs(model_dir)\n","attentionmapfile=model_dir+'/'+args.attentionfilename\n","print('==>processing data')\n","Train,Valid,Test = data.load_data(args)\n","\n","\n","\n","\n","\n","\n","\n","CON=False\n","AUX=False\n","print('==>building model')\n","if(args.model_name=='raw_d'):\n","\tmodel = Model.raw_d(args)\n","elif(args.model_name=='raw_c'):\n","\tmodel = Model.raw_c(args)\n","elif(args.model_name=='raw'):\n","\tmodel = Model.raw(args)\n","elif(args.model_name=='aux'):\n","\targs.shared=False\n","\tmodel = Model.aux(args)\n","\tAUX=True\n","\targs.gamma=0.0\n","elif(args.model_name=='raw_aux'):\n","\targs.shared=False\n","\tmodel = Model.raw_aux(args)\n","\tAUX=True\n","\targs.gamma=0.0\n","elif(args.model_name=='aux_siamese'):\n","\tCON=True\n","\targs.shared=True\n","\tmodel = Model.aux_siamese(args)\n","\tAUX=True\n","\targs.gamma=4.0\n","elif(args.model_name=='raw_aux_siamese'):\n","\tCON=True\n","\targs.shared=True\n","\tmodel = Model.raw_aux_siamese(args)\n","\tAUX=True\n","\targs.gamma=4.0\n","else:\n","\tsys.exit(\"invalid model name\")\n","\n","\n","if torch.cuda.device_count() > 1:\n","\ttorch.cuda.manual_seed_all(1)\n","\tdtype = torch.cuda.FloatTensor\n","\tcuda.set_device(args.gpuid)\n","\tmodel.type(dtype)\n","\tprint('Using GPU '+str(args.gpuid))\n","else:\n","\tdtype = torch.FloatTensor\n","\n","print(model)\n","if(args.test_on_saved_model==False):\n","\tprint(\"==>initializing a new model\")\n","\tfor p in model.parameters():\n","\t\tp.data.uniform_(-0.1,0.1)\n","\n","DiffLoss = nn.MSELoss(size_average=True).type(dtype)\n","AuxLoss = nn.MSELoss(size_average=True).type(dtype)\n","ConLoss = ContrastiveLoss().type(dtype)\n","\n","optimizer = optim.Adam(model.parameters(), lr = args.lr)\n","#optimizer = optim.SGD(model.parameters(), lr = args.lr, momentum=args.momentum)\n","def train(TrainData):\n","\tmodel.train()\n","\t# initialize attention\n","\tdiff_targets = torch.zeros(TrainData.dataset.__len__(),1)\n","\tdiff_predictions = torch.zeros(diff_targets.size(0),1)\n","\tif(args.model_name=='raw_d'):\n","\t\tall_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","\telif(args.model_name=='raw_c'):\n","\t\tall_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","\telif(args.model_name=='raw'):\n","\t\tall_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(TrainData.dataset.__len__(),3*args.n_hms)\n","\n","\telif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","\t\tall_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","\n","\telif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","\t\tall_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(TrainData.dataset.__len__(),5*args.n_hms)\n","\n","\telse:\n","\t\tall_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","\n","\tnum_batches = int(math.ceil(TrainData.dataset.__len__()/float(args.batch_size)))\n","\tall_gene_ids=[None]*TrainData.dataset.__len__()\n","\tper_epoch_loss = 0\n","\tfor idx, Sample in enumerate(TrainData):\n","\t\tif(idx%100==0):\n","\t\t\tprint('TRAINING ON BATCH:',idx)\n","\t\tstart,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, TrainData.dataset.__len__())\n","\t\toptimizer.zero_grad()\n","\t\t# get HM profiles\n","\t\tinputs_1 = Sample['X_A']\n","\t\tinputs_2 = Sample['X_B']\n","\t\t# get targets: both differential and cell specific expression\n","\t\tbatch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","\t\tbatch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","\t\tbatch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","\t\tdiff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","\t\tif(CON==True):\n","\t\t\t# get labels for contrastive loss\n","\t\t\tbatch_contrastive_targets =[]\n","\t\t\tfor label in batch_diff_targets:\n","\t\t\t\tif(label<=-2.0):\n","\t\t\t\t\tbatch_contrastive_targets.append(1)\n","\t\t\t\telif(label>=2.0):\n","\t\t\t\t\tbatch_contrastive_targets.append(1)\n","\t\t\t\telse:\n","\t\t\t\t\tbatch_contrastive_targets.append(0)\n","\t\t\tbatch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","\t\tall_gene_ids[start:end]=Sample['geneID']\n","\t\tbatch_size = inputs_1.size(0)\n","\n","\t\tif(AUX==False):\n","\t\t\t# for raw models: raw_d, raw_c, raw\n","\t\t\tbatch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\t\t\tall_attention_bin[start:end]=batch_alpha.data\n","\t\t\tall_attention_hm[start:end]=batch_beta.data\n","\t\t\tloss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","\t\telif(CON==False):\n","\t\t\t# for aux models\n","\t\t\tbatch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\t\t\tall_attention_bin[start:end]=batch_alpha.data\n","\t\t\tall_attention_hm[start:end]=batch_beta.data\n","\t\t\tloss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","\t\t\tloss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","\t\t\tloss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","\t\telse:\n","\t\t\t# for aux and siamese models\n","\t\t\tbatch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","\t\t\tall_attention_bin[start:end]=batch_alpha.data\n","\t\t\tall_attention_hm[start:end]=batch_beta.data\n","\t\t\tloss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","\t\t\tloss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","\t\t\tloss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","\t\t\tloss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","\t\tdiff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","\t\tper_epoch_loss += loss.item()\n","\t\tloss.backward()\n","\t\ttorch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n","\t\toptimizer.step()\n","\tper_epoch_loss=per_epoch_loss/num_batches\n","\treturn diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","def test(ValidData):\n","\tmodel.eval()\n","\n","\tdiff_targets = torch.zeros(ValidData.dataset.__len__(),1)\n","\tdiff_predictions = torch.zeros(diff_targets.size(0),1)\n","\tif(args.model_name=='raw_d'):\n","\t\tall_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","\telif(args.model_name=='raw_c'):\n","\t\tall_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","\telif(args.model_name=='raw'):\n","\t\tall_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(ValidData.dataset.__len__(),3*args.n_hms)\n","\telif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","\t\tall_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","\telif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","\t\tall_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(ValidData.dataset.__len__(),5*args.n_hms)\n","\telse:\n","\t\tall_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","\t\tall_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","\n","\tnum_batches = int(math.ceil(ValidData.dataset.__len__()/float(args.batch_size)))\n","\tall_gene_ids=[None]*ValidData.dataset.__len__()\n","\tper_epoch_loss = 0\n","\tfor idx, Sample in enumerate(ValidData):\n","\t\tif(idx%100==0):\n","\t\t\tprint('TESTING ON BATCH:',idx)\n","\t\tstart,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, ValidData.dataset.__len__())\n","\t\toptimizer.zero_grad()\n","\t\t# get HM profiles\n","\t\tinputs_1 = Sample['X_A']\n","\t\tinputs_2 = Sample['X_B']\n","\t\t# get targets: both differential and cell specific expression\n","\t\tbatch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","\t\tbatch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","\t\tbatch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","\t\tdiff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","\t\tif(CON==True):\n","\t\t\t# get labels for contrastive loss\n","\t\t\tbatch_contrastive_targets =[]\n","\t\t\tfor label in batch_diff_targets:\n","\t\t\t\tif(label<=-2.0):\n","\t\t\t\t\tbatch_contrastive_targets.append(1)\n","\t\t\t\telif(label>=2.0):\n","\t\t\t\t\tbatch_contrastive_targets.append(1)\n","\t\t\t\telse:\n","\t\t\t\t\tbatch_contrastive_targets.append(0)\n","\t\t\tbatch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","\t\tall_gene_ids[start:end]=Sample['geneID']\n","\t\tbatch_size = inputs_1.size(0)\n","\n","\t\tif(AUX==False):\n","\t\t\t# for raw models: raw_d, raw_c, raw\n","\t\t\tbatch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\t\t\tall_attention_bin[start:end]=batch_alpha.data\n","\t\t\tall_attention_hm[start:end]=batch_beta.data\n","\t\t\tloss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","\t\telif(CON==False):\n","\t\t\t# for aux models\n","\t\t\tbatch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\t\t\tall_attention_bin[start:end]=batch_alpha.data\n","\t\t\tall_attention_hm[start:end]=batch_beta.data\n","\t\t\tloss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","\t\t\tloss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","\t\t\tloss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","\t\telse:\n","\t\t\t# for aux and siamese models\n","\t\t\tbatch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","\t\t\tall_attention_bin[start:end]=batch_alpha.data\n","\t\t\tall_attention_hm[start:end]=batch_beta.data\n","\t\t\tloss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","\t\t\tloss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","\t\t\tloss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","\t\t\tloss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","\t\tdiff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","\t\tper_epoch_loss += loss.item()\n","\tper_epoch_loss=per_epoch_loss/num_batches\n","\treturn diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","\n","\n","\n","\n","best_valid_loss = 10000000000\n","best_valid_MSE=100000\n","best_valid_R2=-1\n","if(args.test_on_saved_model==False):\n","\tfor epoch in range(0, args.epochs):\n","\t\tprint('=---------------------------------------- Training '+str(epoch+1)+' -----------------------------------=')\n","\t\tdiff_predictions,diff_targets,alpha_train,beta_train,train_loss,_ = train(Train)\n","\t\ttrain_MSE, train_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","\t\tdiff_predictions,diff_targets,alpha_valid,beta_valid,valid_loss,gene_ids_valid = test(Valid)\n","\t\tvalid_MSE, valid_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","\n","\t\tif(valid_R2 >= best_valid_R2):\n","\t\t\t\t# save best epoch -- models converge early\n","\t\t\tbest_valid_R2=valid_R2\n","\t\t\ttorch.save(model,model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","\t\tprint(\"Epoch:\",epoch)\n","\t\tprint(\"train R2:\",train_R2)\n","\t\tprint(\"valid R2:\",valid_R2)\n","\t\tprint(\"best valid R2:\", best_valid_R2)\n","\n"," \n","\tprint(\"finished training!!\")\n","\tprint(\"best validation R2:\",best_valid_R2)\n","\tprint(\"testing\")\n","\tmodel=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","\tdiff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","\ttest_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","\tprint(\"test R2:\",test_R2)\n","\n","\tif(args.save_attention_maps):\n","\t\tattentionfile=open(attentionmapfile,'w')\n","\t\tattentionfilewriter=csv.writer(attentionfile)\n","\t\tbeta_test=beta_test.numpy()\n","\t\tfor i in range(len(gene_ids_test)):\n","\t\t\tgene_attention=[]\n","\t\t\tgene_attention.append(gene_ids_test[i])\n","\t\t\tfor e in beta_test[i,:]:\n","\t\t\t\tgene_attention.append(str(e))\n","\t\t\tattentionfilewriter.writerow(gene_attention)\n","\t\tattentionfile.close()\n","\n","\n","else:\n","\tmodel=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","\tdiff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","\ttest_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","\tprint(\"test R2:\",test_R2)\n","\n","\tif(args.save_attention_maps):\n","\t\tattentionfile=open(attentionmapfile,'w')\n","\t\tattentionfilewriter=csv.writer(attentionfile)\n","\t\tbeta_test=beta_test.numpy()\n","\t\tfor i in range(len(gene_ids_test)):\n","\t\t\tgene_attention=[]\n","\t\t\tgene_attention.append(gene_ids_test[i])\n","\t\t\tfor e in beta_test[i,:]:\n","\t\t\t\tgene_attention.append(str(e))\n","\t\t\tattentionfilewriter.writerow(gene_attention)\n","\t\tattentionfile.close()"],"execution_count":0,"outputs":[]}]}