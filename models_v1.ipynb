{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"models_v1.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"dswCWzAlo-P_","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","import argparse\n","import torch\n","import csv\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","import numpy as np\n","\n","def batch_product(iput, mat2):\n","\t\tresult = None\n","\t\tfor i in range(iput.size()[0]):\n","\t\t\top = torch.mm(iput[i], mat2)\n","\t\t\top = op.unsqueeze(0)\n","\t\t\tif(result is None):\n","\t\t\t\tresult = op\n","\t\t\telse:\n","\t\t\t\tresult = torch.cat((result,op),0)\n","\t\treturn result.squeeze(2)\n","\n","\n","class rec_attention(nn.Module):\n","\t# attention with bin context vector per HM and HM context vector\n","\tdef __init__(self,hm,args):\n","\t\tsuper(rec_attention,self).__init__()\n","\t\tself.num_directions=2 if args.bidirectional else 1\n","\t\tif (hm==False):\n","\t\t\tself.bin_rep_size=args.bin_rnn_size*self.num_directions\n","\t\telse:\n","\t\t\tself.bin_rep_size=args.bin_rnn_size\n","\t\n","\t\tself.bin_context_vector=nn.Parameter(torch.Tensor(self.bin_rep_size,1),requires_grad=True)\n","\t\n","\n","\t\tself.softmax=nn.Softmax()\n","\n","\t\tself.bin_context_vector.data.uniform_(-0.1, 0.1)\n","\n","\tdef forward(self,iput):\n","\t\talpha=self.softmax(batch_product(iput,self.bin_context_vector))\n","\t\t[batch_size,source_length,bin_rep_size2]=iput.size()\n","\t\trepres=torch.bmm(alpha.unsqueeze(2).view(batch_size,-1,source_length),iput)\n","\t\treturn repres,alpha\n","\n","\n","\n","class recurrent_encoder(nn.Module):\n","\t# modular LSTM encoder\n","\tdef __init__(self,n_bins,ip_bin_size,hm,args):\n","\t\tsuper(recurrent_encoder,self).__init__()\n","\t\tself.bin_rnn_size=args.bin_rnn_size\n","\t\tself.ipsize=ip_bin_size\n","\t\tself.seq_length=n_bins\n","\n","\t\tself.num_directions=2 if args.bidirectional else 1\n","\t\tif (hm==False):\n","\t\t\tself.bin_rnn_size=args.bin_rnn_size\n","\t\telse:\n","\t\t\tself.bin_rnn_size=args.bin_rnn_size // 2\n","\t\tself.bin_rep_size=self.bin_rnn_size*self.num_directions\n","\n","\n","\t\tself.rnn=nn.LSTM(self.ipsize,self.bin_rnn_size,num_layers=args.num_layers,dropout=args.dropout,bidirectional=args.bidirectional)\n","\n","\t\tself.bin_attention=rec_attention(hm,args)\n","\tdef outputlength(self):\n","\t\treturn self.bin_rep_size\n","\tdef forward(self,single_hm,hidden=None):\n","\n","\t\tbin_output, hidden = self.rnn(single_hm,hidden)\n","\t\tbin_output = bin_output.permute(1,0,2)\n","\t\thm_rep,bin_alpha = self.bin_attention(bin_output)\n","\t\treturn hm_rep,bin_alpha\n","\n","class raw_d(nn.Module):\n","\tdef __init__(self,args):\n","\t\tself.n_hms=args.n_hms\n","\t\tself.n_bins=args.n_bins\n","\t\tself.ip_bin_size=1\n","\t\tsuper(raw_d,self).__init__()\n","\t\tself.rnn_hms=nn.ModuleList()\n","\t\tfor i in range(self.n_hms):\n","\t\t\tself.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.opsize = self.rnn_hms[0].outputlength()\n","\t\tself.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","\t\tself.opsize2=self.hm_level_rnn_1.outputlength()\n","\t\tself.diffopsize=2*(self.opsize2)\n","\t\tself.fdiff1_1=nn.Linear(self.opsize2,1)\n","\n","\tdef forward(self,iput1,iput2):\n","\n","\t\tiput=iput1-iput2\n","\t\tbin_a=None\n","\t\tlevel1_rep=None\n","\t\t[batch_size,_,_]=iput.size()\n","\n","\t\tfor hm,hm_encdr in enumerate(self.rnn_hms):\n","\t\t\thmod=iput[:,:,hm].contiguous()\n","\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\n","\t\t\top,a= hm_encdr(hmod)\n","\t\t\tif level1_rep is None:\n","\t\t\t\tlevel1_rep=op\n","\t\t\t\tbin_a=a\n","\t\t\telse:\n","\t\t\t\tlevel1_rep=torch.cat((level1_rep,op),1)\n","\t\t\t\tbin_a=torch.cat((bin_a,a),1)\n","\t\tlevel1_rep=level1_rep.permute(1,0,2)\n","\t\tfinal_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n","\t\tfinal_rep_1=final_rep_1.squeeze(1)\n","\t\tprediction_m=((self.fdiff1_1(final_rep_1)))\n","\t\treturn prediction_m,hm_level_attention_1, bin_a\n","\n","\n","class raw_c(nn.Module):\n","\tdef __init__(self,args):\n","\t\tsuper(raw_c,self).__init__()\n","\t\tself.n_hms=args.n_hms\n","\t\tself.n_bins=args.n_bins\n","\t\tself.ip_bin_size=1\n","\t\tself.joint=False\n","\t\tself.rnn_hms=nn.ModuleList()\n","\t\tfor i in range(2*self.n_hms):\n","\t\t\tself.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.opsize = self.rnn_hms[0].outputlength()\n","\t\tself.hm_level_rnn_1=recurrent_encoder(2*self.n_hms,self.opsize,True,args)\n","\t\tself.opsize2=self.hm_level_rnn_1.outputlength()\n","\t\tself.diffopsize=2*(self.opsize2)\n","\t\tself.fdiff1_1=nn.Linear(self.opsize2,1)\n","\n","\tdef forward(self,iput1,iput2):\n","\t\tiput=torch.cat((iput1,iput2),2)\n","\t\tbin_a=None\n","\t\tlevel1_rep=None\n","\t\tfor hm,hm_encdr in enumerate(self.rnn_hms):\n","\t\t\thmod=iput[:,:,hm].contiguous()\n","\n","\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\t\t\top,a= hm_encdr(hmod)\n","\n","\t\t\tif level1_rep is None:\n","\t\t\t\tlevel1_rep=op\n","\t\t\t\tbin_a=a\n","\t\t\telse:\n","\t\t\t\tlevel1_rep=torch.cat((level1_rep,op),1)\n","\t\t\t\tbin_a=torch.cat((bin_a,a),1)\n","\n","\t\tlevel1_rep=level1_rep.permute(1,0,2)\n","\t\tfinal_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n","\t\tfinal_rep_1=final_rep_1.squeeze(1)\n","\t\tprediction_m=((self.fdiff1_1(final_rep_1)))\n","\n","\t\treturn prediction_m,hm_level_attention_1, bin_a\n","\n","\n","class raw(nn.Module):\n","\t# Model with all raw features: difference and absolute features\n","\tdef __init__(self,args):\n","\t\tsuper(raw,self).__init__()\n","\t\tself.n_hms=args.n_hms\n","\t\tself.n_bins=args.n_bins\n","\t\tself.ip_bin_size=1\n","\t\tself.rnn_hms=nn.ModuleList()\n","\t\tfor i in range(3*self.n_hms):\n","\t\t\tself.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.opsize = self.rnn_hms[0].outputlength()\n","\t\tself.hm_level_rnn_1=recurrent_encoder(3*self.n_hms,self.opsize,True,args)\n","\t\tself.opsize2=self.hm_level_rnn_1.outputlength()\n","\t\tself.diffopsize=2*(self.opsize2)\n","\t\tself.fdiff1_1=nn.Linear(self.opsize2,1)\n","\n","\tdef forward(self,iput1,iput2):\n","\t\tiput3=iput1-iput2\n","\t\tiput4=torch.cat((iput1,iput2),2)\n","\t\tiput=(torch.cat((iput4,iput3),2))\n","\n","\t\tbin_a=None\n","\t\tlevel1_rep=None\n","\t\tfor hm,hm_encdr in enumerate(self.rnn_hms):\n","\t\t\thmod=iput[:,:,hm].contiguous()\n","\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\t\t\top,a= hm_encdr(hmod)\n","\t\t\tif level1_rep is None:\n","\t\t\t\tlevel1_rep=op\n","\t\t\t\tbin_a=a\n","\t\t\telse:\n","\t\t\t\tlevel1_rep=torch.cat((level1_rep,op),1)\n","\t\t\t\tbin_a=torch.cat((bin_a,a),1)\n","\n","\t\tlevel1_rep=level1_rep.permute(1,0,2)\n","\t\tfinal_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n","\t\tfinal_rep_1=final_rep_1.squeeze(1)\n","\n","\t\tprediction_m=((self.fdiff1_1(final_rep_1)))\n","\t\treturn prediction_m,hm_level_attention_1, bin_a\n","\n","\n","class aux(nn.Module):\n","\tdef __init__(self,args):\n","\t\tsuper(aux,self).__init__()\n","\t\tself.n_hms=args.n_hms\n","\t\tself.n_bins=args.n_bins\n","\t\tself.ip_bin_size=1\n","\t\tself.joint=False\n","\t\tself.shared=False\n","\t\tself.rnn_hms=nn.ModuleList()\n","\t\tfor i in range(self.n_hms):\n","\t\t\tself.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.opsize = self.rnn_hms[0].outputlength()\n","\t\tself.rnn_hms2=nn.ModuleList()\n","\t\tfor i in range(self.n_hms):\n","\t\t\tself.rnn_hms2.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","\t\tself.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","\t\tself.opsize2=self.hm_level_rnn_1.outputlength()\n","\t\tself.f1_1=nn.Linear(self.opsize2,1)\n","\t\tself.f2_1=nn.Linear(self.opsize2,1)\n","\t\tself.diffopsize=2*(self.opsize2)\n","\t\tself.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n","\t\tself.predictor_=nn.Linear(self.diffopsize//2,1)\n","\t\tself.relu=nn.ReLU()\n","\n","\tdef forward_once(self,iput,shared,cellid):\n","\t\tbin_a=None\n","\t\tlevel1_rep=None\n","\t\tif(shared or cellid==1):\n","\t\t\tfor hm,hm_encdr in enumerate(self.rnn_hms):\n","\t\t\t\thmod=iput[:,:,hm].contiguous()\n","\t\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\t\t\t\top,a= hm_encdr(hmod)\n","\t\t\t\tif level1_rep is None:\n","\t\t\t\t\tlevel1_rep=op\n","\t\t\t\t\tbin_a=a\n","\t\t\t\telse:\n","\t\t\t\t\tlevel1_rep=torch.cat((level1_rep,op),1)\n","\t\t\t\t\tbin_a=torch.cat((bin_a,a),1)\n","\t\t\tlevel1_rep=level1_rep.permute(1,0,2)\n","\t\telse:\n","\t\t\tfor hm,hm_encdr in enumerate(self.rnn_hms2):\n","\n","\t\t\t\thmod=iput[:,:,hm].contiguous()\n","\t\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\t\t\t\top,a= hm_encdr(hmod)\n","\t\t\t\tif level1_rep is None:\n","\t\t\t\t\tlevel1_rep=op\n","\t\t\t\t\tbin_a=a\n","\t\t\t\telse:\n","\t\t\t\t\tlevel1_rep=torch.cat((level1_rep,op),1)\n","\t\t\t\t\tbin_a=torch.cat((bin_a,a),1)\n","\t\t\tlevel1_rep=level1_rep.permute(1,0,2)\n","\t\treturn level1_rep,bin_a\n","\n","\tdef forward(self,iput1,iput2):\n"," \n","\t\tlevel1_rep1,bin_a1=self.forward_once(iput1,self.shared,1)\n","\t\tlevel1_rep2,bin_a2=self.forward_once(iput2,self.shared,2)\n","\t\tfinal_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","\t\tif(self.joint):\n","\t\t\tfinal_rep_2,hm_level_attention_2=self.hm_level_rnn_1(level1_rep2)\n","\t\telse:\n","\t\t\tfinal_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","\t\tfinal_rep_1=final_rep_1.squeeze(1)\n","\t\tfinal_rep_2=final_rep_2.squeeze(1)\n","\t\tprediction1=((self.f1_1(final_rep_1)))\n","\t\tprediction2=((self.f2_1(final_rep_2)))\n","\n","\t\tmlp_input=torch.cat((final_rep_1,final_rep_2),1)\n","\t\tmlp_input=self.relu(self.predictor_1(mlp_input))\n","\t\tprediction=(self.predictor_(mlp_input))\n","\t\thm_level_attention=torch.cat((hm_level_attention_1,hm_level_attention_2),1)\n","\t\tbin_a=torch.cat((bin_a1,bin_a2),1)\n","\n","\t\treturn prediction,hm_level_attention,bin_a,prediction1,prediction2\n","\n","\n","class raw_aux(nn.Module):\n","\t# level 1 takes raw feaures, level 2 takes aux features as well as raw feature embeddings from level 1\n","\t# returns attention scores from raw part only\n","\tdef __init__(self,args):\n","\t\tsuper(raw_aux,self).__init__()\n","\t\tself.n_hms=args.n_hms\n","\t\tself.n_bins=args.n_bins\n","\t\tself.ip_bin_size=1\n","\t\tself.joint=False\n","\t\tself.shared=False\n","\t\tself.rnn_hms=nn.ModuleList()\n","\t\tfor i in range(self.n_hms):\n","\t\t\tself.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.opsize = self.rnn_hms[0].outputlength()\n","\t\tself.rnn_hms3=nn.ModuleList()\n","\t\tfor i in range(3*self.n_hms):\n","\t\t\tself.rnn_hms3.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.opsize = self.rnn_hms[0].outputlength()\n","\t\tself.rnn_hms2=nn.ModuleList()\n","\t\tfor i in range(self.n_hms):\n","\t\t\tself.rnn_hms2.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","\t\tself.hm_level_rnn_3=recurrent_encoder(5*self.n_hms,self.opsize,True,args)\n","\t\tself.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","\t\tself.opsize2=self.hm_level_rnn_1.outputlength()\n","\t\tself.f1_1=nn.Linear(self.opsize2,1)\n","\t\tself.f2_1=nn.Linear(self.opsize2,1)\n","\t\tself.diffopsize=3*(self.opsize2)\n","\t\tself.predictor_=nn.Linear(self.opsize2,1)\n","\t\tself.relu=nn.ReLU()\n","\n","\n","\tdef forward(self,iput1,iput2):\n","\t\tiput3=iput1-iput2\n","\t\tiput4=torch.cat((iput1,iput2),2)\n","\t\tiput=(torch.cat((iput4,iput3),2))\n","\t\tbin_a=None\n","\t\tlevel1_rep=None\n","\t\t[batch_size,_,_]=iput.size()\n","\t\tfor hm,hm_encdr in enumerate(self.rnn_hms3):\n","\n","\t\t\thmod=iput[:,:,hm].contiguous()\n","\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\t\t\top,a= hm_encdr(hmod)\n","\t\t\tif level1_rep is None:\n","\t\t\t\tlevel1_rep=op\n","\t\t\t\tbin_a=a\n","\t\t\telse:\n","\t\t\t\tlevel1_rep=torch.cat((level1_rep,op),1)\n","\t\t\t\tbin_a=torch.cat((bin_a,a),1)\n","\t\tlevel1_rep=level1_rep.permute(1,0,2)\n","\n","\t\tbin_a1=None\n","\t\tlevel1_rep1=None\n","\t\t[batch_size,_,_]=iput1.size()\n","\t\tfor hm,hm_encdr in enumerate(self.rnn_hms):\n","\t\t\thmod=iput1[:,:,hm].contiguous()\n","\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\t\t\top,a= hm_encdr(hmod)\n","\t\t\tif level1_rep1 is None:\n","\t\t\t\tlevel1_rep1=op\n","\t\t\t\tbin_a1=a\n","\t\t\telse:\n","\t\t\t\tlevel1_rep1=torch.cat((level1_rep1,op),1)\n","\t\t\t\tbin_a1=torch.cat((bin_a1,a),1)\n","\n","\t\tlevel1_rep1=level1_rep1.permute(1,0,2)\n","\t\tbin_a2=None\n","\t\tlevel1_rep2=None\n","\n","\t\tfor hm,hm_encdr in enumerate(self.rnn_hms2):\n","\t\t\thmod=iput2[:,:,hm].contiguous()\n","\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\t\t\top,a= hm_encdr(hmod)\n","\t\t\tif level1_rep2 is None:\n","\t\t\t\tlevel1_rep2=op\n","\t\t\t\tbin_a2=a\n","\t\t\telse:\n","\t\t\t\tlevel1_rep2=torch.cat((level1_rep2,op),1)\n","\t\t\t\tbin_a2=torch.cat((bin_a2,a),1)\n","\t\tlevel1_rep2=level1_rep2.permute(1,0,2)\n","\t\tlevel1_rep3=torch.cat((level1_rep,level1_rep1,level1_rep2),0)\n","\t\tfinal_rep_3,hm_level_attention_3=self.hm_level_rnn_3(level1_rep3)\n","\t\tfinal_rep_3=final_rep_3.squeeze(1)\n","\n","\t\tfinal_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","\t\tfinal_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","\t\tfinal_rep_1=final_rep_1.squeeze(1)\n","\t\tfinal_rep_2=final_rep_2.squeeze(1)\n","\t\tprediction1=((self.f1_1(final_rep_1)))\n","\t\tprediction2=((self.f2_1(final_rep_2)))\n","\t\tprediction=(self.predictor_(final_rep_3))\n","\t\treturn prediction,hm_level_attention_3,bin_a,prediction1,prediction2\n","\n","\n","class aux_siamese(nn.Module):\n","\t# aux with siamese  same as aux model mostly, but with shared level 1 embedding\n","\tdef __init__(self,args):\n","\t\tsuper(aux_siamese,self).__init__()\n","\t\tself.n_hms=args.n_hms\n","\t\tself.n_bins=args.n_bins\n","\t\tself.ip_bin_size=1\n","\t\tself.joint=False\n","\t\tself.shared=True\n","\t\tself.rnn_hms=nn.ModuleList()\n","\t\tfor i in range(self.n_hms):\n","\t\t\tself.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.opsize = self.rnn_hms[0].outputlength()\n","\t\tself.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","\t\tself.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","\t\tself.opsize2=self.hm_level_rnn_1.outputlength()\n","\t\tself.f1_1=nn.Linear(self.opsize2,1)\n","\t\tself.f2_1=nn.Linear(self.opsize2,1)\n","\t\tself.diffopsize=2*(self.opsize2)\n","\t\tself.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n","\t\tself.predictor_=nn.Linear(self.diffopsize//2,1)\n","\t\tself.relu=nn.ReLU()\n","\n","\tdef forward_once(self,iput):\n","\t\tbin_a=None\n","\t\tlevel1_rep=None\n","\t\t[batch_size,_,_]=iput.size()\n","\t\tfor hm,hm_encdr in enumerate(self.rnn_hms):\n","\t\t\thmod=iput[:,:,hm].contiguous()\n","\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\t\t\top,a= hm_encdr(hmod)\n","\t\t\tif level1_rep is None:\n","\t\t\t\tlevel1_rep=op\n","\t\t\t\tbin_a=a\n","\t\t\telse:\n","\t\t\t\tlevel1_rep=torch.cat((level1_rep,op),1)\n","\t\t\t\tbin_a=torch.cat((bin_a,a),1)\n","\t\tlevel1_rep=level1_rep.permute(1,0,2)\n","\n","\t\treturn level1_rep,bin_a\n","\n","\n","\tdef forward(self,iput1,iput2):\n","\t\tlevel1_rep1,bin_a1=self.forward_once(iput1)\n","\n","\t\tlevel1_rep2,bin_a2=self.forward_once(iput2)\n","\t\tfinal_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","\t\tfinal_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","\t\tfinal_rep_1=final_rep_1.squeeze(1)\n","\t\tfinal_rep_2=final_rep_2.squeeze(1)\n","\t\t[a,b,c]=(level1_rep1.size())\n","\t\talpha_rep_1=level1_rep1.permute(1,0,2).view(b,a*c)\n","\t\talpha_rep_2=level1_rep2.permute(1,0,2).view(b,a*c)\n","\t\tprediction1=((self.f1_1(final_rep_1)))\n","\t\tprediction2=((self.f2_1(final_rep_2)))\n","\t\tmlp_input=torch.cat((final_rep_1,final_rep_2),1)\n","\t\tmlp_input=self.relu(self.predictor_1(mlp_input))\n","\t\tprediction=(self.predictor_(mlp_input))\n","\t\thm_level_attention=torch.cat((hm_level_attention_1,hm_level_attention_2),1)\n","\t\tbin_a=torch.cat((bin_a1,bin_a2),1)\n","\n","\t\treturn prediction,hm_level_attention,bin_a,alpha_rep_1,alpha_rep_2,prediction1,prediction2\n","\n","\n","\n","\n","\n","\n","class raw_aux_siamese(nn.Module):\n","\t# similar to raw_aux model with shared level 1 embedding\n","\t# returns only raw level attentions\n","\t# returns embeddings from shared level 1 for contrastive loss\n","\tdef __init__(self,args):\n","\t\tself.n_hms=args.n_hms\n","\t\tself.n_bins=args.n_bins\n","\t\tself.ip_bin_size=1\n","\t\tself.joint=False\n","\t\tself.shared=True\n","\t\tsuper(raw_aux_siamese,self).__init__()\n","\t\tself.rnn_hms=nn.ModuleList()\n","\t\tself.rnn_hmsx=nn.ModuleList()\n","\t\tfor i in range(self.n_hms):\n","\t\t\tself.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tfor i in range(3*self.n_hms):\n","\t\t\tself.rnn_hmsx.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","\t\tself.opsize = self.rnn_hms[0].outputlength()\n","\t\tself.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","\t\tself.hm_level_rnn_1x=recurrent_encoder(5*self.n_hms,self.opsize,True,args)\n","\t\tself.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","\t\tself.opsize2=self.hm_level_rnn_1.outputlength()\n","\t\tself.f1_1=nn.Linear(self.opsize2,1)\n","\t\tself.f2_1=nn.Linear(self.opsize2,1)\n","\t\tself.diffopsize=3*(self.opsize2)\n","\t\t#self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n","\t\tself.predictor_=nn.Linear(self.opsize2,1)\n","\t\tself.relu=nn.ReLU()\n","\t\tself.finalsoftmax=nn.LogSoftmax()\n","\tdef forward_once(self,iput):\n","\t\tbin_a=None\n","\t\tlevel1_rep=None\n","\t\t[batch_size,_,_]=iput.size()\n","\t\tfor hm,hm_encdr in enumerate(self.rnn_hms):\n","\t\t\thmod=iput[:,:,hm].contiguous()\n","\t\t\thmod=torch.t(hmod).unsqueeze(2)\n","\t\t\top,a= hm_encdr(hmod)\n","\t\t\tif level1_rep is None:\n","\t\t\t\tlevel1_rep=op\n","\t\t\t\tbin_a=a\n","\t\t\telse:\n","\t\t\t\tlevel1_rep=torch.cat((level1_rep,op),1)\n","\t\t\t\tbin_a=torch.cat((bin_a,a),1)\n","\t\tlevel1_rep=level1_rep.permute(1,0,2)\n","\t\treturn level1_rep,bin_a\n","\n","\n","\tdef forward(self,iput1,iput2):\n","\t\tiput3=iput1-iput2\n","\t\tiput4=torch.cat((iput1,iput2),2)\n","\t\tiput=(torch.cat((iput4,iput3),2))\n","\t\tbin_ax=None\n","\t\tlevel1_repx=None\n","\t\t[batch_size,_,_]=iput.size()\n","\t\tfor hm,hm_encdr in enumerate(self.rnn_hmsx):\n","\t\t\thmodx=iput[:,:,hm].contiguous()\n","\t\t\thmodx=torch.t(hmodx).unsqueeze(2)\n","\t\t\topx,ax= hm_encdr(hmodx)\n","\t\t\tif level1_repx is None:\n","\t\t\t\tlevel1_repx=opx\n","\t\t\t\tbin_ax=ax\n","\t\t\telse:\n","\t\t\t\tlevel1_repx=torch.cat((level1_repx,opx),1)\n","\t\t\t\tbin_ax=torch.cat((bin_ax,ax),1)\n","\t\tlevel1_repx=level1_repx.permute(1,0,2)\n","\n","\t\tlevel1_rep1,bin_a1=self.forward_once(iput1)\n","\n","\t\tlevel1_rep2,bin_a2=self.forward_once(iput2)\n","\t\tfinal_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","\t\tfinal_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","\t\tfinal_rep_1=final_rep_1.squeeze(1)\n","\t\tfinal_rep_2=final_rep_2.squeeze(1)\n","\t\t[a,b,c]=(level1_rep1.size())\n","\t\talpha_rep_1=level1_rep1.permute(1,0,2).view(b,a*c)\n","\t\talpha_rep_2=level1_rep2.permute(1,0,2).view(b,a*c)\n","\t\tlevel1_rep3=torch.cat((level1_repx,level1_rep1,level1_rep2),0)\n","\t\tfinal_rep_1x,hm_level_attention_1x=self.hm_level_rnn_1x(level1_rep3)\n","\t\tfinal_rep_1x=final_rep_1x.squeeze(1)\n","\t\tprediction1=((self.f1_1(final_rep_1)))\n","\t\tprediction2=((self.f2_1(final_rep_2)))\n","\t\tprediction=(self.predictor_(final_rep_1x))\n","\n","\t\treturn prediction,hm_level_attention_1x,bin_ax,alpha_rep_1,alpha_rep_2,prediction1,prediction2"],"execution_count":0,"outputs":[]}]}