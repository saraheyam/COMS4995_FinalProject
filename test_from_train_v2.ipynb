{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_from_train_v2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"LwU1-qE9n9xc","colab":{}},"source":["import os\n","from google.colab import drive\n","import sys"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpMaUjrf2tk2","colab_type":"code","outputId":"800b70eb-ac15-4f7e-8c2d-e34134bd1aee","executionInfo":{"status":"ok","timestamp":1575677351784,"user_tz":300,"elapsed":67215,"user":{"displayName":"Sarah Yam","photoUrl":"","userId":"16306504212637033761"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["### ONLY IF USING GOOGLE COLAB \n","drive.mount('/content/drive')\n","\n","print(os.getcwd())\n","os.chdir(\"/content/drive/My Drive/COMS4995_FinalProject\")\n","os.getcwd()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/COMS4995_FinalProject'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"huu_2SCG2hC1","colab_type":"code","colab":{}},"source":["sys.argv = [\"--cell_1=E116\", # placeholder i think because -h messes it up\n","            \"--cell_1=E116\", \n","            \"--cell_2=E123\", \n","            \"--model_name=aux_siamese\", \n","            \"--epochs=20\", \n","            \"--lr=0.001\", \n","            \"--data_root=data/\", \n","            \"--save_root=Results/\", \n","            \"--attentionfilename=E116_E123_compareSC.csv\", \n","            \"--n_bins=1000\", \n","            \"--n_hms=5\", \n","            \"--save_attention_maps\"]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bdse-plma8aC","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","import argparse\n","import json\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import cuda\n","import sys, os\n","import random\n","import numpy as np\n","from sklearn import metrics\n","import models as Model\n","from SiameseLoss import ContrastiveLoss\n","import evaluate\n","import data_v1\n","import gc\n","import csv\n","\n","\n","from __future__ import print_function\n","import argparse\n","import torch\n","import csv\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","import numpy as np\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7lBkptx1mZJ","colab_type":"code","colab":{}},"source":["parser = argparse.ArgumentParser(description='DeepDiff')\n","parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate')\n","parser.add_argument('--model_name', type=str, default='raw_d', help='DeepDiff variation')\n","parser.add_argument('--clip', type=float, default=1,help='gradient clipping')\n","parser.add_argument('--epochs', type=int, default=90, help='upper epoch limit')\n","parser.add_argument('--batch_size', type=int, default=10, help='')\n","parser.add_argument('--dropout', type=float, default=0.5, help='dropout applied to layers (0 = no dropout) if n_layers LSTM > 1')\n","parser.add_argument('--cell_1', type=str, default='Cell1', help='cell type 1')\n","parser.add_argument('--cell_2', type=str, default='Cell2', help='cell type 2')\n","parser.add_argument('--save_root', type=str, default='./Results/', help='where to save')\n","parser.add_argument('--data_root', type=str, default='./data/', help='data location')\n","parser.add_argument('--gpuid', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--gpu', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--n_hms', type=int, default=5, help='number of histone modifications')\n","parser.add_argument('--n_bins', type=int, default=200, help='number of bins')\n","parser.add_argument('--bin_rnn_size', type=int, default=32, help='bin rnn size')\n","parser.add_argument('--num_layers', type=int, default=1, help='number of layers')\n","parser.add_argument('--unidirectional', action='store_true', help='bidirectional/undirectional LSTM')\n","parser.add_argument('--save_attention_maps',action='store_true', help='set to save validation beta attention maps')\n","parser.add_argument('--attentionfilename', type=str, default='beta_attention.txt', help='where to save attnetion maps')\n","parser.add_argument('--test_on_saved_model',action='store_true', help='only test on saved model')\n","args = parser.parse_args()\n","\n","torch.manual_seed(1)\n","\n","model_name = ''\n","model_name += (args.cell_1)+('_')+(args.cell_2)+('_')\n","\n","model_name+=args.model_name\n","\n","\n","\n","\n","args.bidirectional=not args.unidirectional\n","\n","print('the model name: ',model_name)\n","args.data_root+=''\n","args.save_root+=''\n","args.dataset=args.cell_1+('_')+args.cell_2\n","args.data_root = os.path.join(args.data_root)\n","print('loading data from:  ',args.data_root)\n","args.save_root = os.path.join(args.save_root,args.dataset)\n","print('saving results in  from: ',args.save_root)\n","model_dir = os.path.join(args.save_root,model_name)\n","if not os.path.exists(model_dir):\n","    os.makedirs(model_dir)\n","attentionmapfile=model_dir+'/'+args.attentionfilename\n","print('==>processing data')\n","\n","\n","# return processed data before loading\n","# train_inpt, valid_inpt, test_inpt = data_v1.load_data(args)\n","train_inpt, valid_inpt, test_inpt = load_data(args)\n","\n","\n","\n","\n","\n","\n","\n","CON=False\n","AUX=False\n","print('==>building model')\n","if(args.model_name=='raw_d'):\n","    model = Model.raw_d(args)\n","    # model = raw_d(args)\n","\n","elif(args.model_name=='raw_c'):\n","    model = Model.raw_c(args)\n","    # model = raw_c(args)\n","\n","elif(args.model_name=='raw'):\n","    model = Model.raw(args)\n","    # model = raw(args)\n","\n","elif(args.model_name=='aux'):\n","    args.shared=False\n","    \n","    model = Model.aux(args)\n","    # model = aux(args)\n","\n","    AUX=True\n","    args.gamma=0.0\n","elif(args.model_name=='raw_aux'):\n","    args.shared=False\n","\n","    model = Model.raw_aux(args)\n","    # model = raw_aux(args)\n","    \n","    AUX=True\n","    args.gamma=0.0\n","elif(args.model_name=='aux_siamese'):\n","    CON=True\n","    args.shared=True\n","    \n","    model = Model.aux_siamese(args)\n","    # model = aux_siamese(args)\n","    \n","    AUX=True\n","    args.gamma=4.0\n","elif(args.model_name=='raw_aux_siamese'):\n","    CON=True\n","    args.shared=True\n","\n","    model = Model.raw_aux_siamese(args)\n","    # model = raw_aux_siamese(args)\n","\n","    AUX=True\n","    args.gamma=4.0\n","else:\n","    sys.exit(\"invalid model name\")\n","\n","\n","if torch.cuda.device_count() >= 1:\n","    torch.cuda.manual_seed_all(1)\n","    dtype = torch.cuda.FloatTensor\n","    cuda.set_device(args.gpuid)\n","    model.type(dtype)\n","    print('Using GPU '+str(args.gpuid))\n","else:\n","    print(\"No GPU Available\")\n","    dtype = torch.FloatTensor\n","\n","## PRINTING MODEL USES SO MUCH SPACE WHY\n","#print(model)\n","\n","\n","if(args.test_on_saved_model==False):\n","    print(\"==>initializing a new model\")\n","    for p in model.parameters():\n","        p.data.uniform_(-0.1,0.1)\n","\n","DiffLoss = nn.MSELoss(size_average=True).type(dtype)\n","AuxLoss = nn.MSELoss(size_average=True).type(dtype)\n","ConLoss = ContrastiveLoss().type(dtype)\n","\n","optimizer = optim.Adam(model.parameters(), lr = args.lr)\n","#optimizer = optim.SGD(model.parameters(), lr = args.lr, momentum=args.momentum)\n","def train(TrainData):\n","    model.train()\n","    # initialize attention\n","    diff_targets = torch.zeros(TrainData.dataset.__len__(),1)\n","    diff_predictions = torch.zeros(diff_targets.size(0),1)\n","    if(args.model_name=='raw_d'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","    elif(args.model_name=='raw_c'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),3*args.n_hms)\n","\n","    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","\n","    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),5*args.n_hms)\n","\n","    else:\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","\n","    num_batches = int(math.ceil(TrainData.dataset.__len__()/float(args.batch_size)))\n","    all_gene_ids=[None]*TrainData.dataset.__len__()\n","    per_epoch_loss = 0\n","    for idx, Sample in enumerate(TrainData):\n","        if(idx%100==0):\n","            print('TRAINING ON BATCH:',idx)\n","        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, TrainData.dataset.__len__())\n","        optimizer.zero_grad()\n","        # get HM profiles\n","        inputs_1 = Sample['X_A']\n","        inputs_2 = Sample['X_B']\n","\n","\n","        # get targets: both differential and cell specific expression\n","        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","        diff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","        if(CON==True):\n","            # get labels for contrastive loss\n","            batch_contrastive_targets =[]\n","            for label in batch_diff_targets:\n","                if(label<=-2.0):\n","                    batch_contrastive_targets.append(1)\n","                elif(label>=2.0):\n","                    batch_contrastive_targets.append(1)\n","                else:\n","                    batch_contrastive_targets.append(0)\n","            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","        all_gene_ids[start:end]=Sample['geneID']\n","        batch_size = inputs_1.size(0)\n","\n","        if(AUX==False):\n","            # for raw models: raw_d, raw_c, raw\n","            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","        elif(CON==False):\n","            # for aux models\n","            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","        else:\n","            # for aux and siamese models\n","            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","        per_epoch_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n","        optimizer.step()\n","    per_epoch_loss=per_epoch_loss/num_batches\n","    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","def test(ValidData):\n","    model.eval()\n","\n","    diff_targets = torch.zeros(ValidData.dataset.__len__(),1)\n","    diff_predictions = torch.zeros(diff_targets.size(0),1)\n","    if(args.model_name=='raw_d'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","    elif(args.model_name=='raw_c'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),3*args.n_hms)\n","    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),5*args.n_hms)\n","    else:\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","\n","    num_batches = int(math.ceil(ValidData.dataset.__len__()/float(args.batch_size)))\n","    all_gene_ids=[None]*ValidData.dataset.__len__()\n","    per_epoch_loss = 0\n","    for idx, Sample in enumerate(ValidData):\n","        if(idx%100==0):\n","            print('TESTING ON BATCH:',idx)\n","        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, ValidData.dataset.__len__())\n","        optimizer.zero_grad()\n","        # get HM profiles\n","        inputs_1 = Sample['X_A']\n","        inputs_2 = Sample['X_B']\n","\n","        \n","        # get targets: both differential and cell specific expression\n","        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","        diff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","        if(CON==True):\n","            # get labels for contrastive loss\n","            batch_contrastive_targets =[]\n","            for label in batch_diff_targets:\n","                if(label<=-2.0):\n","                    batch_contrastive_targets.append(1)\n","                elif(label>=2.0):\n","                    batch_contrastive_targets.append(1)\n","                else:\n","                    batch_contrastive_targets.append(0)\n","            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","        all_gene_ids[start:end]=Sample['geneID']\n","        batch_size = inputs_1.size(0)\n","\n","        if(AUX==False):\n","            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","        elif(CON==False):\n","            # for aux models\n","            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","        else:\n","            # for aux and siamese models\n","            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","        per_epoch_loss += loss.item()\n","    per_epoch_loss=per_epoch_loss/num_batches\n","    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","    # Train = torch.utils.data.DataLoader(train_inputs, batch_size=args.batch_size, shuffle=True)\n","    # Valid = torch.utils.data.DataLoader(valid_inputs, batch_size=args.batch_size, shuffle=False)\n","    # Test = torch.utils.data.DataLoader(test_inputs, batch_size=args.batch_size, shuffle=False)\n","\n","\n","\n","best_valid_loss = 10000000000\n","best_valid_MSE=100000\n","best_valid_R2=-1\n","if(args.test_on_saved_model==False):\n","    for epoch in range(0, args.epochs):\n","        print('=---------------------------------------- Training '+str(epoch+1)+' -----------------------------------=')\n","        diff_predictions,diff_targets,alpha_train,beta_train,train_loss,_ = train(Train)\n","        train_MSE, train_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","        diff_predictions,diff_targets,alpha_valid,beta_valid,valid_loss,gene_ids_valid = test(Valid)\n","        valid_MSE, valid_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","\n","        if(valid_R2 >= best_valid_R2):\n","                # save best epoch -- models converge early\n","            best_valid_R2=valid_R2\n","            torch.save(model,model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","        print(\"Epoch:\",epoch)\n","        print(\"train R2:\",train_R2)\n","        print(\"valid R2:\",valid_R2)\n","        print(\"best valid R2:\", best_valid_R2)\n","\n"," \n","    print(\"finished training!!\")\n","    print(\"best validation R2:\",best_valid_R2)\n","    print(\"testing\")\n","    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","    print(\"test R2:\",test_R2)\n","\n","    if(args.save_attention_maps):\n","        attentionfile=open(attentionmapfile,'w')\n","        attentionfilewriter=csv.writer(attentionfile)\n","        beta_test=beta_test.numpy()\n","        for i in range(len(gene_ids_test)):\n","            gene_attention=[]\n","            gene_attention.append(gene_ids_test[i])\n","            for e in beta_test[i,:]:\n","                gene_attention.append(str(e))\n","            attentionfilewriter.writerow(gene_attention)\n","        attentionfile.close()\n","\n","\n","else:\n","    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","    print(\"test R2:\",test_R2)\n","\n","    if(args.save_attention_maps):\n","        attentionfile=open(attentionmapfile,'w')\n","        attentionfilewriter=csv.writer(attentionfile)\n","        beta_test=beta_test.numpy()\n","        for i in range(len(gene_ids_test)):\n","            gene_attention=[]\n","            gene_attention.append(gene_ids_test[i])\n","            for e in beta_test[i,:]:\n","                gene_attention.append(str(e))\n","            attentionfilewriter.writerow(gene_attention)\n","        attentionfile.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGN8vZXBsS_v","colab_type":"code","colab":{}},"source":["import torch\n","import collections\n","import pdb\n","import torch.utils.data\n","import csv\n","import json\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import math\n","import numpy as np\n","\n","def getlabel(c1,c2):\n","    # get log fold change of expression\n","\n","    label1=math.log((float(c1)+1.0),2)\n","    label2=math.log((float(c2)+1.0),2)\n","    label=[]\n","    label.append(label1)\n","    label.append(label2)\n","\n","    fold_change=(float(c2)+1.0)/(float(c1)+1.0)\n","    log_fold_change=math.log((fold_change),2)\n","    return (log_fold_change, label)\n","\n","def loadDict(filename):\n","    # get expression value of each gene from cell*.expr.csv\n","    gene_dict={}\n","    with open(filename) as fi:\n","        for line in fi:\n","            geneID,geneExpr=line.split(',')\n","            gene_dict[str(geneID)]=float(geneExpr)\n","    fi.close()\n","    return(gene_dict)\n","\n","\n","def loadData(filename,windows,gene_dict, num_hms):\n","    \n","    with open(filename) as fi:\n","        csv_reader=csv.reader(fi)\n","        data = list(csv_reader)\n","    \n","        ncols=(len(data[0]))\n","    fi.close()\n","\n","    nrows=len(data)\n","    ngenes=nrows/windows\n","    nfeatures=ncols-1\n","    \n","    # testing args.n_hms vs nfeatures\n","    if nfeatures != num_hms:\n","        print(\"nfeatures != num_hms\")\n","    assert(int(nfeatures) == int(num_hms))\n","\n","    \n","    print(\"Number of genes: %d\" % ngenes)\n","    print(\"Number of entries: %d\" % nrows)\n","    print(\"Number of HMs: %d\" % nfeatures)\n","\n","    count = 0\n","    attr = collections.OrderedDict()\n","    alph = [chr(x) for x in range(ord('a'), ord('z') + 1)] # alphabetical key order more stable\n","\n","    gene_keys = [\"geneID\", \"expr\"]\n","    hm_id = [\"hm_\" + alph[i] for i in list(range(nfeatures))]\n","    data_mat = np.array(data)\n","\n","    for i in range(0, nrows, windows):\n","        geneID=str(data[i][0].split(\"_\")[0])\n","    \n","        meta = {}\n","        meta[\"geneID\"] = geneID\n","        meta[\"expr\"] = gene_dict[geneID]\n","    \n","        for j in range(nfeatures):\n","            meta[hm_id[j]] = torch.tensor(np.array([float(z) for z in data_mat[i:(i+windows), j+1]]).reshape(windows, 1))\n","    \n","        attr[count] = meta\n","        count+=1    \n","        \n","    return attr\n","\n","#### NEED TO EDIT THIS TO TAKE N-FEATURES\n","\n","class HMData(Dataset):\n","    # Dataset class for loading data\n","    def __init__(self,data_cell1,data_cell2, n_feat, transform = None): \n","        self.c1=data_cell1\n","        self.c2=data_cell2\n","        self.nfeat = n_feat\n","        assert (len(self.c1)==len(self.c2))\n","    def __len__(self):\n","        return len(self.c1)\n","    def __getitem__(self,i):\n","                 \n","        alph = [chr(x) for x in range(ord('a'), ord('z') + 1)]\n","        hm_id = [\"hm_\" + alph[y] for y in list(range(self.nfeat))]\n","                 \n","#         final_data_c1=torch.cat((self.c1[i]['hm1'],self.c1[i]['hm2'],self.c1[i]['hm3'],self.c1[i]['hm4'],self.c1[i]['hm5']),1)\n","#         final_data_c2=torch.cat((self.c2[i]['hm1'],self.c2[i]['hm2'],self.c2[i]['hm3'],self.c2[i]['hm4'],self.c2[i]['hm5']),1)\n","        \n","        final_data_c1 = torch.cat([self.c1[i][z] for z in hm_id], 1) \n","        final_data_c2 = torch.cat([self.c2[i][w] for w in hm_id], 1)         \n","                \n","        label,orig_label=getlabel(self.c1[i]['expr'],self.c2[i]['expr'])\n","        b_label_c1=orig_label[0]\n","        b_label_c2=orig_label[1]\n","        assert self.c1[i]['geneID']==self.c2[i]['geneID']\n","        geneID=self.c1[i]['geneID']\n","        sample={'geneID':geneID,\n","               'X_A':final_data_c1,\n","               'X_B':final_data_c2,\n","               'diff':label,\n","               'abs_A':b_label_c1,'abs_B':b_label_c2}\n","        return sample\n","    \n","### NEED TO PASS args.n_hms TO HMDATA FOR nfeat ARGUMENT    \n","#  assert that nfeatures == args.n_hms\n","#  could also just pass nfeatures from loadData (still needs assert)\n","\n","def load_data(args):\n","    '''\n","    Loads data into a 3D tensor for each of the 3 splits.\n","\n","    '''\n","    print(\"==>loading train data\")\n","    gene_dict1=loadDict(args.data_root+args.cell_1+\".expr.csv\")\n","    gene_dict2=loadDict(args.data_root+args.cell_2+\".expr.csv\")\n","    \n","    cell_train_dict1=loadData(args.data_root+\"/\"+args.cell_1+\".train.csv\",\n","                              args.n_bins,gene_dict1, args.n_hms) # n_hms assert    \n","    cell_train_dict2=loadData(args.data_root+\"/\"+args.cell_2+\".train.csv\",\n","                              args.n_bins,gene_dict2, args.n_hms) # n_hms assert\n","    train_inputs = HMData(cell_train_dict1,cell_train_dict2, args.n_hms) # added dynamic args.n_hms \n","    \n","    print(\"==>loading valid data\")\n","    cell_valid_dict1=loadData(args.data_root+\"/\"+args.cell_1+\".valid.csv\", \n","                              args.n_bins,gene_dict1, args.n_hms) \n","    cell_valid_dict2=loadData(args.data_root+\"/\"+args.cell_2+\".valid.csv\", \n","                              args.n_bins,gene_dict2, args.n_hms)   \n","    valid_inputs = HMData(cell_valid_dict1,cell_valid_dict2, args.n_hms) \n","    \n","    print(\"==>loading test data\")\n","    cell_test_dict1=loadData(args.data_root+\"/\"+args.cell_1+\".test.csv\", \n","                             args.n_bins,gene_dict1, args.n_hms) \n","    cell_test_dict2=loadData(args.data_root+\"/\"+args.cell_2+\".test.csv\", \n","                             args.n_bins,gene_dict2, args.n_hms)   \n","    test_inputs = HMData(cell_test_dict1,cell_test_dict2, args.n_hms)\n","\n","      \n","    return train_inputs, valid_inputs, test_inputs # return inputs to trainloading/cross validation"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2M2wstAtOWBY","colab_type":"code","colab":{}},"source":["\n","def batch_product(iput, mat2):\n","        result = None\n","        for i in range(iput.size()[0]):\n","            op = torch.mm(iput[i], mat2)\n","            op = op.unsqueeze(0)\n","            if(result is None):\n","                result = op\n","            else:\n","                result = torch.cat((result,op),0)\n","        return result.squeeze(2)\n","\n","\n","class rec_attention(nn.Module):\n","    # attention with bin context vector per HM and HM context vector\n","    def __init__(self,hm,args):\n","        super(rec_attention,self).__init__()\n","        self.num_directions=2 if args.bidirectional else 1\n","        if (hm==False):\n","            self.bin_rep_size=args.bin_rnn_size*self.num_directions\n","        else:\n","            self.bin_rep_size=args.bin_rnn_size\n","    \n","        self.bin_context_vector=nn.Parameter(torch.Tensor(self.bin_rep_size,1),requires_grad=True)\n","    \n","\n","        self.softmax=nn.Softmax()\n","\n","        self.bin_context_vector.data.uniform_(-0.1, 0.1)\n","\n","    def forward(self,iput):\n","        alpha=self.softmax(batch_product(iput,self.bin_context_vector))\n","        [batch_size,source_length,bin_rep_size2]=iput.size()\n","        repres=torch.bmm(alpha.unsqueeze(2).view(batch_size,-1,source_length),iput)\n","        return repres,alpha\n","\n","\n","\n","class recurrent_encoder(nn.Module):\n","    # modular LSTM encoder\n","    def __init__(self,n_bins,ip_bin_size,hm,args):\n","        super(recurrent_encoder,self).__init__()\n","        self.bin_rnn_size=args.bin_rnn_size\n","        self.ipsize=ip_bin_size\n","        self.seq_length=n_bins\n","\n","        self.num_directions=2 if args.bidirectional else 1\n","        if (hm==False):\n","            self.bin_rnn_size=args.bin_rnn_size\n","        else:\n","            self.bin_rnn_size=args.bin_rnn_size // 2\n","        self.bin_rep_size=self.bin_rnn_size*self.num_directions\n","\n","\n","        self.rnn=nn.LSTM(self.ipsize,self.bin_rnn_size,num_layers=args.num_layers,dropout=args.dropout,bidirectional=args.bidirectional)\n","\n","        self.bin_attention=rec_attention(hm,args)\n","    def outputlength(self):\n","        return self.bin_rep_size\n","    def forward(self,single_hm,hidden=None):\n","\n","        bin_output, hidden = self.rnn(single_hm,hidden)\n","        bin_output = bin_output.permute(1,0,2)\n","        hm_rep,bin_alpha = self.bin_attention(bin_output)\n","        return hm_rep,bin_alpha\n","\n","class raw_d(nn.Module):\n","    def __init__(self,args):\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        super(raw_d,self).__init__()\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.diffopsize=2*(self.opsize2)\n","        self.fdiff1_1=nn.Linear(self.opsize2,1)\n","\n","    def forward(self,iput1,iput2):\n","\n","        iput=iput1-iput2\n","        bin_a=None\n","        level1_rep=None\n","        [batch_size,_,_]=iput.size()\n","\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # print(\"Orig: \", bin_a.shape)\n","        # bin_a = bin_a.reshape(-1)\n","        # print(\"Reshape: \", bin_a.shape)\n","\n","        \n","        level1_rep=level1_rep.permute(1,0,2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        prediction_m=((self.fdiff1_1(final_rep_1)))\n","        return prediction_m,hm_level_attention_1, bin_a\n","\n","\n","class raw_c(nn.Module):\n","    def __init__(self,args):\n","        super(raw_c,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(2*self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(2*self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.diffopsize=2*(self.opsize2)\n","        self.fdiff1_1=nn.Linear(self.opsize2,1)\n","\n","    def forward(self,iput1,iput2):\n","        iput=torch.cat((iput1,iput2),2)\n","        bin_a=None\n","        level1_rep=None\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a = bin_a.reshape(-1)\n","\n","        level1_rep=level1_rep.permute(1,0,2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        prediction_m=((self.fdiff1_1(final_rep_1)))\n","\n","        return prediction_m,hm_level_attention_1, bin_a\n","\n","\n","class raw(nn.Module):\n","    # Model with all raw features: difference and absolute features\n","    def __init__(self,args):\n","        super(raw,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(3*self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(3*self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.diffopsize=2*(self.opsize2)\n","        self.fdiff1_1=nn.Linear(self.opsize2,1)\n","\n","    def forward(self,iput1,iput2):\n","        iput3=iput1-iput2\n","        iput4=torch.cat((iput1,iput2),2)\n","        iput=(torch.cat((iput4,iput3),2))\n","\n","        bin_a=None\n","        level1_rep=None\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a = bin_a.reshape(-1)\n","\n","        level1_rep=level1_rep.permute(1,0,2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n","        final_rep_1=final_rep_1.squeeze(1)\n","\n","        prediction_m=((self.fdiff1_1(final_rep_1)))\n","        return prediction_m,hm_level_attention_1, bin_a\n","\n","\n","class aux(nn.Module):\n","    def __init__(self,args):\n","        super(aux,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.shared=False\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.rnn_hms2=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms2.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.f1_1=nn.Linear(self.opsize2,1)\n","        self.f2_1=nn.Linear(self.opsize2,1)\n","        self.diffopsize=2*(self.opsize2)\n","        self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n","        self.predictor_=nn.Linear(self.diffopsize//2,1)\n","        self.relu=nn.ReLU()\n","\n","    def forward_once(self,iput,shared,cellid):\n","        bin_a=None\n","        level1_rep=None\n","        if(shared or cellid==1):\n","            for hm,hm_encdr in enumerate(self.rnn_hms):\n","                hmod=iput[:,:,hm].contiguous()\n","                hmod=torch.t(hmod).unsqueeze(2)\n","                op,a= hm_encdr(hmod)\n","                if level1_rep is None:\n","                    level1_rep=op\n","                    bin_a=a\n","                else:\n","                    level1_rep=torch.cat((level1_rep,op),1)\n","                    bin_a=torch.cat((bin_a,a),1)\n","\n","            #--------TARGET DIMENSION FIX--------No idea if this is right\n","            # flatten bin_a to match all_attention dimensions\n","            # bin_a = bin_a.reshape(-1)\n","\n","            level1_rep=level1_rep.permute(1,0,2)\n","        else:\n","            for hm,hm_encdr in enumerate(self.rnn_hms2):\n","\n","                hmod=iput[:,:,hm].contiguous()\n","                hmod=torch.t(hmod).unsqueeze(2)\n","                op,a= hm_encdr(hmod)\n","                if level1_rep is None:\n","                    level1_rep=op\n","                    bin_a=a\n","                else:\n","                    level1_rep=torch.cat((level1_rep,op),1)\n","                    bin_a=torch.cat((bin_a,a),1)\n"," \n","            #--------TARGET DIMENSION FIX--------No idea if this is right\n","            # flatten bin_a to match all_attention dimensions\n","            # bin_a = bin_a.reshape(-1)\n","\n","            level1_rep=level1_rep.permute(1,0,2)\n","        return level1_rep,bin_a\n","\n","    def forward(self,iput1,iput2):\n"," \n","        level1_rep1,bin_a1=self.forward_once(iput1,self.shared,1)\n","        level1_rep2,bin_a2=self.forward_once(iput2,self.shared,2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","        if(self.joint):\n","            final_rep_2,hm_level_attention_2=self.hm_level_rnn_1(level1_rep2)\n","        else:\n","            final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        final_rep_2=final_rep_2.squeeze(1)\n","        prediction1=((self.f1_1(final_rep_1)))\n","        prediction2=((self.f2_1(final_rep_2)))\n","\n","        mlp_input=torch.cat((final_rep_1,final_rep_2),1)\n","        mlp_input=self.relu(self.predictor_1(mlp_input))\n","        prediction=(self.predictor_(mlp_input))\n","        hm_level_attention=torch.cat((hm_level_attention_1,hm_level_attention_2),1)\n","        \n","        bin_a=torch.cat((bin_a1,bin_a2),1)\n","        # already flattened bin_a's\n","        # bin_a = torch.cat((bin_a1, bin_a2))\n","\n","\n","        return prediction,hm_level_attention,bin_a,prediction1,prediction2\n","\n","\n","class raw_aux(nn.Module):\n","    # level 1 takes raw feaures, level 2 takes aux features as well as raw feature embeddings from level 1\n","    # returns attention scores from raw part only\n","    def __init__(self,args):\n","        super(raw_aux,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.shared=False\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.rnn_hms3=nn.ModuleList()\n","        for i in range(3*self.n_hms):\n","            self.rnn_hms3.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.rnn_hms2=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms2.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_3=recurrent_encoder(5*self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.f1_1=nn.Linear(self.opsize2,1)\n","        self.f2_1=nn.Linear(self.opsize2,1)\n","        self.diffopsize=3*(self.opsize2)\n","        self.predictor_=nn.Linear(self.opsize2,1)\n","        self.relu=nn.ReLU()\n","\n","\n","    def forward(self,iput1,iput2):\n","        iput3=iput1-iput2\n","        iput4=torch.cat((iput1,iput2),2)\n","        iput=(torch.cat((iput4,iput3),2))\n","        bin_a=None\n","        level1_rep=None\n","        [batch_size,_,_]=iput.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hms3):\n","\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a = bin_a.reshape(-1)\n","\n","        level1_rep=level1_rep.permute(1,0,2)\n","\n","        bin_a1=None\n","        level1_rep1=None\n","        [batch_size,_,_]=iput1.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput1[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep1 is None:\n","                level1_rep1=op\n","                bin_a1=a\n","            else:\n","                level1_rep1=torch.cat((level1_rep1,op),1)\n","                bin_a1=torch.cat((bin_a1,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a1 = bin_a1.reshape(-1)\n","\n","        level1_rep1=level1_rep1.permute(1,0,2)\n","        bin_a2=None\n","        level1_rep2=None\n","\n","        for hm,hm_encdr in enumerate(self.rnn_hms2):\n","            hmod=iput2[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep2 is None:\n","                level1_rep2=op\n","                bin_a2=a\n","            else:\n","                level1_rep2=torch.cat((level1_rep2,op),1)\n","                bin_a2=torch.cat((bin_a2,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a2 = bin_a2.reshape(-1)\n","\n","        level1_rep2=level1_rep2.permute(1,0,2)\n","        level1_rep3=torch.cat((level1_rep,level1_rep1,level1_rep2),0)\n","        final_rep_3,hm_level_attention_3=self.hm_level_rnn_3(level1_rep3)\n","        final_rep_3=final_rep_3.squeeze(1)\n","\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","        final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        final_rep_2=final_rep_2.squeeze(1)\n","        prediction1=((self.f1_1(final_rep_1)))\n","        prediction2=((self.f2_1(final_rep_2)))\n","        prediction=(self.predictor_(final_rep_3))\n","\n","        # bin_a1 and bin_a2 supposed to be concatenated like aux and aux_siamese????\n","\n","        return prediction,hm_level_attention_3,bin_a,prediction1,prediction2\n","\n","\n","class aux_siamese(nn.Module):\n","    # aux with siamese  same as aux model mostly, but with shared level 1 embedding\n","    def __init__(self,args):\n","        super(aux_siamese,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.shared=True\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.f1_1=nn.Linear(self.opsize2,1)\n","        self.f2_1=nn.Linear(self.opsize2,1)\n","        self.diffopsize=2*(self.opsize2)\n","        self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n","        self.predictor_=nn.Linear(self.diffopsize//2,1)\n","        self.relu=nn.ReLU()\n","\n","    def forward_once(self,iput):\n","        bin_a=None\n","        level1_rep=None\n","        [batch_size,_,_]=iput.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","        \n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a = bin_a.reshape(-1)\n","\n","        level1_rep=level1_rep.permute(1,0,2)\n","\n","        return level1_rep,bin_a\n","\n","\n","    def forward(self,iput1,iput2):\n","        level1_rep1,bin_a1=self.forward_once(iput1)\n","\n","        level1_rep2,bin_a2=self.forward_once(iput2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","        final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        final_rep_2=final_rep_2.squeeze(1)\n","        [a,b,c]=(level1_rep1.size())\n","        alpha_rep_1=level1_rep1.permute(1,0,2).view(b,a*c)\n","        alpha_rep_2=level1_rep2.permute(1,0,2).view(b,a*c)\n","        prediction1=((self.f1_1(final_rep_1)))\n","        prediction2=((self.f2_1(final_rep_2)))\n","        mlp_input=torch.cat((final_rep_1,final_rep_2),1)\n","        mlp_input=self.relu(self.predictor_1(mlp_input))\n","        prediction=(self.predictor_(mlp_input))\n","        hm_level_attention=torch.cat((hm_level_attention_1,hm_level_attention_2),1)\n","        \n","        bin_a=torch.cat((bin_a1,bin_a2),1)\n","        # already flattened bin_a's\n","        # bin_a = torch.cat((bin_a1, bin_a2))\n","\n","        return prediction,hm_level_attention,bin_a,alpha_rep_1,alpha_rep_2,prediction1,prediction2\n","\n","\n","class raw_aux_siamese(nn.Module):\n","    # similar to raw_aux model with shared level 1 embedding\n","    # returns only raw level attentions\n","    # returns embeddings from shared level 1 for contrastive loss\n","    def __init__(self,args):\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.shared=True\n","        super(raw_aux_siamese,self).__init__()\n","        self.rnn_hms=nn.ModuleList()\n","        self.rnn_hmsx=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        for i in range(3*self.n_hms):\n","            self.rnn_hmsx.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_1x=recurrent_encoder(5*self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.f1_1=nn.Linear(self.opsize2,1)\n","        self.f2_1=nn.Linear(self.opsize2,1)\n","        self.diffopsize=3*(self.opsize2)\n","        #self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n","        self.predictor_=nn.Linear(self.opsize2,1)\n","        self.relu=nn.ReLU()\n","        self.finalsoftmax=nn.LogSoftmax()\n","    def forward_once(self,iput):\n","        bin_a=None\n","        level1_rep=None\n","        [batch_size,_,_]=iput.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","        level1_rep=level1_rep.permute(1,0,2)\n","        return level1_rep,bin_a\n","\n","\n","    def forward(self,iput1,iput2):\n","        iput3=iput1-iput2\n","        iput4=torch.cat((iput1,iput2),2)\n","        iput=(torch.cat((iput4,iput3),2))\n","        bin_ax=None\n","        level1_repx=None\n","        [batch_size,_,_]=iput.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hmsx):\n","            hmodx=iput[:,:,hm].contiguous()\n","            hmodx=torch.t(hmodx).unsqueeze(2)\n","            opx,ax= hm_encdr(hmodx)\n","            if level1_repx is None:\n","                level1_repx=opx\n","                bin_ax=ax\n","            else:\n","                level1_repx=torch.cat((level1_repx,opx),1)\n","                bin_ax=torch.cat((bin_ax,ax),1)\n","        level1_repx=level1_repx.permute(1,0,2)\n","\n","        level1_rep1,bin_a1=self.forward_once(iput1)\n","\n","        level1_rep2,bin_a2=self.forward_once(iput2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","        final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        final_rep_2=final_rep_2.squeeze(1)\n","        [a,b,c]=(level1_rep1.size())\n","        alpha_rep_1=level1_rep1.permute(1,0,2).view(b,a*c)\n","        alpha_rep_2=level1_rep2.permute(1,0,2).view(b,a*c)\n","        level1_rep3=torch.cat((level1_repx,level1_rep1,level1_rep2),0)\n","        final_rep_1x,hm_level_attention_1x=self.hm_level_rnn_1x(level1_rep3)\n","        final_rep_1x=final_rep_1x.squeeze(1)\n","        prediction1=((self.f1_1(final_rep_1)))\n","        prediction2=((self.f2_1(final_rep_2)))\n","        prediction=(self.predictor_(final_rep_1x))\n","\n","        return prediction,hm_level_attention_1x,bin_ax,alpha_rep_1,alpha_rep_2,prediction1,prediction2"],"execution_count":0,"outputs":[]}]}