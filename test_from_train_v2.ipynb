{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test_from_train_v2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"LwU1-qE9n9xc","colab":{}},"source":["import os\n","from google.colab import drive\n","import sys"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpMaUjrf2tk2","colab_type":"code","outputId":"881ca9d8-2617-46df-a4c6-70d1a5fa1a44","executionInfo":{"status":"ok","timestamp":1575491061740,"user_tz":300,"elapsed":19798,"user":{"displayName":"Sarah Yam","photoUrl":"","userId":"16306504212637033761"}},"colab":{"base_uri":"https://localhost:8080/","height":159}},"source":["### ONLY IF USING GOOGLE COLAB \n","drive.mount('/content/drive')\n","\n","print(os.getcwd())\n","os.chdir(\"/content/drive/My Drive/COMS4995_FinalProject\")\n","os.getcwd()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/COMS4995_FinalProject'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"huu_2SCG2hC1","colab_type":"code","colab":{}},"source":["sys.argv = [\"--cell_1=gm\", # placeholder i think because -h messes it up\n","            \"--cell_1=gm\", \n","            \"--cell_2=k562\", \n","            \"--model_name=raw_aux_siamese\", \n","            \"--epochs=10\", \n","            \"--lr=0.001\", \n","            \"--data_root=data/\", \n","            \"--save_root=Results/\", \n","            \"--attentionfilename=gm_k562_test2_11.csv\", \n","            \"--n_bins=100\", \n","            \"--n_hms=11\", \n","            \"--save_attention_maps\"]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bdse-plma8aC","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","import argparse\n","import json\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch import cuda\n","import sys, os\n","import random\n","import numpy as np\n","from sklearn import metrics\n","import models as Model\n","from SiameseLoss import ContrastiveLoss\n","import evaluate\n","import data_v1\n","import gc\n","import csv\n","\n","\n","from __future__ import print_function\n","import argparse\n","import torch\n","import csv\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","import numpy as np\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7lBkptx1mZJ","colab_type":"code","outputId":"76aa2b01-2c35-4d06-a2f7-2ee58a0ed729","executionInfo":{"status":"error","timestamp":1575491250519,"user_tz":300,"elapsed":8446,"user":{"displayName":"Sarah Yam","photoUrl":"","userId":"16306504212637033761"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["parser = argparse.ArgumentParser(description='DeepDiff')\n","parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate')\n","parser.add_argument('--model_name', type=str, default='raw_d', help='DeepDiff variation')\n","parser.add_argument('--clip', type=float, default=1,help='gradient clipping')\n","parser.add_argument('--epochs', type=int, default=90, help='upper epoch limit')\n","parser.add_argument('--batch_size', type=int, default=10, help='')\n","parser.add_argument('--dropout', type=float, default=0.5, help='dropout applied to layers (0 = no dropout) if n_layers LSTM > 1')\n","parser.add_argument('--cell_1', type=str, default='Cell1', help='cell type 1')\n","parser.add_argument('--cell_2', type=str, default='Cell2', help='cell type 2')\n","parser.add_argument('--save_root', type=str, default='./Results/', help='where to save')\n","parser.add_argument('--data_root', type=str, default='./data/', help='data location')\n","parser.add_argument('--gpuid', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--gpu', type=int, default=0, help='CUDA gpu')\n","parser.add_argument('--n_hms', type=int, default=5, help='number of histone modifications')\n","parser.add_argument('--n_bins', type=int, default=200, help='number of bins')\n","parser.add_argument('--bin_rnn_size', type=int, default=32, help='bin rnn size')\n","parser.add_argument('--num_layers', type=int, default=1, help='number of layers')\n","parser.add_argument('--unidirectional', action='store_true', help='bidirectional/undirectional LSTM')\n","parser.add_argument('--save_attention_maps',action='store_true', help='set to save validation beta attention maps')\n","parser.add_argument('--attentionfilename', type=str, default='beta_attention.txt', help='where to save attnetion maps')\n","parser.add_argument('--test_on_saved_model',action='store_true', help='only test on saved model')\n","args = parser.parse_args()\n","\n","torch.manual_seed(1)\n","\n","model_name = ''\n","model_name += (args.cell_1)+('_')+(args.cell_2)+('_')\n","\n","model_name+=args.model_name\n","\n","\n","\n","\n","args.bidirectional=not args.unidirectional\n","\n","print('the model name: ',model_name)\n","args.data_root+=''\n","args.save_root+=''\n","args.dataset=args.cell_1+('_')+args.cell_2\n","args.data_root = os.path.join(args.data_root)\n","print('loading data from:  ',args.data_root)\n","args.save_root = os.path.join(args.save_root,args.dataset)\n","print('saving results in  from: ',args.save_root)\n","model_dir = os.path.join(args.save_root,model_name)\n","if not os.path.exists(model_dir):\n","    os.makedirs(model_dir)\n","attentionmapfile=model_dir+'/'+args.attentionfilename\n","print('==>processing data')\n","Train,Valid,Test = data_v1.load_data(args)\n","\n","\n","\n","\n","\n","\n","\n","CON=False\n","AUX=False\n","print('==>building model')\n","if(args.model_name=='raw_d'):\n","    model = Model.raw_d(args)\n","    # model = raw_d(args)\n","\n","elif(args.model_name=='raw_c'):\n","    model = Model.raw_c(args)\n","    # model = raw_c(args)\n","\n","elif(args.model_name=='raw'):\n","    model = Model.raw(args)\n","    # model = raw(args)\n","\n","elif(args.model_name=='aux'):\n","    args.shared=False\n","    \n","    model = Model.aux(args)\n","    # model = aux(args)\n","\n","    AUX=True\n","    args.gamma=0.0\n","elif(args.model_name=='raw_aux'):\n","    args.shared=False\n","\n","    model = Model.raw_aux(args)\n","    # model = raw_aux(args)\n","    \n","    AUX=True\n","    args.gamma=0.0\n","elif(args.model_name=='aux_siamese'):\n","    CON=True\n","    args.shared=True\n","    \n","    model = Model.aux_siamese(args)\n","    # model = aux_siamese(args)\n","    \n","    AUX=True\n","    args.gamma=4.0\n","elif(args.model_name=='raw_aux_siamese'):\n","    CON=True\n","    args.shared=True\n","\n","    model = Model.raw_aux_siamese(args)\n","    # model = raw_aux_siamese(args)\n","\n","    AUX=True\n","    args.gamma=4.0\n","else:\n","    sys.exit(\"invalid model name\")\n","\n","\n","if torch.cuda.device_count() >= 1:\n","    torch.cuda.manual_seed_all(1)\n","    dtype = torch.cuda.FloatTensor\n","    cuda.set_device(args.gpuid)\n","    model.type(dtype)\n","    print('Using GPU '+str(args.gpuid))\n","else:\n","    print(\"No GPU Available\")\n","    dtype = torch.FloatTensor\n","\n","## PRINTING MODEL USES SO MUCH SPACE WHY\n","#print(model)\n","\n","\n","if(args.test_on_saved_model==False):\n","    print(\"==>initializing a new model\")\n","    for p in model.parameters():\n","        p.data.uniform_(-0.1,0.1)\n","\n","DiffLoss = nn.MSELoss(size_average=True).type(dtype)\n","AuxLoss = nn.MSELoss(size_average=True).type(dtype)\n","ConLoss = ContrastiveLoss().type(dtype)\n","\n","optimizer = optim.Adam(model.parameters(), lr = args.lr)\n","#optimizer = optim.SGD(model.parameters(), lr = args.lr, momentum=args.momentum)\n","def train(TrainData):\n","    model.train()\n","    # initialize attention\n","    diff_targets = torch.zeros(TrainData.dataset.__len__(),1)\n","    diff_predictions = torch.zeros(diff_targets.size(0),1)\n","    if(args.model_name=='raw_d'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","    elif(args.model_name=='raw_c'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),3*args.n_hms)\n","\n","    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),2*args.n_hms)\n","\n","    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),5*args.n_hms)\n","\n","    else:\n","        all_attention_bin=torch.zeros(TrainData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(TrainData.dataset.__len__(),args.n_hms)\n","\n","    num_batches = int(math.ceil(TrainData.dataset.__len__()/float(args.batch_size)))\n","    all_gene_ids=[None]*TrainData.dataset.__len__()\n","    per_epoch_loss = 0\n","    for idx, Sample in enumerate(TrainData):\n","        if(idx%100==0):\n","            print('TRAINING ON BATCH:',idx)\n","        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, TrainData.dataset.__len__())\n","        optimizer.zero_grad()\n","        # get HM profiles\n","        inputs_1 = Sample['X_A']\n","        inputs_2 = Sample['X_B']\n","\n","\n","        # get targets: both differential and cell specific expression\n","        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","        diff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","        if(CON==True):\n","            # get labels for contrastive loss\n","            batch_contrastive_targets =[]\n","            for label in batch_diff_targets:\n","                if(label<=-2.0):\n","                    batch_contrastive_targets.append(1)\n","                elif(label>=2.0):\n","                    batch_contrastive_targets.append(1)\n","                else:\n","                    batch_contrastive_targets.append(0)\n","            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","        all_gene_ids[start:end]=Sample['geneID']\n","        batch_size = inputs_1.size(0)\n","\n","        if(AUX==False):\n","            # for raw models: raw_d, raw_c, raw\n","            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","        elif(CON==False):\n","            # for aux models\n","            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","        else:\n","            # for aux and siamese models\n","            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","        per_epoch_loss += loss.item()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n","        optimizer.step()\n","    per_epoch_loss=per_epoch_loss/num_batches\n","    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","def test(ValidData):\n","    model.eval()\n","\n","    diff_targets = torch.zeros(ValidData.dataset.__len__(),1)\n","    diff_predictions = torch.zeros(diff_targets.size(0),1)\n","    if(args.model_name=='raw_d'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","    elif(args.model_name=='raw_c'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),3*args.n_hms)\n","    elif(args.model_name=='aux' or args.model_name=='aux_siamese'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(2*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),2*args.n_hms)\n","    elif(args.model_name=='raw_aux' or args.model_name=='raw_aux_siamese'):\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(3*args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),5*args.n_hms)\n","    else:\n","        all_attention_bin=torch.zeros(ValidData.dataset.__len__(),(args.n_hms*args.n_bins))\n","        all_attention_hm=torch.zeros(ValidData.dataset.__len__(),args.n_hms)\n","\n","    num_batches = int(math.ceil(ValidData.dataset.__len__()/float(args.batch_size)))\n","    all_gene_ids=[None]*ValidData.dataset.__len__()\n","    per_epoch_loss = 0\n","    for idx, Sample in enumerate(ValidData):\n","        if(idx%100==0):\n","            print('TESTING ON BATCH:',idx)\n","        start,end = (idx*args.batch_size), min((idx*args.batch_size)+args.batch_size, ValidData.dataset.__len__())\n","        optimizer.zero_grad()\n","        # get HM profiles\n","        inputs_1 = Sample['X_A']\n","        inputs_2 = Sample['X_B']\n","\n","        \n","        # get targets: both differential and cell specific expression\n","        batch_diff_targets=(Sample['diff']).float().unsqueeze(1)\n","        batch_diff_targets_c1=(Sample['abs_A']).float().unsqueeze(1)\n","        batch_diff_targets_c2=(Sample['abs_B']).float().unsqueeze(1)\n","        diff_targets[start:end,0] = batch_diff_targets[:,0]\n","\n","        if(CON==True):\n","            # get labels for contrastive loss\n","            batch_contrastive_targets =[]\n","            for label in batch_diff_targets:\n","                if(label<=-2.0):\n","                    batch_contrastive_targets.append(1)\n","                elif(label>=2.0):\n","                    batch_contrastive_targets.append(1)\n","                else:\n","                    batch_contrastive_targets.append(0)\n","            batch_contrastive_targets=torch.Tensor(batch_contrastive_targets)\n","\n","\n","        all_gene_ids[start:end]=Sample['geneID']\n","        batch_size = inputs_1.size(0)\n","\n","        if(AUX==False):\n","            batch_diff_predictions,batch_beta,batch_alpha = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","        elif(CON==False):\n","            # for aux models\n","            batch_diff_predictions,batch_beta,batch_alpha,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","        else:\n","            # for aux and siamese models\n","            batch_diff_predictions,batch_beta,batch_alpha,embedding_1,embedding_2,batch_diff_predictions_c1,batch_diff_predictions_c2 = model(inputs_1.type(dtype),inputs_2.type(dtype))\n","\n","            all_attention_bin[start:end]=batch_alpha.data\n","            all_attention_hm[start:end]=batch_beta.data\n","            loss = DiffLoss(batch_diff_predictions,batch_diff_targets.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c1,batch_diff_targets_c1.type(dtype))\n","            loss+=AuxLoss(batch_diff_predictions_c2,batch_diff_targets_c2.type(dtype))\n","            loss+=args.gamma*ConLoss(embedding_1,embedding_2,batch_contrastive_targets.type(dtype))\n","\n","        diff_predictions[start:end] = batch_diff_predictions.data.cpu()\n","        per_epoch_loss += loss.item()\n","    per_epoch_loss=per_epoch_loss/num_batches\n","    return diff_predictions,diff_targets,all_attention_bin,all_attention_hm,per_epoch_loss,all_gene_ids\n","\n","\n","\n","\n","\n","\n","\n","best_valid_loss = 10000000000\n","best_valid_MSE=100000\n","best_valid_R2=-1\n","if(args.test_on_saved_model==False):\n","    for epoch in range(0, args.epochs):\n","        print('=---------------------------------------- Training '+str(epoch+1)+' -----------------------------------=')\n","        diff_predictions,diff_targets,alpha_train,beta_train,train_loss,_ = train(Train)\n","        train_MSE, train_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","        diff_predictions,diff_targets,alpha_valid,beta_valid,valid_loss,gene_ids_valid = test(Valid)\n","        valid_MSE, valid_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","\n","        if(valid_R2 >= best_valid_R2):\n","                # save best epoch -- models converge early\n","            best_valid_R2=valid_R2\n","            torch.save(model,model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","        print(\"Epoch:\",epoch)\n","        print(\"train R2:\",train_R2)\n","        print(\"valid R2:\",valid_R2)\n","        print(\"best valid R2:\", best_valid_R2)\n","\n"," \n","    print(\"finished training!!\")\n","    print(\"best validation R2:\",best_valid_R2)\n","    print(\"testing\")\n","    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","\n","    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","    print(\"test R2:\",test_R2)\n","\n","    if(args.save_attention_maps):\n","        attentionfile=open(attentionmapfile,'w')\n","        attentionfilewriter=csv.writer(attentionfile)\n","        beta_test=beta_test.numpy()\n","        for i in range(len(gene_ids_test)):\n","            gene_attention=[]\n","            gene_attention.append(gene_ids_test[i])\n","            for e in beta_test[i,:]:\n","                gene_attention.append(str(e))\n","            attentionfilewriter.writerow(gene_attention)\n","        attentionfile.close()\n","\n","\n","else:\n","    model=torch.load(model_dir+\"/\"+model_name+'_R2_model.pt')\n","    diff_predictions,diff_targets,alpha_test,beta_test,test_loss,gene_ids_test = test(Test)\n","    test_MSE, test_R2 = evaluate.compute_metrics(diff_predictions,diff_targets)\n","    print(\"test R2:\",test_R2)\n","\n","    if(args.save_attention_maps):\n","        attentionfile=open(attentionmapfile,'w')\n","        attentionfilewriter=csv.writer(attentionfile)\n","        beta_test=beta_test.numpy()\n","        for i in range(len(gene_ids_test)):\n","            gene_attention=[]\n","            gene_attention.append(gene_ids_test[i])\n","            for e in beta_test[i,:]:\n","                gene_attention.append(str(e))\n","            attentionfilewriter.writerow(gene_attention)\n","        attentionfile.close()\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["the model name:  gm_k562_raw_aux_siamese\n","loading data from:   data/\n","saving results in  from:  Results/gm_k562\n","==>processing data\n","==>loading train data\n","Number of genes: 60\n","Number of entries: 6000\n","Number of HMs: 11\n","Number of genes: 60\n","Number of entries: 6000\n","Number of HMs: 11\n","==>loading valid data\n","Number of genes: 10\n","Number of entries: 1000\n","Number of HMs: 11\n","Number of genes: 10\n","Number of entries: 1000\n","Number of HMs: 11\n","==>loading test data\n","Number of genes: 30\n","Number of entries: 3000\n","Number of HMs: 11\n","Number of genes: 30\n","Number of entries: 3000\n","Number of HMs: 11\n","==>building model\n","Using GPU 0\n","==>initializing a new model\n","=---------------------------------------- Training 1 -----------------------------------=\n","TRAINING ON BATCH: 0\n","TESTING ON BATCH: 0\n","Epoch: 0\n","train R2: -0.3398715744686054\n","valid R2: -0.32130308472710445\n","best valid R2: -0.32130308472710445\n","=---------------------------------------- Training 2 -----------------------------------=\n","TRAINING ON BATCH: 0\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-b39928cd30ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'=---------------------------------------- Training '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' -----------------------------------='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mdiff_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiff_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mtrain_MSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_R2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiff_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mdiff_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiff_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgene_ids_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-b39928cd30ed>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(TrainData)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mdiff_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_diff_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mper_epoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"2M2wstAtOWBY","colab_type":"code","colab":{}},"source":["\n","def batch_product(iput, mat2):\n","        result = None\n","        for i in range(iput.size()[0]):\n","            op = torch.mm(iput[i], mat2)\n","            op = op.unsqueeze(0)\n","            if(result is None):\n","                result = op\n","            else:\n","                result = torch.cat((result,op),0)\n","        return result.squeeze(2)\n","\n","\n","class rec_attention(nn.Module):\n","    # attention with bin context vector per HM and HM context vector\n","    def __init__(self,hm,args):\n","        super(rec_attention,self).__init__()\n","        self.num_directions=2 if args.bidirectional else 1\n","        if (hm==False):\n","            self.bin_rep_size=args.bin_rnn_size*self.num_directions\n","        else:\n","            self.bin_rep_size=args.bin_rnn_size\n","    \n","        self.bin_context_vector=nn.Parameter(torch.Tensor(self.bin_rep_size,1),requires_grad=True)\n","    \n","\n","        self.softmax=nn.Softmax()\n","\n","        self.bin_context_vector.data.uniform_(-0.1, 0.1)\n","\n","    def forward(self,iput):\n","        alpha=self.softmax(batch_product(iput,self.bin_context_vector))\n","        [batch_size,source_length,bin_rep_size2]=iput.size()\n","        repres=torch.bmm(alpha.unsqueeze(2).view(batch_size,-1,source_length),iput)\n","        return repres,alpha\n","\n","\n","\n","class recurrent_encoder(nn.Module):\n","    # modular LSTM encoder\n","    def __init__(self,n_bins,ip_bin_size,hm,args):\n","        super(recurrent_encoder,self).__init__()\n","        self.bin_rnn_size=args.bin_rnn_size\n","        self.ipsize=ip_bin_size\n","        self.seq_length=n_bins\n","\n","        self.num_directions=2 if args.bidirectional else 1\n","        if (hm==False):\n","            self.bin_rnn_size=args.bin_rnn_size\n","        else:\n","            self.bin_rnn_size=args.bin_rnn_size // 2\n","        self.bin_rep_size=self.bin_rnn_size*self.num_directions\n","\n","\n","        self.rnn=nn.LSTM(self.ipsize,self.bin_rnn_size,num_layers=args.num_layers,dropout=args.dropout,bidirectional=args.bidirectional)\n","\n","        self.bin_attention=rec_attention(hm,args)\n","    def outputlength(self):\n","        return self.bin_rep_size\n","    def forward(self,single_hm,hidden=None):\n","\n","        bin_output, hidden = self.rnn(single_hm,hidden)\n","        bin_output = bin_output.permute(1,0,2)\n","        hm_rep,bin_alpha = self.bin_attention(bin_output)\n","        return hm_rep,bin_alpha\n","\n","class raw_d(nn.Module):\n","    def __init__(self,args):\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        super(raw_d,self).__init__()\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.diffopsize=2*(self.opsize2)\n","        self.fdiff1_1=nn.Linear(self.opsize2,1)\n","\n","    def forward(self,iput1,iput2):\n","\n","        iput=iput1-iput2\n","        bin_a=None\n","        level1_rep=None\n","        [batch_size,_,_]=iput.size()\n","\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # print(\"Orig: \", bin_a.shape)\n","        # bin_a = bin_a.reshape(-1)\n","        # print(\"Reshape: \", bin_a.shape)\n","\n","        \n","        level1_rep=level1_rep.permute(1,0,2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        prediction_m=((self.fdiff1_1(final_rep_1)))\n","        return prediction_m,hm_level_attention_1, bin_a\n","\n","\n","class raw_c(nn.Module):\n","    def __init__(self,args):\n","        super(raw_c,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(2*self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(2*self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.diffopsize=2*(self.opsize2)\n","        self.fdiff1_1=nn.Linear(self.opsize2,1)\n","\n","    def forward(self,iput1,iput2):\n","        iput=torch.cat((iput1,iput2),2)\n","        bin_a=None\n","        level1_rep=None\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a = bin_a.reshape(-1)\n","\n","        level1_rep=level1_rep.permute(1,0,2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        prediction_m=((self.fdiff1_1(final_rep_1)))\n","\n","        return prediction_m,hm_level_attention_1, bin_a\n","\n","\n","class raw(nn.Module):\n","    # Model with all raw features: difference and absolute features\n","    def __init__(self,args):\n","        super(raw,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(3*self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(3*self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.diffopsize=2*(self.opsize2)\n","        self.fdiff1_1=nn.Linear(self.opsize2,1)\n","\n","    def forward(self,iput1,iput2):\n","        iput3=iput1-iput2\n","        iput4=torch.cat((iput1,iput2),2)\n","        iput=(torch.cat((iput4,iput3),2))\n","\n","        bin_a=None\n","        level1_rep=None\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a = bin_a.reshape(-1)\n","\n","        level1_rep=level1_rep.permute(1,0,2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep)\n","        final_rep_1=final_rep_1.squeeze(1)\n","\n","        prediction_m=((self.fdiff1_1(final_rep_1)))\n","        return prediction_m,hm_level_attention_1, bin_a\n","\n","\n","class aux(nn.Module):\n","    def __init__(self,args):\n","        super(aux,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.shared=False\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.rnn_hms2=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms2.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.f1_1=nn.Linear(self.opsize2,1)\n","        self.f2_1=nn.Linear(self.opsize2,1)\n","        self.diffopsize=2*(self.opsize2)\n","        self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n","        self.predictor_=nn.Linear(self.diffopsize//2,1)\n","        self.relu=nn.ReLU()\n","\n","    def forward_once(self,iput,shared,cellid):\n","        bin_a=None\n","        level1_rep=None\n","        if(shared or cellid==1):\n","            for hm,hm_encdr in enumerate(self.rnn_hms):\n","                hmod=iput[:,:,hm].contiguous()\n","                hmod=torch.t(hmod).unsqueeze(2)\n","                op,a= hm_encdr(hmod)\n","                if level1_rep is None:\n","                    level1_rep=op\n","                    bin_a=a\n","                else:\n","                    level1_rep=torch.cat((level1_rep,op),1)\n","                    bin_a=torch.cat((bin_a,a),1)\n","\n","            #--------TARGET DIMENSION FIX--------No idea if this is right\n","            # flatten bin_a to match all_attention dimensions\n","            # bin_a = bin_a.reshape(-1)\n","\n","            level1_rep=level1_rep.permute(1,0,2)\n","        else:\n","            for hm,hm_encdr in enumerate(self.rnn_hms2):\n","\n","                hmod=iput[:,:,hm].contiguous()\n","                hmod=torch.t(hmod).unsqueeze(2)\n","                op,a= hm_encdr(hmod)\n","                if level1_rep is None:\n","                    level1_rep=op\n","                    bin_a=a\n","                else:\n","                    level1_rep=torch.cat((level1_rep,op),1)\n","                    bin_a=torch.cat((bin_a,a),1)\n"," \n","            #--------TARGET DIMENSION FIX--------No idea if this is right\n","            # flatten bin_a to match all_attention dimensions\n","            # bin_a = bin_a.reshape(-1)\n","\n","            level1_rep=level1_rep.permute(1,0,2)\n","        return level1_rep,bin_a\n","\n","    def forward(self,iput1,iput2):\n"," \n","        level1_rep1,bin_a1=self.forward_once(iput1,self.shared,1)\n","        level1_rep2,bin_a2=self.forward_once(iput2,self.shared,2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","        if(self.joint):\n","            final_rep_2,hm_level_attention_2=self.hm_level_rnn_1(level1_rep2)\n","        else:\n","            final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        final_rep_2=final_rep_2.squeeze(1)\n","        prediction1=((self.f1_1(final_rep_1)))\n","        prediction2=((self.f2_1(final_rep_2)))\n","\n","        mlp_input=torch.cat((final_rep_1,final_rep_2),1)\n","        mlp_input=self.relu(self.predictor_1(mlp_input))\n","        prediction=(self.predictor_(mlp_input))\n","        hm_level_attention=torch.cat((hm_level_attention_1,hm_level_attention_2),1)\n","        \n","        bin_a=torch.cat((bin_a1,bin_a2),1)\n","        # already flattened bin_a's\n","        # bin_a = torch.cat((bin_a1, bin_a2))\n","\n","\n","        return prediction,hm_level_attention,bin_a,prediction1,prediction2\n","\n","\n","class raw_aux(nn.Module):\n","    # level 1 takes raw feaures, level 2 takes aux features as well as raw feature embeddings from level 1\n","    # returns attention scores from raw part only\n","    def __init__(self,args):\n","        super(raw_aux,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.shared=False\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.rnn_hms3=nn.ModuleList()\n","        for i in range(3*self.n_hms):\n","            self.rnn_hms3.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.rnn_hms2=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms2.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_3=recurrent_encoder(5*self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.f1_1=nn.Linear(self.opsize2,1)\n","        self.f2_1=nn.Linear(self.opsize2,1)\n","        self.diffopsize=3*(self.opsize2)\n","        self.predictor_=nn.Linear(self.opsize2,1)\n","        self.relu=nn.ReLU()\n","\n","\n","    def forward(self,iput1,iput2):\n","        iput3=iput1-iput2\n","        iput4=torch.cat((iput1,iput2),2)\n","        iput=(torch.cat((iput4,iput3),2))\n","        bin_a=None\n","        level1_rep=None\n","        [batch_size,_,_]=iput.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hms3):\n","\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a = bin_a.reshape(-1)\n","\n","        level1_rep=level1_rep.permute(1,0,2)\n","\n","        bin_a1=None\n","        level1_rep1=None\n","        [batch_size,_,_]=iput1.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput1[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep1 is None:\n","                level1_rep1=op\n","                bin_a1=a\n","            else:\n","                level1_rep1=torch.cat((level1_rep1,op),1)\n","                bin_a1=torch.cat((bin_a1,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a1 = bin_a1.reshape(-1)\n","\n","        level1_rep1=level1_rep1.permute(1,0,2)\n","        bin_a2=None\n","        level1_rep2=None\n","\n","        for hm,hm_encdr in enumerate(self.rnn_hms2):\n","            hmod=iput2[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep2 is None:\n","                level1_rep2=op\n","                bin_a2=a\n","            else:\n","                level1_rep2=torch.cat((level1_rep2,op),1)\n","                bin_a2=torch.cat((bin_a2,a),1)\n","\n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a2 = bin_a2.reshape(-1)\n","\n","        level1_rep2=level1_rep2.permute(1,0,2)\n","        level1_rep3=torch.cat((level1_rep,level1_rep1,level1_rep2),0)\n","        final_rep_3,hm_level_attention_3=self.hm_level_rnn_3(level1_rep3)\n","        final_rep_3=final_rep_3.squeeze(1)\n","\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","        final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        final_rep_2=final_rep_2.squeeze(1)\n","        prediction1=((self.f1_1(final_rep_1)))\n","        prediction2=((self.f2_1(final_rep_2)))\n","        prediction=(self.predictor_(final_rep_3))\n","\n","        # bin_a1 and bin_a2 supposed to be concatenated like aux and aux_siamese????\n","\n","        return prediction,hm_level_attention_3,bin_a,prediction1,prediction2\n","\n","\n","class aux_siamese(nn.Module):\n","    # aux with siamese  same as aux model mostly, but with shared level 1 embedding\n","    def __init__(self,args):\n","        super(aux_siamese,self).__init__()\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.shared=True\n","        self.rnn_hms=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.f1_1=nn.Linear(self.opsize2,1)\n","        self.f2_1=nn.Linear(self.opsize2,1)\n","        self.diffopsize=2*(self.opsize2)\n","        self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n","        self.predictor_=nn.Linear(self.diffopsize//2,1)\n","        self.relu=nn.ReLU()\n","\n","    def forward_once(self,iput):\n","        bin_a=None\n","        level1_rep=None\n","        [batch_size,_,_]=iput.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","        \n","        #--------TARGET DIMENSION FIX--------No idea if this is right\n","        # flatten bin_a to match all_attention dimensions\n","        # bin_a = bin_a.reshape(-1)\n","\n","        level1_rep=level1_rep.permute(1,0,2)\n","\n","        return level1_rep,bin_a\n","\n","\n","    def forward(self,iput1,iput2):\n","        level1_rep1,bin_a1=self.forward_once(iput1)\n","\n","        level1_rep2,bin_a2=self.forward_once(iput2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","        final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        final_rep_2=final_rep_2.squeeze(1)\n","        [a,b,c]=(level1_rep1.size())\n","        alpha_rep_1=level1_rep1.permute(1,0,2).view(b,a*c)\n","        alpha_rep_2=level1_rep2.permute(1,0,2).view(b,a*c)\n","        prediction1=((self.f1_1(final_rep_1)))\n","        prediction2=((self.f2_1(final_rep_2)))\n","        mlp_input=torch.cat((final_rep_1,final_rep_2),1)\n","        mlp_input=self.relu(self.predictor_1(mlp_input))\n","        prediction=(self.predictor_(mlp_input))\n","        hm_level_attention=torch.cat((hm_level_attention_1,hm_level_attention_2),1)\n","        \n","        bin_a=torch.cat((bin_a1,bin_a2),1)\n","        # already flattened bin_a's\n","        # bin_a = torch.cat((bin_a1, bin_a2))\n","\n","        return prediction,hm_level_attention,bin_a,alpha_rep_1,alpha_rep_2,prediction1,prediction2\n","\n","\n","class raw_aux_siamese(nn.Module):\n","    # similar to raw_aux model with shared level 1 embedding\n","    # returns only raw level attentions\n","    # returns embeddings from shared level 1 for contrastive loss\n","    def __init__(self,args):\n","        self.n_hms=args.n_hms\n","        self.n_bins=args.n_bins\n","        self.ip_bin_size=1\n","        self.joint=False\n","        self.shared=True\n","        super(raw_aux_siamese,self).__init__()\n","        self.rnn_hms=nn.ModuleList()\n","        self.rnn_hmsx=nn.ModuleList()\n","        for i in range(self.n_hms):\n","            self.rnn_hms.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        for i in range(3*self.n_hms):\n","            self.rnn_hmsx.append(recurrent_encoder(self.n_bins,self.ip_bin_size,False,args))\n","        self.opsize = self.rnn_hms[0].outputlength()\n","        self.hm_level_rnn_1=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_1x=recurrent_encoder(5*self.n_hms,self.opsize,True,args)\n","        self.hm_level_rnn_2=recurrent_encoder(self.n_hms,self.opsize,True,args)\n","        self.opsize2=self.hm_level_rnn_1.outputlength()\n","        self.f1_1=nn.Linear(self.opsize2,1)\n","        self.f2_1=nn.Linear(self.opsize2,1)\n","        self.diffopsize=3*(self.opsize2)\n","        #self.predictor_1=nn.Linear(self.diffopsize,self.diffopsize//2)\n","        self.predictor_=nn.Linear(self.opsize2,1)\n","        self.relu=nn.ReLU()\n","        self.finalsoftmax=nn.LogSoftmax()\n","    def forward_once(self,iput):\n","        bin_a=None\n","        level1_rep=None\n","        [batch_size,_,_]=iput.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hms):\n","            hmod=iput[:,:,hm].contiguous()\n","            hmod=torch.t(hmod).unsqueeze(2)\n","            op,a= hm_encdr(hmod)\n","            if level1_rep is None:\n","                level1_rep=op\n","                bin_a=a\n","            else:\n","                level1_rep=torch.cat((level1_rep,op),1)\n","                bin_a=torch.cat((bin_a,a),1)\n","        level1_rep=level1_rep.permute(1,0,2)\n","        return level1_rep,bin_a\n","\n","\n","    def forward(self,iput1,iput2):\n","        iput3=iput1-iput2\n","        iput4=torch.cat((iput1,iput2),2)\n","        iput=(torch.cat((iput4,iput3),2))\n","        bin_ax=None\n","        level1_repx=None\n","        [batch_size,_,_]=iput.size()\n","        for hm,hm_encdr in enumerate(self.rnn_hmsx):\n","            hmodx=iput[:,:,hm].contiguous()\n","            hmodx=torch.t(hmodx).unsqueeze(2)\n","            opx,ax= hm_encdr(hmodx)\n","            if level1_repx is None:\n","                level1_repx=opx\n","                bin_ax=ax\n","            else:\n","                level1_repx=torch.cat((level1_repx,opx),1)\n","                bin_ax=torch.cat((bin_ax,ax),1)\n","        level1_repx=level1_repx.permute(1,0,2)\n","\n","        level1_rep1,bin_a1=self.forward_once(iput1)\n","\n","        level1_rep2,bin_a2=self.forward_once(iput2)\n","        final_rep_1,hm_level_attention_1=self.hm_level_rnn_1(level1_rep1)\n","        final_rep_2,hm_level_attention_2=self.hm_level_rnn_2(level1_rep2)\n","        final_rep_1=final_rep_1.squeeze(1)\n","        final_rep_2=final_rep_2.squeeze(1)\n","        [a,b,c]=(level1_rep1.size())\n","        alpha_rep_1=level1_rep1.permute(1,0,2).view(b,a*c)\n","        alpha_rep_2=level1_rep2.permute(1,0,2).view(b,a*c)\n","        level1_rep3=torch.cat((level1_repx,level1_rep1,level1_rep2),0)\n","        final_rep_1x,hm_level_attention_1x=self.hm_level_rnn_1x(level1_rep3)\n","        final_rep_1x=final_rep_1x.squeeze(1)\n","        prediction1=((self.f1_1(final_rep_1)))\n","        prediction2=((self.f2_1(final_rep_2)))\n","        prediction=(self.predictor_(final_rep_1x))\n","\n","        return prediction,hm_level_attention_1x,bin_ax,alpha_rep_1,alpha_rep_2,prediction1,prediction2"],"execution_count":0,"outputs":[]}]}